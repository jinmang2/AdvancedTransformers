# [google-research/bert](https://github.com/google-research/bert/issues)의 Issues 공부

## About BERT!
### Paper Detail
- [How is this counted? --> "3.3 billion word corpus"](https://github.com/google-research/bert/issues/1060)
- [How is the number of BERT model parameters calculated?](https://github.com/google-research/bert/issues/656)

### Implement Detail
- [Question: What does "pooler layer" mean? Why it called pooler?](https://github.com/google-research/bert/issues/1102)
- [Why we get last 4 layers while residual connection transfer useful knowledge to the subsequent layers?](https://github.com/google-research/bert/issues/1064)
- [Infill](https://github.com/google-research/bert/pull/913) (very very 중요!! 코드 구현 어캐했는지 보고 나도 구현하기)

### Training Detail
- [Explain the variables in the checkpoint](https://github.com/google-research/bert/issues/1019)
- [NotfoundError: Key bert/embeddings/LayerNorm/beta not found in checkpoint](https://github.com/google-research/bert/issues/997)
- [NotFoundError: [_Derived_]No gradient defined for op: Einsum on Tensorflow 1.15](https://github.com/google-research/bert/issues/1012)
- [How to see masked_lm_loss & next_sentence_loss per iteration step during train?](https://github.com/google-research/bert/issues/952)
- [how use the pretrain checkpoint to continue train on my own corpus?](https://github.com/google-research/bert/issues/888)
- [Can BERT really handle misspelled words?](https://github.com/google-research/bert/issues/812)
- [Experiment using RAdam optimizer](https://github.com/google-research/bert/issues/810)
- [Performance metrics of the classifier](https://github.com/google-research/bert/issues/800)
- [Tutorial: A Pipeline Of Pretraining Bert On Google TPU](https://github.com/google-research/bert/issues/681)
- [Determining training steps](https://github.com/google-research/bert/issues/662)
- [Learning Rate and Warmup Steps](https://github.com/google-research/bert/issues/649)
- [How to freeze layers of bert?](https://github.com/google-research/bert/issues/637)
- [How often is the validation/evaluation performed? (fine-tuning using run_classifier.py)](https://github.com/google-research/bert/issues/636)
- [How to get masked word prediction probabilities](https://github.com/google-research/bert/issues/608)

## Tokenization
- [How to handle labels when using the BERT wordpiece tokenizer](https://github.com/google-research/bert/issues/646)
- [Tokenization behavior with messed-up unicode characters](https://github.com/google-research/bert/issues/1093)
- [x] [fix korean tokenization bug](https://github.com/google-research/bert/pull/1070)
- [x] [Update tokenization](https://github.com/google-research/bert/pull/1042)
- [WordPiece Tokenizer Clarification](https://github.com/google-research/bert/issues/763)

#### Vocabulary (답변 없는 경우 많음)
- [use custom vocab.txt](https://github.com/google-research/bert/issues/1092)
- [Adding custom domain words and abbreviations to vocab.txt](https://github.com/google-research/bert/issues/1083)
- [update load_vocab() function based on ALBERT](https://github.com/google-research/bert/pull/961)
- [Language dependent vocabulary?](https://github.com/google-research/bert/issues/641)

#### Embedding
- [What does bert embedding of a single term signify?](https://github.com/google-research/bert/issues/990)
- [Bert sent embeddings](https://github.com/google-research/bert/pull/691)

## Bert Pre-Training
- [Pretraining BERT without next sentence prediction](https://github.com/google-research/bert/issues/178)
- [Using my pre-trained model](https://github.com/google-research/bert/issues/1040)
- [BERT pretraining num_train_steps questions](https://github.com/google-research/bert/issues/1025)
- [BERT pre-training using only domain specific text](https://github.com/google-research/bert/issues/615)

## Bert Fine-Tuning
- [Does bert have this function ?](https://github.com/google-research/bert/issues/1024) (Bert for LM)
- [Is it possible feed BERT to seq2seq encoder for NMT (for low resource language)?](https://github.com/google-research/bert/issues/1007) (답없음)
- [extract_features sentence embedding BERT](https://github.com/google-research/bert/issues/1085)
- [How does Google calculate a document embeddings using BERT in its new search?](https://github.com/google-research/bert/issues/957) (Fine-tune이라기 보단 feature-based일거 같지만... Google Search에서 어떻게 활용할지?)
- [Exporting bert model to a saved model format](https://github.com/google-research/bert/issues/843) (tf serving)
- [How to use BERT for ranking with Pairwise loss function during Finetuining](https://github.com/google-research/bert/issues/761)
- [Serving fine-tuned Model - best solution](https://github.com/google-research/bert/issues/755) (bert in flask)
- [Sentiment analysis on emoji data.](https://github.com/google-research/bert/issues/748)
- [how to fine tune bert for ner on custom data](https://github.com/google-research/bert/issues/713)
- [Tuned Bert Model on MRPC gives wrong predictions.](https://github.com/google-research/bert/issues/663)
- [How to use run_squad.py to produce multiple answers for a question?](https://github.com/google-research/bert/issues/657)
- [Using bert for Document Classification](https://github.com/google-research/bert/issues/650)
- [Losing Knowledge for Language Model in Fine-Tuning](https://github.com/google-research/bert/issues/651)
- [how to use BERT for Siamese Model paraphrase identify](https://github.com/google-research/bert/issues/648)
- [Classification fine tuning for Q & A](https://github.com/google-research/bert/issues/639)

## Distributed Training
- [Exploding gradients in training BERT from scratch](https://github.com/google-research/bert/issues/1016)
- [Can I run multi-gpu pretraining?](https://github.com/google-research/bert/issues/978)
- [x] [Gradient Accumulation](https://github.com/google-research/bert/pull/976)
- [multi-gpu horovod](https://github.com/google-research/bert/issues/743)
- [Recommended GPU size when training BERT-base](https://github.com/google-research/bert/issues/645)

## Open Issues
- [How to create two BERT model with shared weights?](https://github.com/google-research/bert/issues/605)
- [How to train our own domain-specific data instead of using pre-training models？](https://github.com/google-research/bert/issues/606)
- [create_pretraining_data.py generates tfrecords that are too big](https://github.com/google-research/bert/issues/1161)
- [How can i use BERT to correct the alignment and spellings in a sentence?](https://github.com/google-research/bert/issues/1154)
- [Update the number of parameters](https://github.com/google-research/bert/pull/1150)
- [Incomplete feature vectors generated by Bert model.](https://github.com/google-research/bert/issues/1145)
- [Update tokenization.py](https://github.com/google-research/bert/pull/1117)
- [Dealing with ellipses in BERT tokenization](https://github.com/google-research/bert/issues/1116)
- [A spelling error is fixed](https://github.com/google-research/bert/pull/1168)
- [How to use my own additional vocabulary dictionary?](https://github.com/google-research/bert/issues/396)
- [Is there a plan to release code for fine-tuning on CoQA dataset?](https://github.com/google-research/bert/issues/597)
- [How to use my own vocabulary when do pre-training from scratch?](https://github.com/google-research/bert/issues/589)
- [BERT has a non deterministic behaviour](https://github.com/google-research/bert/issues/583)
- [how to use bert to text summary](https://github.com/google-research/bert/issues/576)
- [BERT multilingual for zero-shot classification](https://github.com/google-research/bert/issues/577)
- [BERT encode emojis as [UNK] token](https://github.com/google-research/bert/issues/587)
- [How to use BERT for sequence labelling](https://github.com/google-research/bert/issues/569)
- [Added support for multi gpu training and distributed training using Horovod](https://github.com/google-research/bert/pull/568)
- [How many articles (Wiki+Book corpus) do Bert use in pretraining?](https://github.com/google-research/bert/issues/570)
- [Problem with wordpiece tokenization](https://github.com/google-research/bert/issues/560)
- [problem multiclass text classification](https://github.com/google-research/bert/issues/559)
- [IndexError in run_classifier.py::MrpcProcessor::_create_examples (2)](https://github.com/google-research/bert/issues/551)
- [bad results after pretraining](https://github.com/google-research/bert/issues/529)
- [Is BERT a kind of cheating?](https://github.com/google-research/bert/issues/514)
- [Fixing normalized korean char](https://github.com/google-research/bert/pull/512)
- [Are BERT word-embeddings capable of synonyms?](https://github.com/google-research/bert/issues/507)
- [How to share BERT between tasks in multi-task setting?](https://github.com/google-research/bert/issues/504)
- [add regression fine-tuning](https://github.com/google-research/bert/pull/503)
- [Pre-trained monolingual in French](https://github.com/google-research/bert/issues/502)
- [what is the synthetic self-training](https://github.com/google-research/bert/issues/488)
- [Fine-Tune encodings on unsupervised data?](https://github.com/google-research/bert/issues/448)
- [Using BERT with custom QA dataset](https://github.com/google-research/bert/issues/411)
- [How can I change vocab size for pretrained model?](https://github.com/google-research/bert/issues/406)
- [How to use my own additional vocabulary dictionary?](https://github.com/google-research/bert/issues/396)
- [Can I use a "[CLS]...[SEP]...[SEP]...[SEP]" in tokens?](https://github.com/google-research/bert/issues/395)
- [Weights from next sentence prediction](https://github.com/google-research/bert/issues/370)
- [Optimize the code logic](https://github.com/google-research/bert/pull/366)
- [BERT vs Word2vec](https://github.com/google-research/bert/issues/362)
- [BERT for text summarization](https://github.com/google-research/bert/issues/352)
- [Wiki Data Formation Problem, Need Sentence Split](https://github.com/google-research/bert/issues/341)
- [how use BERT language model to predict next word](https://github.com/google-research/bert/issues/323)
- [how to get fine_tune model output probability](https://github.com/google-research/bert/issues/322)
- [how the model reflect 'bidirectional'?](https://github.com/google-research/bert/issues/319)
- [Extract features return different layer values (vectors) each time, is it working well?](https://github.com/google-research/bert/issues/312)
- [Is BERT powerful enough to learn sentence embedding and word embedding?](https://github.com/google-research/bert/issues/261)
- [Gpu optimizations](https://github.com/google-research/bert/pull/255)
- [Use BERT fine-tuned model for Tensorflow serving](https://github.com/google-research/bert/issues/146)
- [What is BERT?](https://github.com/google-research/bert/issues/566)
- [BERT with FP16 and XLA inference speed](https://github.com/google-research/bert/issues/391)

## Closed Issues
- [zero-shot for IsNext and NotNext function](https://github.com/google-research/bert/issues/1118)
- [I don't know how to properly use fine tuned Bert Model](https://github.com/google-research/bert/issues/1097)
- [why dropout at predicting time](https://github.com/google-research/bert/issues/1096)
- [LayerNorm normalises the batch dimension as well](https://github.com/google-research/bert/issues/1088)
- [MRPC Produces Two Vastly Different Eval Accuracy](https://github.com/google-research/bert/issues/1037)
- [bert run_classifier](https://github.com/google-research/bert/issues/989)
- [how to realize the tokenization of BERT model in c++](https://github.com/google-research/bert/issues/878)
- [how to infer in python](https://github.com/google-research/bert/issues/614)
- [Bert Context Based QA](https://github.com/google-research/bert/issues/620)
- [Best performance on concatenated layers: which dimension?](https://github.com/google-research/bert/issues/511)
- [Issue with multiclass text classification](https://github.com/google-research/bert/issues/449)
- [What is exactly the learning rate warmup described in the paper?](https://github.com/google-research/bert/issues/425)
- [Fine-Tuning specifications for MNLI/XNLI](https://github.com/google-research/bert/issues/328)
- [fine-tuning with additional masked lm loss, and masked lm loss diverged](https://github.com/google-research/bert/issues/306)
- [Handling domain specific vocabulary](https://github.com/google-research/bert/issues/237)
- [Can you release the hyper-parameter of NER task?](https://github.com/google-research/bert/issues/223)
- [Question about mask strategy.](https://github.com/google-research/bert/issues/169)
- [Is CLS token also Masked in pre-training?](https://github.com/google-research/bert/issues/166)
- [BERT Vector Space shows issues with unknown words](https://github.com/google-research/bert/issues/164)
- [Simplifying BERT for Q&A - One paragraph and Query](https://github.com/google-research/bert/issues/159)
- [Reproducing paper results from feature vectors (STS-B dataset)](https://github.com/google-research/bert/issues/161)
- [Fine tuning BERT to extract embeddings (like ELMo)](https://github.com/google-research/bert/issues/145)
- [Classification quality is depends on max_sequence_length](https://github.com/google-research/bert/issues/113)
- [fine-tuned for a document task](https://github.com/google-research/bert/issues/107)
- [When to stop training? What is a good valid loss value to stop ? How to improve classification performance?](https://github.com/google-research/bert/issues/95)
- [plan to release SWAG code?](https://github.com/google-research/bert/issues/38)
- [Add flag to extract only features for the [CLS] token](https://github.com/google-research/bert/pull/87)
- [run_pretraining.py - clip gradient error: Found Inf or NaN global norm: Tensor had NaN value](https://github.com/google-research/bert/issues/82)
- [How to train models on GPU instead of CPU when TPU is not available?](https://github.com/google-research/bert/issues/75)
- [how to see loss per steps or epoch during train?](https://github.com/google-research/bert/issues/70)
- [Extracting features on for long sequences / SQuAD](https://github.com/google-research/bert/issues/66)
- [Trouble to understand position embedding.](https://github.com/google-research/bert/issues/58)
- [PyTorch implementation](https://github.com/google-research/bert/issues/54)
- [Plans to release sequence tagging task fine-tuning code?](https://github.com/google-research/bert/issues/33)
- [w to get the word embedding after pre-training?](https://github.com/google-research/bert/issues/60)
