# [google-research/bert](https://github.com/google-research/bert/issues)의 Issues 공부

- [Question: What does "pooler layer" mean? Why it called pooler?](https://github.com/google-research/bert/issues/1102)
- [Tokenization behavior with messed-up unicode characters](https://github.com/google-research/bert/issues/1093)
- [use custom vocab.txt](https://github.com/google-research/bert/issues/1092)
- [extract_features sentence embedding BERT](https://github.com/google-research/bert/issues/1085)
- [Adding custom domain words and abbreviations to vocab.txt](https://github.com/google-research/bert/issues/1083)
- [fix korean tokenization bug](https://github.com/google-research/bert/pull/1070)
- [Pretrain without Next Sentence Prediction Task](https://github.com/google-research/bert/issues/1068)
- [Why we get last 4 layers while residual connection transfer useful knowledge to the subsequent layers?](https://github.com/google-research/bert/issues/1064)
- [How is this counted? --> "3.3 billion word corpus"](https://github.com/google-research/bert/issues/1060)
- [is it necessary to drop stop words before training?](https://github.com/google-research/bert/issues/1057)
- [race-condition in optimizer.py](https://github.com/google-research/bert/issues/1050)
- [how compress the fine-tune model to small?](https://github.com/google-research/bert/issues/1043)
- [Update tokenization](https://github.com/google-research/bert/pull/1042)
- [Using my pre-trained model](https://github.com/google-research/bert/issues/1040)
- [BERT pretraining num_train_steps questions](https://github.com/google-research/bert/issues/1025)
- [Does bert have this function ?](https://github.com/google-research/bert/issues/1024)
- [Explain the variables in the checkpoint](https://github.com/google-research/bert/issues/1019)
- [Exploding gradients in training BERT from scratch](https://github.com/google-research/bert/issues/1016)
- [NotFoundError: [_Derived_]No gradient defined for op: Einsum on Tensorflow 1.15](https://github.com/google-research/bert/issues/1012)
- [Is it possible feed BERT to seq2seq encoder for NMT (for low resource language)?](https://github.com/google-research/bert/issues/1007)
- [NotfoundError: Key bert/embeddings/LayerNorm/beta not found in checkpoint](https://github.com/google-research/bert/issues/997)
- [What does bert embedding of a single term signify?](https://github.com/google-research/bert/issues/990)
- [Can I run multi-gpu pretraining?](https://github.com/google-research/bert/issues/978)
- [Gradient Accumulation](https://github.com/google-research/bert/pull/976)
- [Add explicit tensorflow version to predicting_movie_reviews_with_bert…](https://github.com/google-research/bert/pull/965)
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
