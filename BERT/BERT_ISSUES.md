# [google-research/bert](https://github.com/google-research/bert/issues)의 Issues 공부

## Open Issue
- [Question: What does "pooler layer" mean? Why it called pooler?](https://github.com/google-research/bert/issues/1102)
- [Tokenization behavior with messed-up unicode characters](https://github.com/google-research/bert/issues/1093)
- [use custom vocab.txt](https://github.com/google-research/bert/issues/1092)
- [extract_features sentence embedding BERT](https://github.com/google-research/bert/issues/1085)
- [Adding custom domain words and abbreviations to vocab.txt](https://github.com/google-research/bert/issues/1083)
- [fix korean tokenization bug](https://github.com/google-research/bert/pull/1070)
- [Pretrain without Next Sentence Prediction Task](https://github.com/google-research/bert/issues/1068)
- [Why we get last 4 layers while residual connection transfer useful knowledge to the subsequent layers?](https://github.com/google-research/bert/issues/1064)
- [How is this counted? --> "3.3 billion word corpus"](https://github.com/google-research/bert/issues/1060)
- [is it necessary to drop stop words before training?](https://github.com/google-research/bert/issues/1057)
- [race-condition in optimizer.py](https://github.com/google-research/bert/issues/1050)
- [how compress the fine-tune model to small?](https://github.com/google-research/bert/issues/1043)
- [Update tokenization](https://github.com/google-research/bert/pull/1042)
- [Using my pre-trained model](https://github.com/google-research/bert/issues/1040)
- [BERT pretraining num_train_steps questions](https://github.com/google-research/bert/issues/1025)
- [Does bert have this function ?](https://github.com/google-research/bert/issues/1024)
- [Explain the variables in the checkpoint](https://github.com/google-research/bert/issues/1019)
- [Exploding gradients in training BERT from scratch](https://github.com/google-research/bert/issues/1016)
- [NotFoundError: [_Derived_]No gradient defined for op: Einsum on Tensorflow 1.15](https://github.com/google-research/bert/issues/1012)
- [Is it possible feed BERT to seq2seq encoder for NMT (for low resource language)?](https://github.com/google-research/bert/issues/1007)
- [NotfoundError: Key bert/embeddings/LayerNorm/beta not found in checkpoint](https://github.com/google-research/bert/issues/997)
- [What does bert embedding of a single term signify?](https://github.com/google-research/bert/issues/990)
- [Can I run multi-gpu pretraining?](https://github.com/google-research/bert/issues/978)
- [Gradient Accumulation](https://github.com/google-research/bert/pull/976)
- [Add explicit tensorflow version to predicting_movie_reviews_with_bert…](https://github.com/google-research/bert/pull/965)
- [update load_vocab() function based on ALBERT](https://github.com/google-research/bert/pull/961)
- [How does Google calculate a document embeddings using BERT in its new search?](https://github.com/google-research/bert/issues/957)
- [How to see masked_lm_loss & next_sentence_loss per iteration step during train?](https://github.com/google-research/bert/issues/952)
- [how to train BertForMaskedLM model with custom corpus?](https://github.com/google-research/bert/issues/949)
- [Training on specific task after a fine-tuning on the language model](https://github.com/google-research/bert/issues/943)
- [Infill](https://github.com/google-research/bert/pull/913)
- [OOM error fine-tuning](https://github.com/google-research/bert/issues/879)
- [how use the pretrain checkpoint to continue train on my own corpus?](https://github.com/google-research/bert/issues/888)
- [Albert Availability](https://github.com/google-research/bert/issues/868)
- [Prediction out of Bound | Condition x < y did not hold element-wise](https://github.com/google-research/bert/issues/846)
- [Exporting bert model to a saved model format](https://github.com/google-research/bert/issues/843)
- [Can BERT really handle misspelled words?](https://github.com/google-research/bert/issues/812)
- [Experiment using RAdam optimizer](https://github.com/google-research/bert/issues/810)
- [Using tensorflow serving to Serve BERT model](https://github.com/google-research/bert/pull/807)
- [Performance metrics of the classifier](https://github.com/google-research/bert/issues/800)
- [WordPiece Tokenizer Clarification](https://github.com/google-research/bert/issues/763)
- [How to use BERT for ranking with Pairwise loss function during Finetuining](https://github.com/google-research/bert/issues/761)
- [Serving fine-tuned Model - best solution](https://github.com/google-research/bert/issues/755)
- [Sentiment analysis on emoji data.](https://github.com/google-research/bert/issues/748)
- [Bert pre-training loss convergence](https://github.com/google-research/bert/issues/746)
- [Two fields for sentence classification](https://github.com/google-research/bert/issues/744)
- [multi-gpu horovod](https://github.com/google-research/bert/issues/743)
- [how to view the prediction result of each sample？](https://github.com/google-research/bert/issues/725)
- [File "run_classifier.py", line 326, in _create_examples text_b = tokenization.convert_to_unicode(line[4]) IndexError: list index out of range](https://github.com/google-research/bert/issues/717)
- [how to fine tune bert for ner on custom data](https://github.com/google-research/bert/issues/713)
- [Assign requires shapes of both tensors to match. lhs shape= [3072] rhs shape= [4096]](https://github.com/google-research/bert/issues/703)
- [Bert sent embeddings](https://github.com/google-research/bert/pull/691)
- [Tutorial: A Pipeline Of Pretraining Bert On Google TPU](https://github.com/google-research/bert/issues/681)
- [How to classify neutral sentiments using BERT.](https://github.com/google-research/bert/issues/684)
- [Tuned Bert Model on MRPC gives wrong predictions.](https://github.com/google-research/bert/issues/663)
- [Determining training steps](https://github.com/google-research/bert/issues/662)
- [How to use run_squad.py to produce multiple answers for a question?](https://github.com/google-research/bert/issues/657)
- [How is the number of BERT model parameters calculated?](https://github.com/google-research/bert/issues/656)
- [Using bert for Document Classification](https://github.com/google-research/bert/issues/650)
- [Losing Knowledge for Language Model in Fine-Tuning](https://github.com/google-research/bert/issues/651)
- [Learning Rate and Warmup Steps](https://github.com/google-research/bert/issues/649)
- [how to use BERT for Siamese Model paraphrase identify](https://github.com/google-research/bert/issues/648)
- [How to handle labels when using the BERT wordpiece tokenizer](https://github.com/google-research/bert/issues/646)
- [Recommended GPU size when training BERT-base](https://github.com/google-research/bert/issues/645)
- [Language dependent vocabulary?](https://github.com/google-research/bert/issues/641)
- [Classification fine tuning for Q & A](https://github.com/google-research/bert/issues/639)
- [How to freeze layers of bert?](https://github.com/google-research/bert/issues/637)
- [How often is the validation/evaluation performed? (fine-tuning using run_classifier.py)](https://github.com/google-research/bert/issues/636)
- [BERT pre-training using only domain specific text](https://github.com/google-research/bert/issues/615)
- [How to get masked word prediction probabilities](https://github.com/google-research/bert/issues/608)
- [How to create two BERT model with shared weights?](https://github.com/google-research/bert/issues/605)
- [How to train our own domain-specific data instead of using pre-training models？](https://github.com/google-research/bert/issues/606)
- [create_pretraining_data.py generates tfrecords that are too big](https://github.com/google-research/bert/issues/1161)
- [How can i use BERT to correct the alignment and spellings in a sentence?](https://github.com/google-research/bert/issues/1154)
- [Update the number of parameters](https://github.com/google-research/bert/pull/1150)
- [Incomplete feature vectors generated by Bert model.](https://github.com/google-research/bert/issues/1145)
- [Update tokenization.py](https://github.com/google-research/bert/pull/1117)
- [Dealing with ellipses in BERT tokenization](https://github.com/google-research/bert/issues/1116)
- [A spelling error is fixed](https://github.com/google-research/bert/pull/1168)
- [How to use my own additional vocabulary dictionary?](https://github.com/google-research/bert/issues/396)
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
