{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT를 위한 형태소 분석\n",
    "- google research의 tensorflow 버전(원본) bert\n",
    "- ETRI에서 제공한 pre-trained 모델과 한국어 단어 사전 사용\n",
    "- ETRI에서 제공하는 버전은 총 4개\n",
    "    - Pytorch + Morphology\n",
    "    - Tensorflow + Morphology\n",
    "    - Pytorch + Eojeol\n",
    "    - Tensorflow + Eojeol\n",
    "- 이 중 `Pytorch`제외, 형태소 분석이 된 text를 input으로 받는 2번 선택\n",
    "- `Morphology`는 input text에 **TTA 표준 형태소 태그셋(TTAK.KO-11.0010/R1)**에 맞는 **형태소 분석기**를 사용해야 함.\n",
    "- TTA 표준 형태소 태그셋에 맞게 분석하는 형태소 분석기는 다음과 같음\n",
    "    - `Mecab`\n",
    "        - `심사숙고/NNG + 했/XSV+EP + 겠/EP + 지만/EC`와 같이 분석하는 경우가 있음\n",
    "        - Input이 `XSV+EP`으로 나오면 안됨. + 제거 후 `[하/동사파생접미사(XSV), 였/선어말어미(EP)]`로 분석해야함\n",
    "    - `ETRI 형태소 분석기`\n",
    "        - Web API에 접속하여 사용\n",
    "        - 일일 한도 제한있음\n",
    "    - `Khaiii`\n",
    "        - Kakao Hangul Analyzer III\n",
    "        - 속도도 빠르고 만족스러운 성능을 보임\n",
    "        - 윈도우 지원 안함\n",
    "        - `심사숙고/NNG + 하/XSV + 였/EP + 겠/EP + 지만/EC`와 같이 아주 잘 분석함\n",
    "- 아래 표는 `TTAK.KO-11.0010/R1`\n",
    "\n",
    "    <table class=\"table table-striped table-bordered\" style=\"width:450px;\">\n",
    "      <thead>\n",
    "        <tr>\n",
    "          <th style=\"width:100px\" align=\"center\">대분류</td>\n",
    "          <th style=\"width:150px\" align=\"center\">중분류</td>\n",
    "          <th style=\"width:200px\" align=\"center\">대분류</td>\n",
    "        </tr>\n",
    "      </thead>\n",
    "      <tbody>\n",
    "        <tr>\n",
    "          <td rowspan=\"5\" align=\"center\">(1) 체언</td>\n",
    "          <td rowspan=\"3\" align=\"center\">명사</td>\n",
    "          <td align=\"center\">일반명사(NNG)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td align=\"center\">고유명사(NNP)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td align=\"center\">의존명사(NNB)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td align=\"center\">대명사(NP)</td>\n",
    "          <td align=\"center\">대명사(NP)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td align=\"center\">수사(NR)</td>\n",
    "          <td align=\"center\">수사(NR)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td rowspan=\"5\" align=\"center\">(2) 용언</td>\n",
    "          <td align=\"center\">동사(VV)</td>\n",
    "          <td align=\"center\">동사(VV)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td align=\"center\">형용사(VA)</td>\n",
    "          <td align=\"center\">형용사(VA)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td align=\"center\">보조용언(VX)</td>\n",
    "          <td align=\"center\">보조용언(VX)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td rowspan=\"2\" align=\"center\">지정사(VC)</td>\n",
    "          <td align=\"center\">긍정지정사(VCP)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td align=\"center\">부정지정사(VCN)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td rowspan=\"5\" align=\"center\">(3) 수식언</td>\n",
    "          <td rowspan=\"3\" align=\"center\">관형사(MM)</td>\n",
    "          <td align=\"center\">성상 관형사(MMA)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td align=\"center\">지시 관형사(MMD)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td align=\"center\">수 관형사(MMN)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td rowspan=\"2\" align=\"center\">부사(MA)</td>\n",
    "          <td align=\"center\">일반부사(MAG)</td>        \n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td align=\"center\">접속부사(MAJ)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td align=\"center\">(4) 독립언</td>\n",
    "          <td align=\"center\">감탄사(IC)</td>\n",
    "          <td align=\"center\">감탄사(IC)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td rowspan=\"9\" align=\"center\">(5) 관계언</td>\n",
    "          <td rowspan=\"7\" align=\"center\">격조사(JK)</td>\n",
    "          <td align=\"center\">주격조사(JKS)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td align=\"center\">보격조사(JKC)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td align=\"center\">관형격조사(JKG)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td align=\"center\">목적격조사(JKO)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td align=\"center\">부사격조사(JKB)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td align=\"center\">호격조사(JKV)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td align=\"center\">인용격조사(JKQ)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td align=\"center\">보조사(JX)</td>\n",
    "          <td align=\"center\">보조사(JX)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td align=\"center\">접속조사(JC)</td>\n",
    "          <td align=\"center\">접속조사(JC)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td rowspan=\"10\" align=\"center\">(6) 의존형태</td>\n",
    "          <td rowspan=\"5\" align=\"center\">어미(EM)</td>\n",
    "          <td align=\"center\">선어말어미(EP)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td align=\"center\">종결어미(EF)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td align=\"center\">연결어미(EC)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td align=\"center\">명사형전성어미(ETN)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td align=\"center\">관형형전성어미(ETM)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td align=\"center\">접두사(XP)</td>\n",
    "          <td align=\"center\">체언접두사(XPN)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td rowspan=\"3\" align=\"center\">접미사(XS)</td>\n",
    "          <td align=\"center\">명사파생접미사(XSN)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td align=\"center\">동사파생접미사(XSV)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td align=\"center\">형용사파생접미사(XSA)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td align=\"center\">어근(XR)</td>\n",
    "          <td align=\"center\">어근(XR)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td rowspan=\"10\" align=\"center\">(7) 기초</td>\n",
    "          <td rowspan=\"6\" align=\"center\">일반기호(ST)</td>\n",
    "          <td align=\"center\">마침표, 물음표, 느낌표(SF)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td align=\"center\">쉼표, 가운뎃점, 콜론, 빗금(SP)</td>\n",
    "        </tr>  \n",
    "        <tr>\n",
    "          <td align=\"center\">따옴표, 괄호표, 줄표(SS)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td align=\"center\">줄임표(SE)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td align=\"center\">붙임표(물결)(SO)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td align=\"center\">기타 기호(SW)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td align=\"center\">외국어(SL)</td>\n",
    "          <td align=\"center\">외국어(SL)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td align=\"center\">한자(SH)</td>\n",
    "          <td align=\"center\">한자(SH)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td align=\"center\">숫자(SN)</td>\n",
    "          <td align=\"center\">숫자(SN)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td align=\"center\">분석불능범주(NA)</td>\n",
    "          <td align=\"center\">분석불능범주(NA)</td>\n",
    "        </tr>\n",
    "      </tdoby>\n",
    "    </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTTokenizer\n",
    "- End 2 End Tokenizer\n",
    "- `BasicTokenizer`와 `WordpieceTokenizer`를 연결하여 한번에 Tokenizing시킨다\n",
    "- input은 위에서 언급했듯이 TTAK 기준으로 형태소 분석된 텍스트를 넣어줘야하며\n",
    "- 이를 Token화 시키는 것이 해당 Tokenizer의 역할\n",
    "- 사실상 하는 역할이 없다. 아래 어떻게 작동되는지 보면 안다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert_single_example\n",
    "- BERT 논문에 나오는 positional embedding을 실시\n",
    "- 내부적으로 `BERTTokenizer`의 input으로 들어갈 수 있게 형태소 분석을 해주고\n",
    "- Token화된 input을 positional embedding시켜 feature화시키고 이를 반환한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## file_based_convert_examples_to_features\n",
    "- 들어오는 input을 `convert_single_example`함수에 넣어 positional embedding된 feature로 받고\n",
    "- 이를 `tf_record`파일로 기록한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gocha!\n",
    "- 어떻게 Tokenizing하는지 들여다보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 사전 정의\n",
    "- `BERTTokenizer`의 생성자를 보면 아래 항목을 argument로 받고\n",
    "    - vocab_file: 단어 사전이 저장된 file path\n",
    "    - do_lower_case: 원래는 소문자 변환을 할 것인지, 한국어에서는 정준분해를 할 것인지\n",
    "    - max_len: 최대 길이\n",
    "- 아래 속성을 정의한다.\n",
    "    - 사전\n",
    "    - 역방향 사전\n",
    "    - `BasicTokenizer`\n",
    "    - `WordpieceTokenizer`\n",
    "    - 최대 길이\n",
    "- 하나씩 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBRARIES\n",
    "\n",
    "import collections       # OrderedDict를 위해 호출\n",
    "import re                # 정규표현식\n",
    "import unicodedata       # 한국어 정준분해 및 문자열 확인\n",
    "import six               # Python version 체크\n",
    "import tensorflow as tf  # Tensorflow 파일 불러오기 및 logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGUMENTS\n",
    "\n",
    "# ETRI에서 받은 file path를 저장\n",
    "path2 = '../KorBERT/2_bert_download_002_bert_morp_tensorflow/002_bert_morp_tensorflow/'\n",
    "path4 = '../KorBERT/4_bert_download_004_bert_eojeol_tensorflow/004_bert_eojeol_tensorflow/'\n",
    "# 한국어 vocab 사전을 등록\n",
    "morph_vocab_file = path2 + 'vocab.korean_morp.list'\n",
    "rawtext_vocab_file = path4 + 'vocab.korean.rawtext.list'\n",
    "\n",
    "do_lower_case = True # default=False, 정준분해 예시를 위해 True로 설정\n",
    "max_len = None # 없으면 1e12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 사전 호출\n",
    "\n",
    "def convert_to_unicode(text):\n",
    "    # Python version이 3.x일 때,\n",
    "    # type(text)이 `bytes`일 경우, utf-8로 변환\n",
    "    if six.PY3:\n",
    "        if isinstance(text, str):\n",
    "            return text\n",
    "        elif isinstance(text, bytes):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    # Python version이 2.x일 때,\n",
    "    # type(text)이 `str`일 경우, utf-8로 변환\n",
    "    elif six.PY2:\n",
    "        if isinstance(text, str):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        elif isinstance(text, unicode):\n",
    "            return text\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    # Python 3.x, 2.x만 허용!\n",
    "    else:\n",
    "        raise ValueError(\"Not running on Python2 or Python 3?\")\n",
    "        \n",
    "        \n",
    "def _load_vocab(vocab_file):\n",
    "    # 단어 사전을 저장할 OrderedDict 객체 생성\n",
    "    vocab = collections.OrderedDict()\n",
    "    index = 0\n",
    "    with tf.io.gfile.GFile(vocab_file, 'r') as reader:\n",
    "        while True:\n",
    "            # Binary Text를 unicode(utf-8)로 decode.\n",
    "            token = convert_to_unicode(reader.readline())\n",
    "            if not token: break\n",
    "            if ((token.find('n_iters=') == 0) or\n",
    "                (token.find('max_length=') == 0)):\n",
    "                continue\n",
    "            token = token.split('\\t')[0]\n",
    "            token = token.strip()\n",
    "            # 토큰과 해당 index를 기록\n",
    "            vocab[token] = index\n",
    "            index += 1\n",
    "    return vocab\n",
    "\n",
    "# 단어 사전 호출\n",
    "morph_vocab = _load_vocab(morph_vocab_file)\n",
    "rawtext_vocab = _load_vocab(rawtext_vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[PAD]': 0,\n",
       " '[UNK]': 1,\n",
       " '[CLS]': 2,\n",
       " '[SEP]': 3,\n",
       " '[MASK]': 4,\n",
       " '<S>': 5,\n",
       " '<T>': 6,\n",
       " './SF_': 7,\n",
       " '다/EF_': 8,\n",
       " '하/XSV_': 9,\n",
       " 'ㄴ/ETM_': 10,\n",
       " '을/JKO_': 11,\n",
       " '었/EP_': 12,\n",
       " '의/JKG_': 13,\n",
       " '에/JKB_': 14,\n",
       " '이/VCP_': 15,\n",
       " '이/JKS_': 16,\n",
       " ',/SP_': 17,\n",
       " '는/JX_': 18,\n",
       " '를/JKO_': 19,\n",
       " '어/EC_': 20,\n",
       " '은/JX_': 21,\n",
       " '는/ETM_': 22,\n",
       " '고/EC_': 23,\n",
       " '가/JKS_': 24,\n",
       " '\"/SS_': 25,\n",
       " \"'/SS_\": 26,\n",
       " '에서/JKB_': 27,\n",
       " '으로/JKB_': 28,\n",
       " '(/SS_': 29,\n",
       " ')/SS_': 30,\n",
       " '로/JKB_': 31,\n",
       " '되/XSV_': 32,\n",
       " '것/NNB_': 33,\n",
       " '도/JX_': 34,\n",
       " 'ㄹ/ETM_': 35,\n",
       " '들/XSN_': 36,\n",
       " '있/VX_': 37,\n",
       " '있/VA_': 38,\n",
       " '년/NNB_': 39,\n",
       " '하/VV_': 40,\n",
       " 'ㄴ다/EF_': 41,\n",
       " '하/XSA_': 42,\n",
       " '았/EP_': 43,\n",
       " '일/NNB_': 44,\n",
       " '은/ETM_': 45,\n",
       " '과/JC_': 46,\n",
       " '게/EC_': 47,\n",
       " '지/EC_': 48,\n",
       " '기/ETN_': 49,\n",
       " '1/SN_': 50,\n",
       " '등/NNB_': 51,\n",
       " '자/XSN_': 52,\n",
       " '며/EC_': 53,\n",
       " '2/SN_': 54,\n",
       " '수/NNB_': 55,\n",
       " '와/JC_': 56,\n",
       " '되/VV_': 57,\n",
       " '적/XSN_': 58,\n",
       " '않/VX_': 59,\n",
       " '월/NNB_': 60,\n",
       " '하/VX_': 61,\n",
       " '아/EC_': 62,\n",
       " '3/SN_': 63,\n",
       " '고/JKQ_': 64,\n",
       " '‘/SS_': 65,\n",
       " '’/SS_': 66,\n",
       " '“/SS_': 67,\n",
       " '던/ETM_': 68,\n",
       " '”/SS_': 69,\n",
       " '없/VA_': 70,\n",
       " '면/EC_': 71,\n",
       " '말/NNG_': 72,\n",
       " '대하/VV_': 73,\n",
       " '지만/EC_': 74,\n",
       " '·/SP_': 75,\n",
       " '에게/JKB_': 76,\n",
       " '이/NP_': 77,\n",
       " '받/VV_': 78,\n",
       " '까지/JX_': 79,\n",
       " '이/MM_': 80,\n",
       " '%/SW_': 81,\n",
       " '4/SN_': 82,\n",
       " '/NNG_': 83,\n",
       " '과/JKB_': 84,\n",
       " '만/NR_': 85,\n",
       " '원/NNB_': 86,\n",
       " '명/NNB_': 87,\n",
       " '면서/EC_': 88,\n",
       " '다는/ETM_': 89,\n",
       " '그/NP_': 90,\n",
       " '5/SN_': 91,\n",
       " '한/MM_': 92,\n",
       " '을/ETM_': 93,\n",
       " '어서/EC_': 94,\n",
       " '-/SS_': 95,\n",
       " '다고/EC_': 96,\n",
       " '위하/VV_': 97,\n",
       " '만/JX_': 98,\n",
       " '중/NNB_': 99}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 100개만 출력해보기\n",
    "# 뒤의 품사에 `_`가 붙은 것을 잘 기억해두기\n",
    "{key:value for i, (key, value) in enumerate(morph_vocab.items()) if i < 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[PAD]': 0,\n",
       " '[UNK]': 1,\n",
       " '[CLS]': 2,\n",
       " '[SEP]': 3,\n",
       " '[MASK]': 4,\n",
       " '<S>': 5,\n",
       " '<T>': 6,\n",
       " '._': 7,\n",
       " ',_': 8,\n",
       " '_': 9,\n",
       " '이_': 10,\n",
       " '의_': 11,\n",
       " '을_': 12,\n",
       " '에_': 13,\n",
       " '\"': 14,\n",
       " '(': 15,\n",
       " '한_': 16,\n",
       " \"'\": 17,\n",
       " '은_': 18,\n",
       " ')': 19,\n",
       " '이': 20,\n",
       " '는_': 21,\n",
       " '에서_': 22,\n",
       " '고_': 23,\n",
       " '했다': 24,\n",
       " '가_': 25,\n",
       " '로_': 26,\n",
       " '지': 27,\n",
       " '있다': 28,\n",
       " '도_': 29,\n",
       " '과_': 30,\n",
       " '으로_': 31,\n",
       " '를_': 32,\n",
       " '다': 33,\n",
       " '하는_': 34,\n",
       " '사': 35,\n",
       " '시': 36,\n",
       " '기': 37,\n",
       " '대': 38,\n",
       " '고': 39,\n",
       " '수': 40,\n",
       " '가': 41,\n",
       " '.': 42,\n",
       " '수_': 43,\n",
       " '전': 44,\n",
       " '주': 45,\n",
       " '일': 46,\n",
       " '리': 47,\n",
       " '자': 48,\n",
       " '정': 49,\n",
       " '할_': 50,\n",
       " '인': 51,\n",
       " '1': 52,\n",
       " '아': 53,\n",
       " '와_': 54,\n",
       " '부': 55,\n",
       " '스': 56,\n",
       " '인_': 57,\n",
       " '하고_': 58,\n",
       " '해': 59,\n",
       " '보': 60,\n",
       " '유': 61,\n",
       " '어': 62,\n",
       " '이다': 63,\n",
       " '상': 64,\n",
       " '2': 65,\n",
       " ')_': 66,\n",
       " '신': 67,\n",
       " '원': 68,\n",
       " '무': 69,\n",
       " '장': 70,\n",
       " '3': 71,\n",
       " '마': 72,\n",
       " '비': 73,\n",
       " '조': 74,\n",
       " '동': 75,\n",
       " '제': 76,\n",
       " '로': 77,\n",
       " '해_': 78,\n",
       " '소': 79,\n",
       " '성': 80,\n",
       " '도': 81,\n",
       " '지_': 82,\n",
       " '세': 83,\n",
       " '‘': 84,\n",
       " '나': 85,\n",
       " '오': 86,\n",
       " '미': 87,\n",
       " '“': 88,\n",
       " '공': 89,\n",
       " '하': 90,\n",
       " '연': 91,\n",
       " '있는_': 92,\n",
       " '구': 93,\n",
       " '라': 94,\n",
       " '재': 95,\n",
       " '한': 96,\n",
       " '여': 97,\n",
       " '5': 98,\n",
       " '4': 99}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 100개만 출력해보기\n",
    "# 뒤의 품사에 `_`가 붙은 것을 잘 기억해두기\n",
    "{key:value for i, (key, value) in enumerate(rawtext_vocab.items()) if i < 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30797, 30349)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rawtext_vocab), len(morph_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-248586d9b49b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 역방향 사전 정의\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# 근데 결국 사용안하드라\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0minv_vocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "# 역방향 사전 정의\n",
    "# 근데 결국 사용안하드라\n",
    "inv_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_len을 사용자가 넣지 않았을 경우\n",
    "# 1000000000000을 상한으로 함\n",
    "# 사실상 무한대지 뭐.\n",
    "max_len = max_len if max_len is not None else int(1e12)\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'심사숙고/NNG 하/XSV 였/EP 겠/EP 지만/EC 참으로/MAG 유감/NNG 이/JX 야/EC'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input Text\n",
    "text = '심사숙고했겠지만 참으로 유감이야' # 예시 text 생성\n",
    "# 형태소 분석을 아래와 같이 실시했다고 가정하자.\n",
    "# 형태소 분석 API부분은 `convert_single_example`함수에서 다시 다룰게요.\n",
    "text = '심사숙고/NNG + 하/XSV + 였/EP + 겠/EP + 지만/EC + 참으로/MAG + 유감/NNG + 이/JX + 야/EC'\n",
    "# ETRI에서 정의한 대로 input을 만들어줍시다.\n",
    "text = text.replace(' + ', ' ')\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `BERTTokenizer.tokenize()`\n",
    "- 아래와 같이 동작한다.\n",
    "    ```python\n",
    "    def tokenize(self, text):\n",
    "        split_tokens = []\n",
    "        # End to End Tokenizing.\n",
    "        for token in self.basic_tokenizer.tokenize(text):\n",
    "            # ETRI Vocab 양식에 맞게 token 끝에 '_'를 붙여준다.\n",
    "            token += '_'\n",
    "            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
    "                split_tokens.append(sub_token)\n",
    "        return split_tokens\n",
    "    ```\n",
    "- 여기서 `BasicTokenizer`와 `WordpieceTokenizer`를 정의하지 않고 어떻게 동작하는지 그 흐름대로 살펴보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_TOKENS = [] # 최종적으로 return할 list 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `BasicTokenizer.tokenize()`\n",
    "- 아래와 같이 동작한다.\n",
    "    ```python\n",
    "    def tokenize(self, text):\n",
    "        text = convert_to_unicode(text) #1\n",
    "        text = self._clean_text(text)   #2\n",
    "\n",
    "        orig_tokens = whitespace_tokenize(text) #3\n",
    "        split_tokens = []\n",
    "        for token in orig_tokens:\n",
    "            if self.do_lower_case:\n",
    "                # 현재 input으로 '고객/NNG'와 같이 Part-of-speech가 이미\n",
    "                # tagging되어있고 vocab은 '고객/NNG_'로 단어를 기록하고 있음.\n",
    "                # 여기서 `lower` 메서드를 사용하면 뒤의 tagging이 소문자로\n",
    "                # 변환되어 값의 비교를 못하게 되므로 이를 주석처리.\n",
    "\n",
    "                # token.lower()\n",
    "\n",
    "                # 모든 음절을 정준 분해시키는 함수\n",
    "                token = self._run_strip_accents(token) #4\n",
    "            split_tokens.extend(self._run_split_on_punc(token)) #5\n",
    "        output_tokens = whitespace_tokenize(\" \".join(split_tokens)) #6\n",
    "        return output_tokens\n",
    "    ```\n",
    "- 순서대로 보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'심사숙고/NNG 하/XSV 였/EP 겠/EP 지만/EC 참으로/MAG 유감/NNG 이/JX 야/EC'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. unicode 변환\n",
    "text = convert_to_unicode(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize하면서 계속 사용된 character 단위 함수 정의\n",
    "def _is_control(char):\n",
    "    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        # 개행문자이면 False 반환\n",
    "        return False\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat.startswith(\"C\"):\n",
    "        # unicode category가\n",
    "        # Cc(Control)\n",
    "        # Cf(format)\n",
    "        # Co(Private Use, is 0)\n",
    "        # Cs(Surrrogate, is 0)일 경우, True 반환\n",
    "        # https://en.wikipedia.org/wiki/Control_character\n",
    "        return True\n",
    "    # 이 외의 경우 전부 False 반환\n",
    "    return False\n",
    "    \n",
    "def _is_whitespace(char):\n",
    "    if char == \" \" or char == '\\t' or char == '\\n' or char == '\\r':\n",
    "        # 개행문자이거나 띄어쓰기면 True 반환\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat == 'Zs':\n",
    "        # unicode category가 Space Seperator면 True 반환\n",
    "        # https://www.compart.com/en/unicode/category/Zs\n",
    "        return True\n",
    "    # 이 외의 경우 전부 False 반환\n",
    "    return False\n",
    "\n",
    "def _is_punctuation(char):\n",
    "    # 한국어 형태소 분석기이기 때문에 공백과 같은지 여부만 반환\n",
    "    return char == ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'심사숙고/NNG 하/XSV 였/EP 겠/EP 지만/EC 참으로/MAG 유감/NNG 이/JX 야/EC'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. text cleaning\n",
    "def _clean_text(text):\n",
    "    output = [] # char을 저장할 list 생성\n",
    "    for char in text:\n",
    "        # 텍스트에서 Char 단위로 출력\n",
    "        cp = ord(char)\n",
    "        if cp == 0 or cp == 0xfffd or _is_control(char):\n",
    "            # \\x00이거나 �이거나 unicode cat.이 C로 시작할 경우\n",
    "            # (개행문자 제외) output에 추가하지 않는다.\n",
    "            continue\n",
    "        if _is_whitespace(char):\n",
    "            # 공백일 경우 \" \"으로 output에 추가\n",
    "            output.append(\" \")\n",
    "        else:\n",
    "            # 이 외의 경우 전부 output에 추가\n",
    "            output.append(char)\n",
    "    # cleaning 작업을 거친 Text를 후처리하여 반환\n",
    "    return \"\".join(output)\n",
    "\n",
    "_clean_text(text) # 뭐가 변했을까? 안변한거 같지>?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'심사숙고했겠지만 참으로 유감이야'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 더러운 text가 있다고 생각해봐.\n",
    "dirty_text = '심사\\x00숙고했겠�지만 참으로 유감이야'\n",
    "_clean_text(dirty_text) # 이렇게 바꿔준다.\n",
    "                  # 영어 BERT에는 중국어 변환, 기타 unicode도 신경쓰는데\n",
    "                  # ETRI에서 이렇게 수정해서 코드를 배포했으니 잘 사용하도록 하자!\n",
    "                  # 아니 근데 생각해보니까 이거 형태소 분석 전에 실시해야하는거 아니야?\n",
    "                  # 코드 다시 짜는거 생각해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['심사숙고/NNG',\n",
       " '하/XSV',\n",
       " '였/EP',\n",
       " '겠/EP',\n",
       " '지만/EC',\n",
       " '참으로/MAG',\n",
       " '유감/NNG',\n",
       " '이/JX',\n",
       " '야/EC']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3. whitespacing(띄어쓰기로 token화)\n",
    "# text 단위 공백 처리\n",
    "def whitespace_tokenize(text):\n",
    "    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
    "    text = text.strip() # 양 사이드의 공백을 제거\n",
    "    if not text: # 어떠한 값도 없을 시, 빈 list를 반환\n",
    "        return []\n",
    "    tokens = text.split() # 공백 단위로 쪼갠 list를 반환\n",
    "    return tokens\n",
    "\n",
    "orig_tokens = whitespace_tokenize(text)\n",
    "orig_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'심사숙고/NNG'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_tokens = []\n",
    "# >>> 첫 번째 for loop\n",
    "token = orig_tokens[0]\n",
    "token # for loop 돌리기 전에 어떻게 돌아가는지 체크\n",
    "# token = '심사 숙고'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "심사숙고/NNG >> ['ᄉ', 'ᅵ', 'ᆷ', 'ᄉ', 'ᅡ', 'ᄉ', 'ᅮ', 'ᆨ', 'ᄀ', 'ᅩ', '/', 'N', 'N', 'G']\n",
      "print(token) == 심사숙고/NNG (사실 출력시에는 변화 X)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'심사숙고/NNG'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4. 음절을 정준분해\n",
    "print(token, end=' >> ')\n",
    "token = unicodedata.normalize(\"NFD\", token)\n",
    "print(list(token))\n",
    "print('print(token) ==', token, '(사실 출력시에는 변화 X)')\n",
    "# https://gist.github.com/Pusnow/aa865fa21f9557fa58d691a8b79f8a6d\n",
    "# 모든 음절을 정준 분해(Canonical Decomposition)시킴\n",
    "# '각'을 'ㄱ+ㅏ+ㄱ'으로 저장(출력되는 값은 동일)\n",
    "output = []\n",
    "for char in token:\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat == \"Mn\":\n",
    "        # unicode category가 \"Mark, Nonspacing\"일 경우 pass\n",
    "        continue\n",
    "    output.append(char)\n",
    "token = ''.join(output)\n",
    "token # if문에 해당하는 char가 없었기에 원본 text를 출력\n",
    "      # 정준분해된 상태임을 기억해라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ᄉ', 'ᅵ', 'ᆷ', 'ᄉ', 'ᅡ', 'ᄉ', 'ᅮ', 'ᆨ', 'ᄀ', 'ᅩ', '/', 'N', 'N', 'G']\n"
     ]
    }
   ],
   "source": [
    "#5. punctuation 구분(사실상 의미가 없다)\n",
    "chars = list(token)\n",
    "i, start_new_word = 0, True\n",
    "output = []\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char\t_is_puntuation(char)\tstart_new_word\tOutput\n",
      "ᄉ\tFalse\t\t\tFalse\t\t[['ᄉ']]\n",
      "ᅵ\tFalse\t\t\tFalse\t\t[['ᄉ', 'ᅵ']]\n",
      "ᆷ\tFalse\t\t\tFalse\t\t[['ᄉ', 'ᅵ', 'ᆷ']]\n",
      "ᄉ\tFalse\t\t\tFalse\t\t[['ᄉ', 'ᅵ', 'ᆷ', 'ᄉ']]\n",
      "ᅡ\tFalse\t\t\tFalse\t\t[['ᄉ', 'ᅵ', 'ᆷ', 'ᄉ', 'ᅡ']]\n",
      "ᄉ\tFalse\t\t\tFalse\t\t[['ᄉ', 'ᅵ', 'ᆷ', 'ᄉ', 'ᅡ', 'ᄉ']]\n",
      "ᅮ\tFalse\t\t\tFalse\t\t[['ᄉ', 'ᅵ', 'ᆷ', 'ᄉ', 'ᅡ', 'ᄉ', 'ᅮ']]\n",
      "ᆨ\tFalse\t\t\tFalse\t\t[['ᄉ', 'ᅵ', 'ᆷ', 'ᄉ', 'ᅡ', 'ᄉ', 'ᅮ', 'ᆨ']]\n",
      "ᄀ\tFalse\t\t\tFalse\t\t[['ᄉ', 'ᅵ', 'ᆷ', 'ᄉ', 'ᅡ', 'ᄉ', 'ᅮ', 'ᆨ', 'ᄀ']]\n",
      "ᅩ\tFalse\t\t\tFalse\t\t[['ᄉ', 'ᅵ', 'ᆷ', 'ᄉ', 'ᅡ', 'ᄉ', 'ᅮ', 'ᆨ', 'ᄀ', 'ᅩ']]\n",
      "/\tFalse\t\t\tFalse\t\t[['ᄉ', 'ᅵ', 'ᆷ', 'ᄉ', 'ᅡ', 'ᄉ', 'ᅮ', 'ᆨ', 'ᄀ', 'ᅩ', '/']]\n",
      "N\tFalse\t\t\tFalse\t\t[['ᄉ', 'ᅵ', 'ᆷ', 'ᄉ', 'ᅡ', 'ᄉ', 'ᅮ', 'ᆨ', 'ᄀ', 'ᅩ', '/', 'N']]\n",
      "N\tFalse\t\t\tFalse\t\t[['ᄉ', 'ᅵ', 'ᆷ', 'ᄉ', 'ᅡ', 'ᄉ', 'ᅮ', 'ᆨ', 'ᄀ', 'ᅩ', '/', 'N', 'N']]\n",
      "G\tFalse\t\t\tFalse\t\t[['ᄉ', 'ᅵ', 'ᆷ', 'ᄉ', 'ᅡ', 'ᄉ', 'ᅮ', 'ᆨ', 'ᄀ', 'ᅩ', '/', 'N', 'N', 'G']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['심사숙고/NNG']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('char\\t_is_puntuation(char)\\tstart_new_word\\tOutput')\n",
    "while i < len(chars):\n",
    "    char = chars[i]\n",
    "    print(char, end='\\t')\n",
    "    print(_is_punctuation(char), end='\\t\\t\\t')\n",
    "    if _is_punctuation(char):\n",
    "        print('In Here!! <CODE BLOCK \"IF _IS_PUNCTUATOIN\">')\n",
    "        output.append([char])\n",
    "        start_new_word = True\n",
    "    else:\n",
    "        if start_new_word:\n",
    "            output.append([])\n",
    "        start_new_word = False\n",
    "        output[-1].append(char)\n",
    "    print(start_new_word, end='\\t\\t')\n",
    "    print(output)\n",
    "    i += 1\n",
    "[\"\".join(x) for x in output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "심사숙고/NNG >> ['ᄉ', 'ᅵ', 'ᆷ', 'ᄉ', 'ᅡ', 'ᄉ', 'ᅮ', 'ᆨ', 'ᄀ', 'ᅩ', '/', 'N', 'N', 'G']\n",
      "print(token) == 심사숙고/NNG (사실 출력시에는 변화 X)\n",
      "['ᄉ', 'ᅵ', 'ᆷ', 'ᄉ', 'ᅡ', 'ᄉ', 'ᅮ', 'ᆨ', 'ᄀ', 'ᅩ', '/', 'N', 'N', 'G']\n",
      "char\t_is_puntuation(char)\tstart_new_word\tOutput\n",
      "ᄉ\tFalse\t\t\tFalse\t\t[['ᄉ']]\n",
      "ᅵ\tFalse\t\t\tFalse\t\t[['ᄉ', 'ᅵ']]\n",
      "ᆷ\tFalse\t\t\tFalse\t\t[['ᄉ', 'ᅵ', 'ᆷ']]\n",
      "ᄉ\tFalse\t\t\tFalse\t\t[['ᄉ', 'ᅵ', 'ᆷ', 'ᄉ']]\n",
      "ᅡ\tFalse\t\t\tFalse\t\t[['ᄉ', 'ᅵ', 'ᆷ', 'ᄉ', 'ᅡ']]\n",
      "ᄉ\tFalse\t\t\tFalse\t\t[['ᄉ', 'ᅵ', 'ᆷ', 'ᄉ', 'ᅡ', 'ᄉ']]\n",
      "ᅮ\tFalse\t\t\tFalse\t\t[['ᄉ', 'ᅵ', 'ᆷ', 'ᄉ', 'ᅡ', 'ᄉ', 'ᅮ']]\n",
      "ᆨ\tFalse\t\t\tFalse\t\t[['ᄉ', 'ᅵ', 'ᆷ', 'ᄉ', 'ᅡ', 'ᄉ', 'ᅮ', 'ᆨ']]\n",
      "ᄀ\tFalse\t\t\tFalse\t\t[['ᄉ', 'ᅵ', 'ᆷ', 'ᄉ', 'ᅡ', 'ᄉ', 'ᅮ', 'ᆨ', 'ᄀ']]\n",
      "ᅩ\tFalse\t\t\tFalse\t\t[['ᄉ', 'ᅵ', 'ᆷ', 'ᄉ', 'ᅡ', 'ᄉ', 'ᅮ', 'ᆨ', 'ᄀ', 'ᅩ']]\n",
      "/\tFalse\t\t\tFalse\t\t[['ᄉ', 'ᅵ', 'ᆷ', 'ᄉ', 'ᅡ', 'ᄉ', 'ᅮ', 'ᆨ', 'ᄀ', 'ᅩ', '/']]\n",
      "N\tFalse\t\t\tFalse\t\t[['ᄉ', 'ᅵ', 'ᆷ', 'ᄉ', 'ᅡ', 'ᄉ', 'ᅮ', 'ᆨ', 'ᄀ', 'ᅩ', '/', 'N']]\n",
      "N\tFalse\t\t\tFalse\t\t[['ᄉ', 'ᅵ', 'ᆷ', 'ᄉ', 'ᅡ', 'ᄉ', 'ᅮ', 'ᆨ', 'ᄀ', 'ᅩ', '/', 'N', 'N']]\n",
      "G\tFalse\t\t\tFalse\t\t[['ᄉ', 'ᅵ', 'ᆷ', 'ᄉ', 'ᅡ', 'ᄉ', 'ᅮ', 'ᆨ', 'ᄀ', 'ᅩ', '/', 'N', 'N', 'G']]\n",
      "하/XSV >> ['ᄒ', 'ᅡ', '/', 'X', 'S', 'V']\n",
      "print(token) == 하/XSV (사실 출력시에는 변화 X)\n",
      "['ᄒ', 'ᅡ', '/', 'X', 'S', 'V']\n",
      "char\t_is_puntuation(char)\tstart_new_word\tOutput\n",
      "ᄒ\tFalse\t\t\tFalse\t\t[['ᄒ']]\n",
      "ᅡ\tFalse\t\t\tFalse\t\t[['ᄒ', 'ᅡ']]\n",
      "/\tFalse\t\t\tFalse\t\t[['ᄒ', 'ᅡ', '/']]\n",
      "X\tFalse\t\t\tFalse\t\t[['ᄒ', 'ᅡ', '/', 'X']]\n",
      "S\tFalse\t\t\tFalse\t\t[['ᄒ', 'ᅡ', '/', 'X', 'S']]\n",
      "V\tFalse\t\t\tFalse\t\t[['ᄒ', 'ᅡ', '/', 'X', 'S', 'V']]\n",
      "였/EP >> ['ᄋ', 'ᅧ', 'ᆻ', '/', 'E', 'P']\n",
      "print(token) == 였/EP (사실 출력시에는 변화 X)\n",
      "['ᄋ', 'ᅧ', 'ᆻ', '/', 'E', 'P']\n",
      "char\t_is_puntuation(char)\tstart_new_word\tOutput\n",
      "ᄋ\tFalse\t\t\tFalse\t\t[['ᄋ']]\n",
      "ᅧ\tFalse\t\t\tFalse\t\t[['ᄋ', 'ᅧ']]\n",
      "ᆻ\tFalse\t\t\tFalse\t\t[['ᄋ', 'ᅧ', 'ᆻ']]\n",
      "/\tFalse\t\t\tFalse\t\t[['ᄋ', 'ᅧ', 'ᆻ', '/']]\n",
      "E\tFalse\t\t\tFalse\t\t[['ᄋ', 'ᅧ', 'ᆻ', '/', 'E']]\n",
      "P\tFalse\t\t\tFalse\t\t[['ᄋ', 'ᅧ', 'ᆻ', '/', 'E', 'P']]\n",
      "겠/EP >> ['ᄀ', 'ᅦ', 'ᆻ', '/', 'E', 'P']\n",
      "print(token) == 겠/EP (사실 출력시에는 변화 X)\n",
      "['ᄀ', 'ᅦ', 'ᆻ', '/', 'E', 'P']\n",
      "char\t_is_puntuation(char)\tstart_new_word\tOutput\n",
      "ᄀ\tFalse\t\t\tFalse\t\t[['ᄀ']]\n",
      "ᅦ\tFalse\t\t\tFalse\t\t[['ᄀ', 'ᅦ']]\n",
      "ᆻ\tFalse\t\t\tFalse\t\t[['ᄀ', 'ᅦ', 'ᆻ']]\n",
      "/\tFalse\t\t\tFalse\t\t[['ᄀ', 'ᅦ', 'ᆻ', '/']]\n",
      "E\tFalse\t\t\tFalse\t\t[['ᄀ', 'ᅦ', 'ᆻ', '/', 'E']]\n",
      "P\tFalse\t\t\tFalse\t\t[['ᄀ', 'ᅦ', 'ᆻ', '/', 'E', 'P']]\n",
      "지만/EC >> ['ᄌ', 'ᅵ', 'ᄆ', 'ᅡ', 'ᆫ', '/', 'E', 'C']\n",
      "print(token) == 지만/EC (사실 출력시에는 변화 X)\n",
      "['ᄌ', 'ᅵ', 'ᄆ', 'ᅡ', 'ᆫ', '/', 'E', 'C']\n",
      "char\t_is_puntuation(char)\tstart_new_word\tOutput\n",
      "ᄌ\tFalse\t\t\tFalse\t\t[['ᄌ']]\n",
      "ᅵ\tFalse\t\t\tFalse\t\t[['ᄌ', 'ᅵ']]\n",
      "ᄆ\tFalse\t\t\tFalse\t\t[['ᄌ', 'ᅵ', 'ᄆ']]\n",
      "ᅡ\tFalse\t\t\tFalse\t\t[['ᄌ', 'ᅵ', 'ᄆ', 'ᅡ']]\n",
      "ᆫ\tFalse\t\t\tFalse\t\t[['ᄌ', 'ᅵ', 'ᄆ', 'ᅡ', 'ᆫ']]\n",
      "/\tFalse\t\t\tFalse\t\t[['ᄌ', 'ᅵ', 'ᄆ', 'ᅡ', 'ᆫ', '/']]\n",
      "E\tFalse\t\t\tFalse\t\t[['ᄌ', 'ᅵ', 'ᄆ', 'ᅡ', 'ᆫ', '/', 'E']]\n",
      "C\tFalse\t\t\tFalse\t\t[['ᄌ', 'ᅵ', 'ᄆ', 'ᅡ', 'ᆫ', '/', 'E', 'C']]\n",
      "참으로/MAG >> ['ᄎ', 'ᅡ', 'ᆷ', 'ᄋ', 'ᅳ', 'ᄅ', 'ᅩ', '/', 'M', 'A', 'G']\n",
      "print(token) == 참으로/MAG (사실 출력시에는 변화 X)\n",
      "['ᄎ', 'ᅡ', 'ᆷ', 'ᄋ', 'ᅳ', 'ᄅ', 'ᅩ', '/', 'M', 'A', 'G']\n",
      "char\t_is_puntuation(char)\tstart_new_word\tOutput\n",
      "ᄎ\tFalse\t\t\tFalse\t\t[['ᄎ']]\n",
      "ᅡ\tFalse\t\t\tFalse\t\t[['ᄎ', 'ᅡ']]\n",
      "ᆷ\tFalse\t\t\tFalse\t\t[['ᄎ', 'ᅡ', 'ᆷ']]\n",
      "ᄋ\tFalse\t\t\tFalse\t\t[['ᄎ', 'ᅡ', 'ᆷ', 'ᄋ']]\n",
      "ᅳ\tFalse\t\t\tFalse\t\t[['ᄎ', 'ᅡ', 'ᆷ', 'ᄋ', 'ᅳ']]\n",
      "ᄅ\tFalse\t\t\tFalse\t\t[['ᄎ', 'ᅡ', 'ᆷ', 'ᄋ', 'ᅳ', 'ᄅ']]\n",
      "ᅩ\tFalse\t\t\tFalse\t\t[['ᄎ', 'ᅡ', 'ᆷ', 'ᄋ', 'ᅳ', 'ᄅ', 'ᅩ']]\n",
      "/\tFalse\t\t\tFalse\t\t[['ᄎ', 'ᅡ', 'ᆷ', 'ᄋ', 'ᅳ', 'ᄅ', 'ᅩ', '/']]\n",
      "M\tFalse\t\t\tFalse\t\t[['ᄎ', 'ᅡ', 'ᆷ', 'ᄋ', 'ᅳ', 'ᄅ', 'ᅩ', '/', 'M']]\n",
      "A\tFalse\t\t\tFalse\t\t[['ᄎ', 'ᅡ', 'ᆷ', 'ᄋ', 'ᅳ', 'ᄅ', 'ᅩ', '/', 'M', 'A']]\n",
      "G\tFalse\t\t\tFalse\t\t[['ᄎ', 'ᅡ', 'ᆷ', 'ᄋ', 'ᅳ', 'ᄅ', 'ᅩ', '/', 'M', 'A', 'G']]\n",
      "유감/NNG >> ['ᄋ', 'ᅲ', 'ᄀ', 'ᅡ', 'ᆷ', '/', 'N', 'N', 'G']\n",
      "print(token) == 유감/NNG (사실 출력시에는 변화 X)\n",
      "['ᄋ', 'ᅲ', 'ᄀ', 'ᅡ', 'ᆷ', '/', 'N', 'N', 'G']\n",
      "char\t_is_puntuation(char)\tstart_new_word\tOutput\n",
      "ᄋ\tFalse\t\t\tFalse\t\t[['ᄋ']]\n",
      "ᅲ\tFalse\t\t\tFalse\t\t[['ᄋ', 'ᅲ']]\n",
      "ᄀ\tFalse\t\t\tFalse\t\t[['ᄋ', 'ᅲ', 'ᄀ']]\n",
      "ᅡ\tFalse\t\t\tFalse\t\t[['ᄋ', 'ᅲ', 'ᄀ', 'ᅡ']]\n",
      "ᆷ\tFalse\t\t\tFalse\t\t[['ᄋ', 'ᅲ', 'ᄀ', 'ᅡ', 'ᆷ']]\n",
      "/\tFalse\t\t\tFalse\t\t[['ᄋ', 'ᅲ', 'ᄀ', 'ᅡ', 'ᆷ', '/']]\n",
      "N\tFalse\t\t\tFalse\t\t[['ᄋ', 'ᅲ', 'ᄀ', 'ᅡ', 'ᆷ', '/', 'N']]\n",
      "N\tFalse\t\t\tFalse\t\t[['ᄋ', 'ᅲ', 'ᄀ', 'ᅡ', 'ᆷ', '/', 'N', 'N']]\n",
      "G\tFalse\t\t\tFalse\t\t[['ᄋ', 'ᅲ', 'ᄀ', 'ᅡ', 'ᆷ', '/', 'N', 'N', 'G']]\n",
      "이/JX >> ['ᄋ', 'ᅵ', '/', 'J', 'X']\n",
      "print(token) == 이/JX (사실 출력시에는 변화 X)\n",
      "['ᄋ', 'ᅵ', '/', 'J', 'X']\n",
      "char\t_is_puntuation(char)\tstart_new_word\tOutput\n",
      "ᄋ\tFalse\t\t\tFalse\t\t[['ᄋ']]\n",
      "ᅵ\tFalse\t\t\tFalse\t\t[['ᄋ', 'ᅵ']]\n",
      "/\tFalse\t\t\tFalse\t\t[['ᄋ', 'ᅵ', '/']]\n",
      "J\tFalse\t\t\tFalse\t\t[['ᄋ', 'ᅵ', '/', 'J']]\n",
      "X\tFalse\t\t\tFalse\t\t[['ᄋ', 'ᅵ', '/', 'J', 'X']]\n",
      "야/EC >> ['ᄋ', 'ᅣ', '/', 'E', 'C']\n",
      "print(token) == 야/EC (사실 출력시에는 변화 X)\n",
      "['ᄋ', 'ᅣ', '/', 'E', 'C']\n",
      "char\t_is_puntuation(char)\tstart_new_word\tOutput\n",
      "ᄋ\tFalse\t\t\tFalse\t\t[['ᄋ']]\n",
      "ᅣ\tFalse\t\t\tFalse\t\t[['ᄋ', 'ᅣ']]\n",
      "/\tFalse\t\t\tFalse\t\t[['ᄋ', 'ᅣ', '/']]\n",
      "E\tFalse\t\t\tFalse\t\t[['ᄋ', 'ᅣ', '/', 'E']]\n",
      "C\tFalse\t\t\tFalse\t\t[['ᄋ', 'ᅣ', '/', 'E', 'C']]\n",
      "split_tokens: ['심사숙고/NNG', '하/XSV', '였/EP', '겠/EP', '지만/EC', '참으로/MAG', '유감/NNG', '이/JX', '야/EC']\n",
      "output_tokens: ['심사숙고/NNG', '하/XSV', '였/EP', '겠/EP', '지만/EC', '참으로/MAG', '유감/NNG', '이/JX', '야/EC']\n"
     ]
    }
   ],
   "source": [
    "# for loop을 적용하면 아래와 같이 된다.\n",
    "split_tokens = []\n",
    "for token in orig_tokens:\n",
    "    #4. 음절을 정준분해\n",
    "    print(token, end=' >> ')\n",
    "    token = unicodedata.normalize(\"NFD\", token)\n",
    "    print(list(token))\n",
    "    print('print(token) ==', token, '(사실 출력시에는 변화 X)')\n",
    "    # https://gist.github.com/Pusnow/aa865fa21f9557fa58d691a8b79f8a6d\n",
    "    # 모든 음절을 정준 분해(Canonical Decomposition)시킴\n",
    "    # '각'을 'ㄱ+ㅏ+ㄱ'으로 저장(출력되는 값은 동일)\n",
    "    output = []\n",
    "    for char in token:\n",
    "        cat = unicodedata.category(char)\n",
    "        if cat == \"Mn\":\n",
    "            # unicode category가 \"Mark, Nonspacing\"일 경우 pass\n",
    "            continue\n",
    "        output.append(char)\n",
    "    token = ''.join(output) # if문에 해당하는 char가 없었기에 원본 text를 출력\n",
    "                            # 정준분해된 상태임을 기억해라\n",
    "    #5. punctuation 구분(사실상 의미가 없다)\n",
    "    chars = list(token)\n",
    "    i, start_new_word = 0, True\n",
    "    output = []\n",
    "    print(chars)\n",
    "    print('char\\t_is_puntuation(char)\\tstart_new_word\\tOutput')\n",
    "    while i < len(chars):\n",
    "        char = chars[i]\n",
    "        print(char, end='\\t')\n",
    "        print(_is_punctuation(char), end='\\t\\t\\t')\n",
    "        if _is_punctuation(char):\n",
    "            print('In Here!! <CODE BLOCK \"IF _IS_PUNCTUATOIN\">')\n",
    "            output.append([char])\n",
    "            start_new_word = True\n",
    "        else:\n",
    "            if start_new_word:\n",
    "                output.append([])\n",
    "            start_new_word = False\n",
    "            output[-1].append(char)\n",
    "        print(start_new_word, end='\\t\\t')\n",
    "        print(output)\n",
    "        i += 1\n",
    "    split_tokens.extend([\"\".join(x) for x in output])\n",
    "print('split_tokens:', split_tokens)\n",
    "output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
    "print('output_tokens:', output_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### `BasicTokenizer` 파트 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['심사숙고/NNG_',\n",
       " '하/XSV_',\n",
       " '였/EP_',\n",
       " '겠/EP_',\n",
       " '지만/EC_',\n",
       " '참으로/MAG_',\n",
       " '유감/NNG_',\n",
       " '이/JX_',\n",
       " '야/EC_']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tokens = [token + '_' for token in output_tokens]\n",
    "output_tokens # ETRI 단어 사전에 맞게 form을 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_token = '[UNK]'\n",
    "max_input_chars_per_word = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = morph_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "심사숙고/NNG_\n",
      "심사숙고/NNG\n",
      "심사숙고/NN\n",
      "심사숙고/N\n",
      "심사숙고/\n",
      "심사숙고\n",
      "심사숙ᄀ\n",
      "심사숙\n",
      "심사수\n",
      "심사ᄉ\n",
      "심사\n",
      "심ᄉ\n",
      "사숙고/NNG_\n",
      "사숙고/NNG\n",
      "사숙고/NN\n",
      "사숙고/N\n",
      "사숙고/\n",
      "사숙고\n",
      "사숙ᄀ\n",
      "사숙\n",
      "사수\n",
      "사ᄉ\n",
      "숙고/NNG_\n",
      "숙고/NNG\n",
      "숙고/NN\n",
      "숙고/N\n",
      "숙고/\n",
      "숙고\n",
      "숙ᄀ\n",
      "참으로/MAG_\n",
      "참으로/MAG\n",
      "참으로/MA\n",
      "참으로/M\n",
      "참으로/\n",
      "참으로\n",
      "참으ᄅ\n",
      "참으\n",
      "참ᄋ\n",
      "으로/MAG_\n",
      "으로/MAG\n",
      "으로/MA\n",
      "으로/M\n",
      "으로/\n",
      "으로\n",
      "으ᄅ\n",
      "이/JX_\n",
      "이/JX\n",
      "이/J\n",
      "이/\n"
     ]
    }
   ],
   "source": [
    "SPLIT_TOKENS = []\n",
    "for text in output_tokens:\n",
    "    text = convert_to_unicode(text)\n",
    "    _output_tokens = []\n",
    "    # whitespacing 생략\n",
    "    chars = list(text)\n",
    "    if len(chars) > max_input_chars_per_word:\n",
    "        _output_tokens.append(unk_token)\n",
    "    is_bad = False\n",
    "    start = 0\n",
    "    sub_tokens = []\n",
    "    while start < len(chars):\n",
    "        end = len(chars)\n",
    "        cur_substr = None\n",
    "        while start < end:\n",
    "            substr = \"\".join(chars[start:end])\n",
    "            substr = unicodedata.normalize(\"NFC\", substr)\n",
    "            if substr in vocab:\n",
    "                cur_substr = substr\n",
    "                break\n",
    "            end -= 1\n",
    "            print(substr)\n",
    "        if cur_substr is None:\n",
    "            is_bad = True\n",
    "            break\n",
    "        sub_tokens.append(cur_substr)\n",
    "        start = end\n",
    "    if is_bad:\n",
    "        _output_tokens.append(unk_token)\n",
    "    else:\n",
    "        _output_tokens.extend(sub_tokens)\n",
    "    \n",
    "    for sub_token in _output_tokens:\n",
    "        SPLIT_TOKENS.append(sub_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['[CLS]' '심' '사' '숙' '고/NNG_' '하/XSV_' '였/EP_' '겠/EP_' '지만/EC_' '참' '으'\n",
      "  '로/MAG_' '유감/NNG_' '이' '/JX_' '야/EC_' '[SEP]']\n",
      " ['2' '855' '174' '2341' '576' '9' '840' '124' '74' '1855' '2392' '2337'\n",
      "  '6770' '134' '3087' '4741' '3']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.vstack((tokens, [morph_vocab[token] for token in tokens])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SPLIT_TOKENS) > max_seq_length - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The convention in BERT is:\n",
    "# (a) For sequence pairs:\n",
    "# tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "# type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
    "# (b) For single sequences:\n",
    "# tokens:   [CLS] the dog is hairy . [SEP]\n",
    "# type_ids: 0     0   0   0  0     0 0\n",
    "#\n",
    "# Where \"type_ids\" are used to indicate whether this is the first\n",
    "# sequence or the second sequence. The embedding vectors for 'type=0' and\n",
    "# 'type=1' were learned during pre-training and are added to the wordpiece\n",
    "# embedding vector (and position vector). This is not \"strictly\" necessary\n",
    "# since the [SEP] token unambigiously separates the sequences, but it makes\n",
    "# if easier for the model to learn the concept of sequences.\n",
    "#\n",
    "# For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "# used as the \"sentence vector\". Note that this only makes sense because\n",
    "# the entire model is fine-tuned.\n",
    "\n",
    "tokens = [\"[CLS]\"] + SPLIT_TOKENS + [\"[SEP]\"]\n",
    "segment_ids = [0] * len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '심', '사', '숙', '고/NNG_', '하/XSV_', '였/EP_', '겠/EP_', '지만/EC_', '참', '으', '로/MAG_', '유감/NNG_', '이', '/JX_', '야/EC_', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 855, 174, 2341, 576, 9, 840, 124, 74, 1855, 2392, 2337, 6770, 134, 3087, 4741, 3]\n"
     ]
    }
   ],
   "source": [
    "input_ids = [morph_vocab[token] for token in tokens]\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "# tokens are attended to.\n",
    "input_mask = [1] * len(input_ids)\n",
    "input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-pad up to the sequence length.\n",
    "padding = [0] * (max_seq_length - len(input_ids))\n",
    "input_ids += padding\n",
    "input_mask += padding\n",
    "segment_ids += padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(input_ids) == max_seq_length\n",
    "assert len(input_mask) == max_seq_length\n",
    "assert len(segment_ids) == max_seq_length"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
