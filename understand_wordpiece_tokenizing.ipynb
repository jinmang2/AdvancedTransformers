{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT_MODEL_HUB\t https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "BERT_MODEL = 'uncased_L-12_H-768_A-12' #@param {type:\"string\"}\n",
    "BERT_MODEL_HUB = 'https://tfhub.dev/google/bert_' + BERT_MODEL + '/1'\n",
    "\n",
    "print('BERT_MODEL_HUB\\t', BERT_MODEL_HUB)\n",
    "\n",
    "# Vocab_file을 저장하고 directory 주소를 binary 형태로 얻는다.\n",
    "with tf.Graph().as_default():\n",
    "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "    tokenization_info = bert_module(signature='tokenization_info',\n",
    "                                    as_dict=True)\n",
    "    with tf.Session() as sess:\n",
    "        vocab_file, do_lower_case = sess.run(\n",
    "            [tokenization_info['vocab_file'],\n",
    "             tokenization_info['do_lower_case']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'C:\\\\Users\\\\jinma\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\5a395eafef2a37bd9fc55d7f6ae676d2a134a838\\\\assets\\\\vocab.txt'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "# 단어 사전을 저장할 Ordereddict 객체 생성\n",
    "vocab = collections.OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary text를 unicode(utf-8)로 decode하는 함수 작성\n",
    "def convert_to_unicode(text):\n",
    "    if isinstance(text, str):\n",
    "        return text\n",
    "    elif isinstance(text, bytes):\n",
    "        return text.decode('utf-8', 'ignore')\n",
    "    else:\n",
    "        raise ValueError('Unsupported string type: %s' % type(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_file의 각 text를 unicode로 변환, vocab에 기록\n",
    "index = 0\n",
    "with tf.gfile.GFile(vocab_file, 'r') as reader:\n",
    "    while True:\n",
    "        token = convert_to_unicode(reader.readline())\n",
    "        if not token:\n",
    "            break\n",
    "        token = token.strip()\n",
    "        vocab[token] = index\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2023, 19204)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.get('this'), vocab.get('token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]',\n",
       " '\"',\n",
       " 'to',\n",
       " 'paris',\n",
       " 'tears',\n",
       " 'knight',\n",
       " 'peninsula',\n",
       " 'licensed',\n",
       " 'mouse',\n",
       " 'screenplay',\n",
       " 'raven',\n",
       " 'tonnes',\n",
       " 'princes',\n",
       " 'osaka',\n",
       " 'liability',\n",
       " '##lip',\n",
       " 'kappa',\n",
       " 'hasan',\n",
       " 'belts',\n",
       " '##leader',\n",
       " 'chunk',\n",
       " 'colton',\n",
       " 'artworks',\n",
       " 'radiated',\n",
       " 'plank',\n",
       " 'fielder',\n",
       " 'fide',\n",
       " 'selector',\n",
       " 'statehood',\n",
       " 'gunners',\n",
       " '##ᄌ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab.keys())[::1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab의 key와 value를 바꾼 dict 객체 생성\n",
    "inv_vocab = {v:k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_lower_case = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Char 단위 함수 작성\n",
    "import unicodedata\n",
    "\n",
    "def _is_whitespace(char):\n",
    "    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        # 공백 혹은 개행문자일 경우 True 반환\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat == 'Zs':\n",
    "        # unicode category가 \"Space Separator\"일 경우 True 반환\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "def _is_control(char):\n",
    "    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        # 개행문자일 경우 False 반환\n",
    "        return False\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat in ('Cc', 'Cf'):\n",
    "        # unicode category가 \"Control\", 혹은 \"Format\"일 경우 True 반환\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _is_punctuation(char):\n",
    "    cp = ord(char)\n",
    "    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
    "       (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat.startswith(\"P\"):\n",
    "        # unicode category가 P로 시작할 경우 True 반환\n",
    "        # Pc (Connector Punctuatoin)\n",
    "        # Pd (Dash Punctuation)\n",
    "        # Pe (Close Punctuation)\n",
    "        # Pf (Final Punctuatoin)\n",
    "        # Pi (Initial Punctuation)\n",
    "        # Po (Other Punctuation)\n",
    "        # Ps (Open Punctuation)\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\x00', '�')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0), chr(0xfffd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n This \\t here's \\t an example of using the BERT tokenizer\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예제 text를 할당한다.\n",
    "text = \"\\n This \\t here's \\t an example of using the BERT tokenizer\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. BasicTokenizer로 tokenize\n",
      "\n",
      "Origin Text  : This   here's   an example of using the BERT tokenizer\n",
      "Cleaned Text : This   here's   an example of using the BERT tokenizer\n",
      "  Do Lower Case... run strip accents.\n",
      "\t   origin Token : this\n",
      "\tnormalize Token : this\n",
      "\t   output Token : this\n",
      "  \t\trun split on Punctuation.\n",
      "\tStart on list(token) : ['t', 'h', 'i', 's']\n",
      "\tEnd output : [['t', 'h', 'i', 's']]\n",
      "  Do Lower Case... run strip accents.\n",
      "\t   origin Token : here's\n",
      "\tnormalize Token : here's\n",
      "\t   output Token : here's\n",
      "  \t\trun split on Punctuation.\n",
      "\tStart on list(token) : ['h', 'e', 'r', 'e', \"'\", 's']\n",
      "\tEnd output : [['h', 'e', 'r', 'e'], [\"'\"], ['s']]\n",
      "  Do Lower Case... run strip accents.\n",
      "\t   origin Token : an\n",
      "\tnormalize Token : an\n",
      "\t   output Token : an\n",
      "  \t\trun split on Punctuation.\n",
      "\tStart on list(token) : ['a', 'n']\n",
      "\tEnd output : [['a', 'n']]\n",
      "  Do Lower Case... run strip accents.\n",
      "\t   origin Token : example\n",
      "\tnormalize Token : example\n",
      "\t   output Token : example\n",
      "  \t\trun split on Punctuation.\n",
      "\tStart on list(token) : ['e', 'x', 'a', 'm', 'p', 'l', 'e']\n",
      "\tEnd output : [['e', 'x', 'a', 'm', 'p', 'l', 'e']]\n",
      "  Do Lower Case... run strip accents.\n",
      "\t   origin Token : of\n",
      "\tnormalize Token : of\n",
      "\t   output Token : of\n",
      "  \t\trun split on Punctuation.\n",
      "\tStart on list(token) : ['o', 'f']\n",
      "\tEnd output : [['o', 'f']]\n",
      "  Do Lower Case... run strip accents.\n",
      "\t   origin Token : using\n",
      "\tnormalize Token : using\n",
      "\t   output Token : using\n",
      "  \t\trun split on Punctuation.\n",
      "\tStart on list(token) : ['u', 's', 'i', 'n', 'g']\n",
      "\tEnd output : [['u', 's', 'i', 'n', 'g']]\n",
      "  Do Lower Case... run strip accents.\n",
      "\t   origin Token : the\n",
      "\tnormalize Token : the\n",
      "\t   output Token : the\n",
      "  \t\trun split on Punctuation.\n",
      "\tStart on list(token) : ['t', 'h', 'e']\n",
      "\tEnd output : [['t', 'h', 'e']]\n",
      "  Do Lower Case... run strip accents.\n",
      "\t   origin Token : bert\n",
      "\tnormalize Token : bert\n",
      "\t   output Token : bert\n",
      "  \t\trun split on Punctuation.\n",
      "\tStart on list(token) : ['b', 'e', 'r', 't']\n",
      "\tEnd output : [['b', 'e', 'r', 't']]\n",
      "  Do Lower Case... run strip accents.\n",
      "\t   origin Token : tokenizer\n",
      "\tnormalize Token : tokenizer\n",
      "\t   output Token : tokenizer\n",
      "  \t\trun split on Punctuation.\n",
      "\tStart on list(token) : ['t', 'o', 'k', 'e', 'n', 'i', 'z', 'e', 'r']\n",
      "\tEnd output : [['t', 'o', 'k', 'e', 'n', 'i', 'z', 'e', 'r']]\n",
      "split_tokens : ['this', 'here', \"'\", 's', 'an', 'example', 'of', 'using', 'the', 'bert', 'tokenizer']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Final Result : ['this', 'here', \"'\", 's', 'an', 'example', 'of', 'using', 'the', 'bert', 'tokenizer']\n"
     ]
    }
   ],
   "source": [
    "print('1. BasicTokenizer로 tokenize\\n')\n",
    "text = convert_to_unicode(text)\n",
    "## _clean_text(self, text):\n",
    "output = []\n",
    "for char in text:\n",
    "    cp = ord(char)\n",
    "    if cp == 0 or cp == 0xfffd or _is_control(char):\n",
    "        continue\n",
    "    if _is_whitespace(char): # 공백 혹은 개행문자면 \n",
    "        output.append(\" \")\n",
    "    else:\n",
    "        output.append(char)\n",
    "print('Origin Text  :', text)\n",
    "text = \"\".join(output)\n",
    "print('Cleaned Text :', text)\n",
    "\n",
    "## whitespace_tokenize(text)\n",
    "text = text.strip()\n",
    "orig_tokens = text.split()\n",
    "split_tokens = []\n",
    "for token in orig_tokens:\n",
    "    if do_lower_case:\n",
    "        print('  Do Lower Case... run strip accents.')\n",
    "        token = token.lower()\n",
    "        ## _run_strip_accents(self, text)\n",
    "        print('\\t   origin Token :', token)\n",
    "        token = unicodedata.normalize(\"NFD\", token)\n",
    "        print('\\tnormalize Token :', token)\n",
    "        output = []\n",
    "        for char in token:\n",
    "            cat = unicodedata.category(char)\n",
    "            if cat == 'Mn':\n",
    "                # unicode category가 \"Nonspacing Mark\"일 경우 pass\n",
    "                continue\n",
    "            output.append(char)\n",
    "        token = \"\".join(output)\n",
    "        print('\\t   output Token :', token)\n",
    "    ## _run_split_on_punc(self, text)\n",
    "    print('  \\t\\trun split on Punctuation.')\n",
    "    chars = list(token)\n",
    "    i, start_new_word, output = 0, True, []\n",
    "    print('\\tStart on list(token) :', chars)\n",
    "    while i < len(chars):\n",
    "        char = chars[i]\n",
    "        if _is_punctuation(char):\n",
    "            output.append([char])\n",
    "            start_new_word = True\n",
    "        else:\n",
    "            if start_new_word:\n",
    "                output.append([])\n",
    "            start_new_word = False\n",
    "            output[-1].append(char)\n",
    "        i += 1\n",
    "    print('\\tEnd output :', output)\n",
    "    split_tokens.extend([\"\".join(x) for x in output])\n",
    "print('split_tokens :', split_tokens)\n",
    "t = \" \".join(split_tokens)\n",
    "t = t.strip()\n",
    "output_tokens = t.split()\n",
    "print('-' * 100 + '\\n' + 'Final Result :', output_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. WordpieceTokenizer로 tokenize\n",
      "\n",
      "token : this\n",
      "0 4\t this\n",
      "token : here\n",
      "0 4\t here\n",
      "token : '\n",
      "0 1\t '\n",
      "token : s\n",
      "0 1\t s\n",
      "token : an\n",
      "0 2\t an\n",
      "token : example\n",
      "0 7\t example\n",
      "token : of\n",
      "0 2\t of\n",
      "token : using\n",
      "0 5\t using\n",
      "token : the\n",
      "0 3\t the\n",
      "token : bert\n",
      "0 4\t bert\n",
      "token : tokenizer\n",
      "0 9\t tokenizer\n",
      "\t tokenize\n",
      "\t tokeniz\n",
      "\t tokeni\n",
      "\t token\n",
      "5 9\t izer\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Final Result : ['this', 'here', \"'\", 's', 'an', 'example', 'of', 'using', 'the', 'bert', 'token', '##izer']\n"
     ]
    }
   ],
   "source": [
    "print('2. WordpieceTokenizer로 tokenize\\n')\n",
    "\n",
    "split_tokens = []\n",
    "for token in output_tokens:\n",
    "    print('token :', token)\n",
    "    ## wordpiece tokenizing (greedy longest-match-first algorithm)\n",
    "    unk_token = \"[UNK]\"\n",
    "    max_input_chars_per_word = 200\n",
    "    # Start\n",
    "    token = convert_to_unicode(token)\n",
    "    output_tokens_ = []\n",
    "    ## whitspacing\n",
    "    if not token.strip():\n",
    "        tokens = []\n",
    "    else:\n",
    "        tokens = token.strip().split()\n",
    "    for token in tokens:\n",
    "        chars = list(token)\n",
    "        if len(chars) > max_input_chars_per_word:\n",
    "            # 200글자를 넘을 경우 UNK 처리\n",
    "            output_tokens_.append(unk_token)\n",
    "            continue\n",
    "        \n",
    "        is_bad = False\n",
    "        start = 0\n",
    "        sub_tokens = []\n",
    "        while start < len(chars):\n",
    "            end = len(chars)\n",
    "            print(start, end, end='')\n",
    "            cur_substr = None\n",
    "            # 첫번째 글짜부터 천천히 vocab에 있는 단어인지 체크\n",
    "            while start < end:\n",
    "                substr = \"\".join(chars[start:end])\n",
    "                print('\\t', substr)\n",
    "                if start > 0:\n",
    "                    ## start에 end가 할당됐을 경우,\n",
    "                    ## 이는 어미이므로 ##을 붙여서 vocab에 있는지 체크\n",
    "                    substr = \"##\" + substr\n",
    "                if substr in vocab:\n",
    "                    cur_substr = substr\n",
    "                    break\n",
    "                end -= 1\n",
    "            # 만일 못찾았을 경우, [UNK]으로 처리\n",
    "            if cur_substr is None:\n",
    "                is_bad = True\n",
    "                break\n",
    "            sub_tokens.append(cur_substr)\n",
    "            # 어미를 추가하기 위해 start에 end값을 할당\n",
    "            start = end\n",
    "        if is_bad:\n",
    "            output_tokens_.append(unk_token)\n",
    "        else:\n",
    "            output_tokens_.extend(sub_tokens)\n",
    "    for sub_token in sub_tokens:\n",
    "        split_tokens.append(sub_token)\n",
    "        \n",
    "print('-' * 100 + '\\n' + 'Final Result :', split_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
