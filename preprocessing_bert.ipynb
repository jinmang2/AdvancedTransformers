{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_20191230 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ETRI BERT Config\n",
    "```python\n",
    "bert_config = {\n",
    "  \"attention_probs_dropout_prob\": 0.1, \n",
    "  \"directionality\": \"bidi\", \n",
    "  \"hidden_act\": \"gelu\", \n",
    "  \"hidden_dropout_prob\": 0.1, \n",
    "  \"hidden_size\": 768, \n",
    "  \"initializer_range\": 0.02, \n",
    "  \"intermediate_size\": 3072, \n",
    "  \"max_position_embeddings\": 512, \n",
    "  \"num_attention_heads\": 12, \n",
    "  \"num_hidden_layers\": 12, \n",
    "  \"pooler_fc_size\": 768, \n",
    "  \"pooler_num_attention_heads\": 12, \n",
    "  \"pooler_num_fc_layers\": 3, \n",
    "  \"pooler_size_per_head\": 128, \n",
    "  \"pooler_type\": \"first_token_transform\", \n",
    "  \"type_vocab_size\": 2, \n",
    "  \"vocab_size\": 30349\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ETRI Bert Config\n",
    "path = '../KorBERT/2_bert_download_002_bert_morp_tensorflow/002_bert_morp_tensorflow/'\n",
    "FLAGS.bert_config_file = path + 'bert_config.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = BertConfig.from_json_file(FLAGS.bert_config_file)\n",
    "# bert_config = BertConfig.from_dict(bert_config) # 위의 dictionary를 메모리에 올려서 \n",
    "                                                 # 다음 메서드로 호출하는 것도 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     (FLAGS) MAX_SEQ_LENGTH           : 128\n",
      "(BERTConfig) MAX_POSITION_EMBEDDINGS  : 512\n"
     ]
    }
   ],
   "source": [
    "if FLAGS.max_seq_length > bert_config.max_position_embeddings:\n",
    "    raise ValueError(\n",
    "        \"Cannot use sequence length %d because the BERT model \"\n",
    "        \"was only trained up to sequence length %d\" %\n",
    "        (FLAGS.max_seq_length, bert_config.max_position_embeddings))\n",
    "else:\n",
    "    print('     (FLAGS) MAX_SEQ_LENGTH           :', FLAGS.max_seq_length)\n",
    "    print('(BERTConfig) MAX_POSITION_EMBEDDINGS  :', bert_config.max_position_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do not use TPU\n"
     ]
    }
   ],
   "source": [
    "# do not use tpu\n",
    "tpu_cluster_resolver = None\n",
    "if FLAGS.use_tpu and FLAGS.tpu_name:\n",
    "    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n",
    "        FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n",
    "else:\n",
    "    print('Do not use TPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
    "run_config = tf.contrib.tpu.RunConfig(\n",
    "    cluster=tpu_cluster_resolver,\n",
    "    master=FLAGS.master,\n",
    "    model_dir=FLAGS.output_dir,\n",
    "    save_checkpoints_steps=FLAGS.save_checkpoints_steps, # 1000\n",
    "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "        iterations_per_loop=FLAGS.iterations_per_loop, # 1000\n",
    "        num_shards=FLAGS.num_tpu_cores, # 8\n",
    "        per_host_input_for_training=is_per_host)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensorflow gpu 사용 가능한지 체크\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 vocab 사전을 등록\n",
    "FLAGS.vocab_file = path + 'vocab.korean_morp.list' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "간략한 파일 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dacon_path = '../dacon문자스미싱/filedown (2)/'\n",
    "df_train = pd.read_csv(dacon_path + 'train.csv')\n",
    "df_test = pd.read_csv(dacon_path + 'public_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((236756, 2), (59189, 2), (236756,), (59189,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train = df_train.set_index('id')\n",
    "df_test = df_test.set_index('id')\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "                 df_train[[col for col in df_train.columns if col != 'smishing']], \n",
    "                 df_train['smishing'],\n",
    "                 random_state=42, test_size=.2,\n",
    "                 stratify=df_train['smishing'])\n",
    "X_train.shape, X_valid.shape, y_train.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat((X_train, y_train), axis=1)\n",
    "df_valid = pd.concat((X_valid, y_valid), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 100개씩 뽑아서 미리 test\n",
    "df_train.sample(100).to_csv(dacon_path + 'train_100.tsv', sep='\\t')\n",
    "df_valid.sample(100).to_csv(dacon_path + 'dev_100.tsv', sep='\\t')\n",
    "df_test.sample(100).to_csv(dacon_path + 'test_100.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataProcessor 제작\n",
    "class SmishingProcessor(DataProcessor):\n",
    "\n",
    "    def get_train_examples(self, data_dir, filename='train.tsv'):\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, filename)), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir, filename='dev.tsv'):\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, filename)), \"dev\")\n",
    "\n",
    "    def get_test_examples(self, data_dir, filename='test.tsv'):\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, filename)), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = convert_to_unicode(line[2])\n",
    "            if set_type == \"test\":\n",
    "                label = \"0\"\n",
    "            else:\n",
    "                label = convert_to_unicode(line[-1])\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, label=label))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = SmishingProcessor()\n",
    "label_list = processor.get_labels()\n",
    "\n",
    "# get train samples\n",
    "train_examples = processor.get_train_examples(dacon_path, 'train_100.tsv')\n",
    "num_train_steps = int(\n",
    "    len(train_examples) / FLAGS.train_batch_size * FLAGS.num_train_epochs)\n",
    "num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record ETRI model weights\n",
    "FLAGS.init_checkpoint = path + 'model.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = model_fn_builder(\n",
    "    bert_config=bert_config,\n",
    "    num_labels=len(label_list),             # 2\n",
    "    init_checkpoint=FLAGS.init_checkpoint,  # None\n",
    "    learning_rate=FLAGS.learning_rate,      # 5e-05\n",
    "    num_train_steps=num_train_steps,        # 22195\n",
    "    num_warmup_steps=num_warmup_steps,      # 2219\n",
    "    use_tpu=FLAGS.use_tpu,                  # False\n",
    "    use_one_hot_embeddings=FLAGS.use_tpu)   # False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x000002196B19C0D0>) includes params argument, but params are not passed to Estimator.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\jinma\\AppData\\Local\\Temp\\tmp95ip6j47\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\jinma\\\\AppData\\\\Local\\\\Temp\\\\tmp95ip6j47', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002196B1B3A20>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': None}\n",
      "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
      "WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\n"
     ]
    }
   ],
   "source": [
    "# If TPU is not available, this will fall back to normal Estimator on CPU\n",
    "# or GPU\n",
    "estimator = tf.contrib.tpu.TPUEstimator(\n",
    "    use_tpu=FLAGS.use_tpu,                        # False\n",
    "    model_fn=model_fn,\n",
    "    config=run_config,\n",
    "    train_batch_size=FLAGS.train_batch_size,      # 32\n",
    "    eval_batch_size=FLAGS.eval_batch_size,        # 8\n",
    "    predict_batch_size=FLAGS.predict_batch_size   # 8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS.output_dir = './output_dir/smishing/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.gfile.MakeDirs(FLAGS.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding\n",
    "- TTA 표준 형태소 태그셋(TTAK.KO-11.0010/R1)에 맞는 형태소 분석기를 사용해야 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"table table-striped table-bordered\" style=\"width:800px;\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th style=\"width:200px\" align=\"center\">대분류</td>\n",
    "      <th style=\"width:200px\" align=\"center\">중분류</td>\n",
    "      <th style=\"width:400px\" align=\"center\">대분류</td>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td rowspan=\"5\" align=\"center\">(1) 체언</td>\n",
    "      <td rowspan=\"3\" align=\"center\">명사</td>\n",
    "      <td align=\"center\">일반명사(NNG)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">고유명사(NNP)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">의존명사(NNB)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">대명사(NP)</td>\n",
    "      <td align=\"center\">대명사(NP)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">수사(NR)</td>\n",
    "      <td align=\"center\">수사(NR)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td rowspan=\"5\" align=\"center\">(2) 용언</td>\n",
    "      <td align=\"center\">동사(VV)</td>\n",
    "      <td align=\"center\">동사(VV)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">형용사(VA)</td>\n",
    "      <td align=\"center\">형용사(VA)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">보조용언(VX)</td>\n",
    "      <td align=\"center\">보조용언(VX)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td rowspan=\"2\" align=\"center\">지정사(VC)</td>\n",
    "      <td align=\"center\">긍정지정사(VCP)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">부정지정사(VCN)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td rowspan=\"5\" align=\"center\">(3) 수식언</td>\n",
    "      <td rowspan=\"3\" align=\"center\">관형사(MM)</td>\n",
    "      <td align=\"center\">성상 관형사(MMA)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">지시 관형사(MMD)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">수 관형사(MMN)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td rowspan=\"2\" align=\"center\">부사(MA)</td>\n",
    "      <td align=\"center\">일반부사(MAG)</td>        \n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">접속부사(MAJ)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">(4) 독립언</td>\n",
    "      <td align=\"center\">감탄사(IC)</td>\n",
    "      <td align=\"center\">감탄사(IC)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td rowspan=\"9\" align=\"center\">(5) 관계언</td>\n",
    "      <td rowspan=\"7\" align=\"center\">격조사(JK)</td>\n",
    "      <td align=\"center\">주격조사(JKS)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">보격조사(JKC)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">관형격조사(JKG)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">목적격조사(JKO)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">부사격조사(JKB)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">호격조사(JKV)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">인용격조사(JKQ)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">보조사(JX)</td>\n",
    "      <td align=\"center\">보조사(JX)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">접속조사(JC)</td>\n",
    "      <td align=\"center\">접속조사(JC)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td rowspan=\"10\" align=\"center\">(6) 의존형태</td>\n",
    "      <td rowspan=\"5\" align=\"center\">어미(EM)</td>\n",
    "      <td align=\"center\">선어말어미(EP)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">종결어미(EF)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">연결어미(EC)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">명사형전성어미(ETN)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">관형형전성어미(ETM)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">접두사(XP)</td>\n",
    "      <td align=\"center\">체언접두사(XPN)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td rowspan=\"3\" align=\"center\">접미사(XS)</td>\n",
    "      <td align=\"center\">명사파생접미사(XSN)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">동사파생접미사(XSV)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">형용사파생접미사(XSA)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">어근(XR)</td>\n",
    "      <td align=\"center\">어근(XR)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td rowspan=\"10\" align=\"center\">(7) 기초</td>\n",
    "      <td rowspan=\"6\" align=\"center\">일반기호(ST)</td>\n",
    "      <td align=\"center\">마침표, 물음표, 느낌표(SF)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">쉼표, 가운뎃점, 콜론, 빗금(SP)</td>\n",
    "    </tr>  \n",
    "    <tr>\n",
    "      <td align=\"center\">따옴표, 괄호표, 줄표(SS)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">줄임표(SE)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">붙임표(물결)(SO)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">기타 기호(SW)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">외국어(SL)</td>\n",
    "      <td align=\"center\">외국어(SL)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">한자(SH)</td>\n",
    "      <td align=\"center\">한자(SH)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">숫자(SN)</td>\n",
    "      <td align=\"center\">숫자(SN)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">분석불능범주(NA)</td>\n",
    "      <td align=\"center\">분석불능범주(NA)</td>\n",
    "    </tr>\n",
    "  </tdoby>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTA 가이드라인\n",
    "# http://aiopen.etri.re.kr/data/001.형태소분석_가이드라인.pdf\n",
    "tta_guide = {\n",
    "    '체언': {\n",
    "        '명사': ('NN',\n",
    "               {'일반명사': 'NNG',\n",
    "                '고유명사': 'NNP',\n",
    "                '의존명사': 'NNB'}),\n",
    "        '대명사': ('NP',{'대명사': 'NP'}),\n",
    "        '수사': ('NR', {'수사': 'NR'})\n",
    "    },\n",
    "    '용언': {\n",
    "        '동사': ('VV', {'동사': 'VV'}),\n",
    "        '형용사': ('VA', {'형용사': 'VA'}),\n",
    "        '보조용언': ('VX', {'보조용언': 'VX'}),\n",
    "        '지정사': ('VC', \n",
    "                {'긍정지정사': 'VCP',\n",
    "                 '부정지정사': 'VCN'})\n",
    "    },\n",
    "    '수식언': {\n",
    "        '관형사': ('MM', \n",
    "                {'성상 관형사': 'MMA',\n",
    "                 '지시 관형사': 'MMD',\n",
    "                 '수 관형사': 'MMN'}),\n",
    "        '부사': ('MA', \n",
    "               {'일반부사': 'MAG',\n",
    "                '접속부사': 'MAJ'})\n",
    "    },\n",
    "    '독립언': {\n",
    "        '감탄사': ('IC', {'감탄사': 'IC'})\n",
    "    },\n",
    "    '관계언': {\n",
    "        '격조사': ('JK', \n",
    "                {'주격조사': 'JKS',\n",
    "                 '보격조사': 'JKC',\n",
    "                 '관형격조사': 'JKG',\n",
    "                 '목적격조사': 'JKO',\n",
    "                 '부사격조사': 'JKB',\n",
    "                 '호격조사': 'JKV',\n",
    "                 '인용격조사': 'JKQ'}),\n",
    "        '보조사': ('JX', {'보조사': 'JK'}),\n",
    "        '접속조사': ('JC', {'접속조사': 'JC'})\n",
    "    },\n",
    "    '의존형태': {\n",
    "        '어미': ('EM', \n",
    "               {'선어말어미': 'EP',\n",
    "                '종결어미': 'EF',\n",
    "                '연결어미': 'EC',\n",
    "                '명사형전성어미': 'ETN',\n",
    "                '관형형전성어미': 'ETM'}),\n",
    "        '접두사': ('XP', {'체언접두사': 'XPN'}),\n",
    "        '접미사': ('XS', \n",
    "                {'명사파생접미사': 'XSN',\n",
    "                 '동사파생접미사': 'XSV',\n",
    "                 '형용사파생접미사': 'XSA'}),\n",
    "        '어근': ('XR', {'어근': 'XR'})\n",
    "    },\n",
    "    '기호': {\n",
    "        '일반기호': ('ST', \n",
    "                 {'마침표, 물음표, 느낌표': 'SF',\n",
    "                  '쉼표, 가운뎃점, 콜론, 빗금': 'SP',\n",
    "                  '따옴표, 괄호표, 줄표': 'SS',\n",
    "                  '줄임표': 'SE',\n",
    "                  '붙임표(물결)': 'SO',\n",
    "                  '기타 기호': 'SW'}),\n",
    "        '외국어': ('SL', {'외국어': 'SL'}),\n",
    "        '한자': ('SH', {'한자': 'SH'}),\n",
    "        '숫자': ('SN', {'숫자': 'SN'}),\n",
    "        '분석불능범주': ('NA', {'분석불능범주': 'NA'})\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'체언': {'명사': ('NN', {'일반명사': 'NNG', '고유명사': 'NNP', '의존명사': 'NNB'}),\n",
       "  '대명사': ('NP', {'대명사': 'NP'}),\n",
       "  '수사': ('NR', {'수사': 'NR'})},\n",
       " '용언': {'동사': ('VV', {'동사': 'VV'}),\n",
       "  '형용사': ('VA', {'형용사': 'VA'}),\n",
       "  '보조용언': ('VX', {'보조용언': 'VX'}),\n",
       "  '지정사': ('VC', {'긍정지정사': 'VCP', '부정지정사': 'VCN'})},\n",
       " '수식언': {'관형사': ('MM', {'성상 관형사': 'MMA', '지시 관형사': 'MMD', '수 관형사': 'MMN'}),\n",
       "  '부사': ('MA', {'일반부사': 'MAG', '접속부사': 'MAJ'})},\n",
       " '독립언': {'감탄사': ('IC', {'감탄사': 'IC'})},\n",
       " '관계언': {'격조사': ('JK',\n",
       "   {'주격조사': 'JKS',\n",
       "    '보격조사': 'JKC',\n",
       "    '관형격조사': 'JKG',\n",
       "    '목적격조사': 'JKO',\n",
       "    '부사격조사': 'JKB',\n",
       "    '호격조사': 'JKV',\n",
       "    '인용격조사': 'JKQ'}),\n",
       "  '보조사': ('JX', {'보조사': 'JK'}),\n",
       "  '접속조사': ('JC', {'접속조사': 'JC'})},\n",
       " '의존형태': {'어미': ('EM',\n",
       "   {'선어말어미': 'EP',\n",
       "    '종결어미': 'EF',\n",
       "    '연결어미': 'EC',\n",
       "    '명사형전성어미': 'ETN',\n",
       "    '관형형전성어미': 'ETM'}),\n",
       "  '접두사': ('XP', {'체언접두사': 'XPN'}),\n",
       "  '접미사': ('XS', {'명사파생접미사': 'XSN', '동사파생접미사': 'XSV', '형용사파생접미사': 'XSA'}),\n",
       "  '어근': ('XR', {'어근': 'XR'})},\n",
       " '기호': {'일반기호': ('ST',\n",
       "   {'마침표, 물음표, 느낌표': 'SF',\n",
       "    '쉼표, 가운뎃점, 콜론, 빗금': 'SP',\n",
       "    '따옴표, 괄호표, 줄표': 'SS',\n",
       "    '줄임표': 'SE',\n",
       "    '붙임표(물결)': 'SO',\n",
       "    '기타 기호': 'SW'}),\n",
       "  '외국어': ('SL', {'외국어': 'SL'}),\n",
       "  '한자': ('SH', {'한자': 'SH'}),\n",
       "  '숫자': ('SN', {'숫자': 'SN'}),\n",
       "  '분석불능범주': ('NA', {'분석불능범주': 'NA'})}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tta_guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_set = []\n",
    "for VALUE in tta_guide.values():\n",
    "    for VALUE2 in VALUE.values():\n",
    "        pos_set.append(VALUE2[0])\n",
    "        for VALUE3 in VALUE2[1].values():\n",
    "            pos_set.append(VALUE3)\n",
    "pos_set = list(set(pos_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n",
    "\n",
    "komoran = Komoran()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i[1] for i in komoran.pos(train_examples[0].text_a) if i[1] not in pos_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Komoran 형태소 분석기로 분석 실시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exam in train_examples:\n",
    "    exam.text_a = ' '.join(\n",
    "        [i[0] + '/' + i[1] \n",
    "         for i in komoran.pos(exam.text_a)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'XXX/SL 지점/NNP 을/JKO 거래/NNG 하/XSV 아/EC 주/VX 시/EP 어서/EC 대단히/MAG 감사/NNG 하/XSV ㅂ니다/EF ./SF 내/NNP 점/NNB 하/NNP XXX/NNP 고객/NNG 님/XSN 고객/NNG 만족도/NNG 설문/NNP 조사/NNP 전화/NNG 받/VV 으시/EP 면/EC 매우/MAG 동의/NNG 하/XSV ㄴ다라고/EC 우수/NNP 직원/NNP 추천/NNG 해주시/NNP 이/VCP 고/EC 사은품/NNG 받/VV 아/EC 가/VX 시/EP 어요/EC ../SE 더욱더/MAG 친절히/MAG 모시/VV 겠/EP 습니다/EC XXX/SL 은행/NNP XXX/NNP 올림/NNP'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples[0].text_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = os.path.join(FLAGS.output_dir, 'train.tf_record')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn: file_based_convert_examples_to_features\n",
    "\n",
    "## Arguments\n",
    "examples = train_examples\n",
    "label_list = label_list\n",
    "max_seq_length = FLAGS.max_seq_length\n",
    "# tokenizer = FullTokenizer() # 예시로 tokenizing을 어떻게 하는지 전부 기록\n",
    "output_file = train_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.python_io.TFRecordWriter(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_index = 5\n",
    "example = examples[ex_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fn: convert_single_example\n",
    "isinstance(example, PaddingInputExample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 0, '1': 1}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {}\n",
    "for (i, label) in enumerate(label_list):\n",
    "    label_map[label] = i\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'XXX 고객님항상 XXX은행 모란역 지점을 이용해 주시는 고객님께 감사의 마음을 전합니다. 혹시 업무와 관련해 궁금한 점이 있으시면 이 번호로 연락주시기바랍니다. 성심껏 도와드리겠습니다. 또 혹시 고객만족도 조사 전화를 받으시면 매우 동의한다 로 칭찬해 주세요 조금은 쌀쌀한 10월의 첫주입니다.환절기  감기조심하시고 따듯한 차와 함께 건강한 한주 보내시기 바랍니다.XXX은행모란역XXX올림'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class: FullTokenizer\n",
    "path = '../KorBERT/2_bert_download_002_bert_morp_tensorflow/002_bert_morp_tensorflow/'\n",
    "FLAGS.vocab_file = path + 'vocab.korean_morp.list' \n",
    "vocab_file = FLAGS.vocab_file\n",
    "do_lower_case = FLAGS.do_lower_case\n",
    "text = example.text_a\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = load_vocab(vocab_file)\n",
    "inv_vocab = {v:k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "openApiURL = \"http://aiopen.etri.re.kr:8000/WiseNLU\"\n",
    "openapi_key = ''\n",
    "requestJson = { \"access_key\": openapi_key, \"argument\": { \"text\": text, \"analysis_code\": \"morp\" } }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'access_key': '0da70af6-e163-44b5-8d5d-f217bebb5765',\n",
       " 'argument': {'text': 'XXX 고객님항상 XXX은행 모란역 지점을 이용해 주시는 고객님께 감사의 마음을 전합니다. 혹시 업무와 관련해 궁금한 점이 있으시면 이 번호로 연락주시기바랍니다. 성심껏 도와드리겠습니다. 또 혹시 고객만족도 조사 전화를 받으시면 매우 동의한다 로 칭찬해 주세요 조금은 쌀쌀한 10월의 첫주입니다.환절기  감기조심하시고 따듯한 차와 함께 건강한 한주 보내시기 바랍니다.XXX은행모란역XXX올림',\n",
       "  'analysis_code': 'morp'}}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requestJson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "http = urllib3.PoolManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = http.request( \"POST\", openApiURL, headers={\"Content-Type\": \"application/json; charset=UTF-8\"}, body=json.dumps(requestJson))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = json.loads(response.data.decode('utf-8'))\n",
    "json_result = json_data[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = json.loads(response.data.decode('utf-8'))\n",
    "json_return_obj = json_data[\"return_object\"]\n",
    "return_result = \"\"\n",
    "json_sentence = json_return_obj[\"sentence\"]\n",
    "for json_morp in json_sentence:                        \n",
    "    for morp in json_morp[\"morp\"]:\n",
    "        return_result = return_result+str(morp[\"lemma\"])+\"/\"+str(morp[\"type\"])+\" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'XXX/SL 고객/NNG 님/XSN 항상/MAG XXX/SL 은행/NNG 모란/NNG 역/NNG 지점/NNG 을/JKO 이용/NNG 하/XSV 어/EC 주/VX 시/EP 는/ETM 고객/NNG 님/XSN 께/JKB 감사/NNG 의/JKG 마음/NNG 을/JKO 전하/VV ㅂ니다/EF ./SF 혹시/MAG 업무/NNG 와/JKB 관련/NNG 하/XSV 어/EC 궁금하/VA ㄴ/ETM 점/NNG 이/JKS 있/VA 으시/EP 면/EC 이/MM 번호/NNG 로/JKB 연락/NNG 주/VV 시/EP 기/ETN 바라/VV ㅂ니다/EF ./SF 성심껏/MAG 돕/VV 아/EC 드리/VX 겠/EP 습니다/EF ./SF 또/MAG 혹시/MAG 고객/NNG 만족/NNG 도/NNG 조사/NNG 전화/NNG 를/JKO 받/VV 으시/EP 면/EC 매우/MAG 동의/NNG 하/XSV ㄴ다/EF 로/JKB 칭찬/NNG 하/XSV 어/EC 주/VX 시/EP 어요/EF 조금/NNG 은/JX 쌀쌀하/VA ㄴ/ETM 10/SN 월/NNB 의/JKG 첫주/NNG 이/VCP ㅂ니다/EF ./SF 환절/NNG 기/XSN 감기/NNG 조심/NNG 하/XSV 시/EP 고/EC 따듯하/VA ㄴ/ETM 차/NNG 와/JC 함께/MAG 건강/NNG 하/XSA ㄴ/ETM 한/MM 주/NNB 보내/VV 시/EP 기/ETN 바라/VV ㅂ니다/EF ./SF XXX/SL 은행/NNG 모란/NNG 역/NNG XXX/SL 올림/NNG '"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'XXX/SL 고객/NNG 님/NNG 항상/MAG XXX/SL 은행/NNP 모란역/NNP 지점/NNG 을/JKO 이용/NNG 하/XSV 아/EC 주시/NNP 는/JX 고객/NNG 님/XSN 께/JKB 감사/NNG 의/JKG 마음/NNG 을/JKO 전하/VV ㅂ니다/EF ./SF 혹시/MAG 업무/NNG 와/JC 관련/NNG 하/XSV 아/EC 궁금/XR 하/XSA ㄴ/ETM 점/NNB 이/JKS 있/VX 으시/EP 면/EC 이/MM 번호/NNG 로/JKB 연락/NNG 주/NNG 시기/NNG 바라/VV ㅂ니다/EF ./SF 성심껏/MAG 돕/VV 아/EC 드리/VX 겠/EP 습니다/EF ./SF 또/MAJ 혹시/MAG 고객/NNP 만족/NNP 도/JX 조사/NNG 전화/NNG 를/JKO 받/VV 으시/EP 면/EC 매우/MAG 동의/NNG 하/XSV ㄴ다/EC 로/NNG 칭찬/NNG 하/XSV 아/EC 주/VX 시/EP 어요/EC 조금/NNG 은/JX 쌀쌀/XR 하/XSA ㄴ/ETM 10월/NNP 의/JKG 첫/MM 주/NNB 이/VCP ㅂ니다/EF ./SF 환절기/NNG 감기/NNP 조심/NNG 하/XSV 시/EP 고/EC 따듯/XR 하/XSA ㄴ/ETM 차/NNG 와/JC 함께/MAG 건강/NNG 하/XSV ㄴ/ETM 한/MM 주/NNP 보내/VV 시/EP 기/ETN 바라/VV ㅂ니다/EF ./SF XXX/SL 은행/NNP 모란역/NNP XXX/NNP 올림/NNP'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([i[0] + '/' + i[1] for i in komoran.pos(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_lang ( openapi_key, text ) :\n",
    "    openApiURL = \"http://aiopen.etri.re.kr:8000/WiseNLU\"\n",
    "\t \n",
    "    requestJson = { \"access_key\": openapi_key, \"argument\": { \"text\": text, \"analysis_code\": \"morp\" } }\n",
    "\t \n",
    "    http = urllib3.PoolManager()\n",
    "    response = http.request( \"POST\", openApiURL, headers={\"Content-Type\": \"application/json; charset=UTF-8\"}, body=json.dumps(requestJson))\n",
    "    \n",
    "    json_data = json.loads(response.data.decode('utf-8'))\n",
    "    json_result = json_data[\"result\"]\n",
    "    \n",
    "    if json_result == -1:\n",
    "        json_reason = json_data[\"reason\"]\n",
    "        if \"Invalid Access Key\" in json_reason:\n",
    "            logger.info(json_reason)\n",
    "            logger.info(\"Please check the openapi access key.\")\n",
    "            sys.exit()\n",
    "        return \"openapi error - \" + json_reason      \n",
    "    else:\n",
    "        json_data = json.loads(response.data.decode('utf-8'))\n",
    "    \n",
    "        json_return_obj = json_data[\"return_object\"]\n",
    "        \n",
    "        return_result = \"\"\n",
    "        json_sentence = json_return_obj[\"sentence\"]\n",
    "        for json_morp in json_sentence:                        \n",
    "            for morp in json_morp[\"morp\"]:\n",
    "                return_result = return_result+str(morp[\"lemma\"])+\"/\"+str(morp[\"type\"])+\" \"\n",
    "\n",
    "        return return_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'XXX/SL 고객/NNG 님/XSN 항상/MAG XXX/SL 은행/NNG 모란/NNG 역/NNG 지점/NNG 을/JKO 이용/NNG 하/XSV 어/EC 주/VX 시/EP 는/ETM 고객/NNG 님/XSN 께/JKB 감사/NNG 의/JKG 마음/NNG 을/JKO 전하/VV ㅂ니다/EF ./SF 혹시/MAG 업무/NNG 와/JKB 관련/NNG 하/XSV 어/EC 궁금하/VA ㄴ/ETM 점/NNG 이/JKS 있/VA 으시/EP 면/EC 이/MM 번호/NNG 로/JKB 연락/NNG 주/VV 시/EP 기/ETN 바라/VV ㅂ니다/EF ./SF 성심껏/MAG 돕/VV 아/EC 드리/VX 겠/EP 습니다/EF ./SF 또/MAG 혹시/MAG 고객/NNG 만족/NNG 도/NNG 조사/NNG 전화/NNG 를/JKO 받/VV 으시/EP 면/EC 매우/MAG 동의/NNG 하/XSV ㄴ다/EF 로/JKB 칭찬/NNG 하/XSV 어/EC 주/VX 시/EP 어요/EF 조금/NNG 은/JX 쌀쌀하/VA ㄴ/ETM 10/SN 월/NNB 의/JKG 첫주/NNG 이/VCP ㅂ니다/EF ./SF 환절/NNG 기/XSN 감기/NNG 조심/NNG 하/XSV 시/EP 고/EC 따듯하/VA ㄴ/ETM 차/NNG 와/JC 함께/MAG 건강/NNG 하/XSA ㄴ/ETM 한/MM 주/NNB 보내/VV 시/EP 기/ETN 바라/VV ㅂ니다/EF ./SF XXX/SL 은행/NNG 모란/NNG 역/NNG XXX/SL 올림/NNG '"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_tokens = collections.OrderedDict(\n",
    "    [(ids, tok) for tok, ids in vocab.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "never_split=(\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "do_lower_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../KorBERT/2_bert_download_002_bert_morp_tensorflow/002_bert_morp_tensorflow/vocab.korean_morp.list'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.isdir(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from torch.hub import _get_torch_home\n",
    "\n",
    "torch_cache_home = _get_torch_home()\n",
    "    \n",
    "default_cache_path = os.path.join(torch_cache_home, \"transformers\")\n",
    "\n",
    "PYTORCH_PRETRAINED_BERT_CACHE = Path(\n",
    "    os.getenv(\"PYTORCH_TRANSFORMERS_CACHE\", os.getenv(\"PYTORCH_PRETRAINED_BERT_CACHE\", default_cache_path))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORMERS_CACHE = PYTORCH_PRETRAINED_BERT_CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(vocab_file, Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\jinma\\\\.cache\\\\torch\\\\transformers'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if isinstance(TRANSFORMERS_CACHE, Path):\n",
    "    cache_dir = str(TRANSFORMERS_CACHE)\n",
    "cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def is_remote_url(url_or_filename):\n",
    "    parsed = urlparse(url_or_filename)\n",
    "    return parsed.scheme in ('http', 'https', 's3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]']"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = []\n",
    "tokens.append('[CLS]')\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_remote_url(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cached_path(\n",
    "    url_or_filename, cache_dir=None, force_download=False, proxies=None, resume_download=False, user_agent=None\n",
    "):\n",
    "    if cache_dir is None:\n",
    "        cache_dir = TRANSFORMERS_CACHE\n",
    "    if isinstance(url_or_filename, Path):\n",
    "        url_or_filename = str(url_or_filename)\n",
    "    if isinstance(cache_dir, Path):\n",
    "        cache_dir = str(cache_dir)\n",
    "\n",
    "    if is_remote_url(url_or_filename):\n",
    "        # URL, so get it from the cache (downloading if necessary)\n",
    "        return get_from_cache(\n",
    "            url_or_filename,\n",
    "            cache_dir=cache_dir,\n",
    "            force_download=force_download,\n",
    "            proxies=proxies,\n",
    "            resume_download=resume_download,\n",
    "            user_agent=user_agent,\n",
    "        )\n",
    "    elif os.path.exists(url_or_filename):\n",
    "        # File, and it exists.\n",
    "        return url_or_filename\n",
    "    elif urlparse(url_or_filename).scheme == \"\":\n",
    "        # File, but it doesn't exist.\n",
    "        raise EnvironmentError(\"file {} not found\".format(url_or_filename))\n",
    "    else:\n",
    "        # Something unknown\n",
    "        raise ValueError(\"unable to parse {} as a URL or as a local path\".format(url_or_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../KorBERT/2_bert_download_002_bert_morp_tensorflow/002_bert_morp_tensorflow/vocab.korean_morp.list'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached_path(vocab_file) # vocab file 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clean_text(text):\n",
    "    output = [] # char을 저장할 list 생성\n",
    "    for char in text:\n",
    "        # 텍스트에서 Char 단위로 출력\n",
    "        cp = ord(char)\n",
    "        if cp == 0 or cp == 0xfffd or _is_control(char):\n",
    "            # \\x00이거나 �이거나 unicode cat.이 C로 시작할 경우\n",
    "            # (개행문자 제외) output에 추가하지 않는다.\n",
    "            continue\n",
    "        if _is_whitespace(char):\n",
    "            # 공백일 경우 \" \"으로 output에 추가\n",
    "            output.append(\" \")\n",
    "        else:\n",
    "            # 이 외의 경우 전부 output에 추가\n",
    "            output.append(char)\n",
    "    # cleaning 작업을 거친 Text를 후처리하여 반환\n",
    "    return \"\".join(output)\n",
    "\n",
    "# char 단위 함수들\n",
    "def _is_whitespace(char):\n",
    "    if char == \" \" or char == '\\t' or char == '\\n' or char == '\\r':\n",
    "        # 개행문자이거나 띄어쓰기면 True 반환\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat == 'Zs':\n",
    "        # unicode category가 Space Seperator면 True 반환\n",
    "        return True\n",
    "    # 이 외의 경우 전부 False 반환\n",
    "    return False\n",
    "\n",
    "def _is_control(char):\n",
    "    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        # 개행문자이면 False 반환\n",
    "        return False\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat.startswith(\"C\"):\n",
    "        # unicode category가\n",
    "        # Cc(Control) \n",
    "        # Cf(format)\n",
    "        # Co(Private Use, is 0)\n",
    "        # Cs(Surrrogate, is 0)일 경우, True 반환\n",
    "        return True\n",
    "    # 이 외의 경우 전부 False 반환\n",
    "    return False\n",
    "\n",
    "def _is_punctuation(char):\n",
    "    # 한국어 형태소 분석기이기 때문에 공백과 같은지 여부만 반환\n",
    "    return char == ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whitespace_tokenize(text):\n",
    "\t\"\"\"Runs basic whitespace cleaning and splitting on a peice of text.\"\"\"\n",
    "\ttext = text.strip()\n",
    "\tif not text:\n",
    "\t\treturn []\n",
    "\ttokens = text.split()\n",
    "\treturn tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_c(text, is_print):\n",
    "    if is_print:\n",
    "        print(text)\n",
    "    else:\n",
    "        print(end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do_lower_case = False\n",
    "do_lower_case = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "곧/NNG\n",
      "곧/NNG\n",
      "False\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print('곧/NNG')\n",
    "print(unicodedata.normalize(\"NFD\", '곧/NNG'))\n",
    "print('곧/NNG' == unicodedata.normalize(\"NFD\", '곧/NNG'))\n",
    "print('곧/NNG' == unicodedata.normalize(\"NFC\",\n",
    "                    unicodedata.normalize(\"NFD\", '곧/NNG')))\n",
    "print(unicodedata.normalize(\"NFC\", '곧/NNG') == '곧/NNG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    --|> Exists in vocab_file.'"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'{:>30}'.format('--|> Exists in vocab_file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************** START TOKENING MORPHLOGY **************\n",
      "\n",
      "Origin Text:   XXX/SL 고객/NNG 님/XSN 항상/MAG XXX/SL 은행/NNG 모란/NNG 역/NNG 지점/NNG 을/JKO 이용/NNG 하/XSV 어/EC 주/VX 시/EP 는/ETM 고객/NNG 님/XSN 께/JKB 감사/NNG 의/JKG 마음/NNG 을/JKO 전하/VV ㅂ니다/EF ./SF 혹시/MAG 업무/NNG 와/JKB 관련/NNG 하/XSV 어/EC 궁금하/VA ㄴ/ETM 점/NNG 이/JKS 있/VA 으시/EP 면/EC 이/MM 번호/NNG 로/JKB 연락/NNG 주/VV 시/EP 기/ETN 바라/VV ㅂ니다/EF ./SF 성심껏/MAG 돕/VV 아/EC 드리/VX 겠/EP 습니다/EF ./SF 또/MAG 혹시/MAG 고객/NNG 만족/NNG 도/NNG 조사/NNG 전화/NNG 를/JKO 받/VV 으시/EP 면/EC 매우/MAG 동의/NNG 하/XSV ㄴ다/EF 로/JKB 칭찬/NNG 하/XSV 어/EC 주/VX 시/EP 어요/EF 조금/NNG 은/JX 쌀쌀하/VA ㄴ/ETM 10/SN 월/NNB 의/JKG 첫주/NNG 이/VCP ㅂ니다/EF ./SF 환절/NNG 기/XSN 감기/NNG 조심/NNG 하/XSV 시/EP 고/EC 따듯하/VA ㄴ/ETM 차/NNG 와/JC 함께/MAG 건강/NNG 하/XSA ㄴ/ETM 한/MM 주/NNB 보내/VV 시/EP 기/ETN 바라/VV ㅂ니다/EF ./SF XXX/SL 은행/NNG 모란/NNG 역/NNG XXX/SL 올림/NNG \n",
      "\n",
      "Cleaned Text:  XXX/SL 고객/NNG 님/XSN 항상/MAG XXX/SL 은행/NNG 모란/NNG 역/NNG 지점/NNG 을/JKO 이용/NNG 하/XSV 어/EC 주/VX 시/EP 는/ETM 고객/NNG 님/XSN 께/JKB 감사/NNG 의/JKG 마음/NNG 을/JKO 전하/VV ㅂ니다/EF ./SF 혹시/MAG 업무/NNG 와/JKB 관련/NNG 하/XSV 어/EC 궁금하/VA ㄴ/ETM 점/NNG 이/JKS 있/VA 으시/EP 면/EC 이/MM 번호/NNG 로/JKB 연락/NNG 주/VV 시/EP 기/ETN 바라/VV ㅂ니다/EF ./SF 성심껏/MAG 돕/VV 아/EC 드리/VX 겠/EP 습니다/EF ./SF 또/MAG 혹시/MAG 고객/NNG 만족/NNG 도/NNG 조사/NNG 전화/NNG 를/JKO 받/VV 으시/EP 면/EC 매우/MAG 동의/NNG 하/XSV ㄴ다/EF 로/JKB 칭찬/NNG 하/XSV 어/EC 주/VX 시/EP 어요/EF 조금/NNG 은/JX 쌀쌀하/VA ㄴ/ETM 10/SN 월/NNB 의/JKG 첫주/NNG 이/VCP ㅂ니다/EF ./SF 환절/NNG 기/XSN 감기/NNG 조심/NNG 하/XSV 시/EP 고/EC 따듯하/VA ㄴ/ETM 차/NNG 와/JC 함께/MAG 건강/NNG 하/XSA ㄴ/ETM 한/MM 주/NNB 보내/VV 시/EP 기/ETN 바라/VV ㅂ니다/EF ./SF XXX/SL 은행/NNG 모란/NNG 역/NNG XXX/SL 올림/NNG \n",
      "\n",
      "Orig. Tokens:  ['XXX/SL', '고객/NNG', '님/XSN', '항상/MAG', 'XXX/SL', '은행/NNG', '모란/NNG', '역/NNG', '지점/NNG', '을/JKO', '이용/NNG', '하/XSV', '어/EC', '주/VX', '시/EP', '는/ETM', '고객/NNG', '님/XSN', '께/JKB', '감사/NNG', '의/JKG', '마음/NNG', '을/JKO', '전하/VV', 'ㅂ니다/EF', './SF', '혹시/MAG', '업무/NNG', '와/JKB', '관련/NNG', '하/XSV', '어/EC', '궁금하/VA', 'ㄴ/ETM', '점/NNG', '이/JKS', '있/VA', '으시/EP', '면/EC', '이/MM', '번호/NNG', '로/JKB', '연락/NNG', '주/VV', '시/EP', '기/ETN', '바라/VV', 'ㅂ니다/EF', './SF', '성심껏/MAG', '돕/VV', '아/EC', '드리/VX', '겠/EP', '습니다/EF', './SF', '또/MAG', '혹시/MAG', '고객/NNG', '만족/NNG', '도/NNG', '조사/NNG', '전화/NNG', '를/JKO', '받/VV', '으시/EP', '면/EC', '매우/MAG', '동의/NNG', '하/XSV', 'ㄴ다/EF', '로/JKB', '칭찬/NNG', '하/XSV', '어/EC', '주/VX', '시/EP', '어요/EF', '조금/NNG', '은/JX', '쌀쌀하/VA', 'ㄴ/ETM', '10/SN', '월/NNB', '의/JKG', '첫주/NNG', '이/VCP', 'ㅂ니다/EF', './SF', '환절/NNG', '기/XSN', '감기/NNG', '조심/NNG', '하/XSV', '시/EP', '고/EC', '따듯하/VA', 'ㄴ/ETM', '차/NNG', '와/JC', '함께/MAG', '건강/NNG', '하/XSA', 'ㄴ/ETM', '한/MM', '주/NNB', '보내/VV', '시/EP', '기/ETN', '바라/VV', 'ㅂ니다/EF', './SF', 'XXX/SL', '은행/NNG', '모란/NNG', '역/NNG', 'XXX/SL', '올림/NNG']\n",
      "\n",
      "\tstripped accent+norm(NFD) Token : XXX/SL\n",
      "\tchars : ['X', 'X', 'X', '/', 'S', 'L']\n",
      "\tstripped accent+norm(NFD) Token : 고객/NNG\n",
      "\tchars : ['ᄀ', 'ᅩ', 'ᄀ', 'ᅢ', 'ᆨ', '/', 'N', 'N', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 님/XSN\n",
      "\tchars : ['ᄂ', 'ᅵ', 'ᆷ', '/', 'X', 'S', 'N']\n",
      "\tstripped accent+norm(NFD) Token : 항상/MAG\n",
      "\tchars : ['ᄒ', 'ᅡ', 'ᆼ', 'ᄉ', 'ᅡ', 'ᆼ', '/', 'M', 'A', 'G']\n",
      "\tstripped accent+norm(NFD) Token : XXX/SL\n",
      "\tchars : ['X', 'X', 'X', '/', 'S', 'L']\n",
      "\tstripped accent+norm(NFD) Token : 은행/NNG\n",
      "\tchars : ['ᄋ', 'ᅳ', 'ᆫ', 'ᄒ', 'ᅢ', 'ᆼ', '/', 'N', 'N', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 모란/NNG\n",
      "\tchars : ['ᄆ', 'ᅩ', 'ᄅ', 'ᅡ', 'ᆫ', '/', 'N', 'N', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 역/NNG\n",
      "\tchars : ['ᄋ', 'ᅧ', 'ᆨ', '/', 'N', 'N', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 지점/NNG\n",
      "\tchars : ['ᄌ', 'ᅵ', 'ᄌ', 'ᅥ', 'ᆷ', '/', 'N', 'N', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 을/JKO\n",
      "\tchars : ['ᄋ', 'ᅳ', 'ᆯ', '/', 'J', 'K', 'O']\n",
      "\tstripped accent+norm(NFD) Token : 이용/NNG\n",
      "\tchars : ['ᄋ', 'ᅵ', 'ᄋ', 'ᅭ', 'ᆼ', '/', 'N', 'N', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 하/XSV\n",
      "\tchars : ['ᄒ', 'ᅡ', '/', 'X', 'S', 'V']\n",
      "\tstripped accent+norm(NFD) Token : 어/EC\n",
      "\tchars : ['ᄋ', 'ᅥ', '/', 'E', 'C']\n",
      "\tstripped accent+norm(NFD) Token : 주/VX\n",
      "\tchars : ['ᄌ', 'ᅮ', '/', 'V', 'X']\n",
      "\tstripped accent+norm(NFD) Token : 시/EP\n",
      "\tchars : ['ᄉ', 'ᅵ', '/', 'E', 'P']\n",
      "\tstripped accent+norm(NFD) Token : 는/ETM\n",
      "\tchars : ['ᄂ', 'ᅳ', 'ᆫ', '/', 'E', 'T', 'M']\n",
      "\tstripped accent+norm(NFD) Token : 고객/NNG\n",
      "\tchars : ['ᄀ', 'ᅩ', 'ᄀ', 'ᅢ', 'ᆨ', '/', 'N', 'N', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 님/XSN\n",
      "\tchars : ['ᄂ', 'ᅵ', 'ᆷ', '/', 'X', 'S', 'N']\n",
      "\tstripped accent+norm(NFD) Token : 께/JKB\n",
      "\tchars : ['ᄁ', 'ᅦ', '/', 'J', 'K', 'B']\n",
      "\tstripped accent+norm(NFD) Token : 감사/NNG\n",
      "\tchars : ['ᄀ', 'ᅡ', 'ᆷ', 'ᄉ', 'ᅡ', '/', 'N', 'N', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 의/JKG\n",
      "\tchars : ['ᄋ', 'ᅴ', '/', 'J', 'K', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 마음/NNG\n",
      "\tchars : ['ᄆ', 'ᅡ', 'ᄋ', 'ᅳ', 'ᆷ', '/', 'N', 'N', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 을/JKO\n",
      "\tchars : ['ᄋ', 'ᅳ', 'ᆯ', '/', 'J', 'K', 'O']\n",
      "\tstripped accent+norm(NFD) Token : 전하/VV\n",
      "\tchars : ['ᄌ', 'ᅥ', 'ᆫ', 'ᄒ', 'ᅡ', '/', 'V', 'V']\n",
      "\tstripped accent+norm(NFD) Token : ㅂ니다/EF\n",
      "\tchars : ['ㅂ', 'ᄂ', 'ᅵ', 'ᄃ', 'ᅡ', '/', 'E', 'F']\n",
      "\tstripped accent+norm(NFD) Token : ./SF\n",
      "\tchars : ['.', '/', 'S', 'F']\n",
      "\tstripped accent+norm(NFD) Token : 혹시/MAG\n",
      "\tchars : ['ᄒ', 'ᅩ', 'ᆨ', 'ᄉ', 'ᅵ', '/', 'M', 'A', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 업무/NNG\n",
      "\tchars : ['ᄋ', 'ᅥ', 'ᆸ', 'ᄆ', 'ᅮ', '/', 'N', 'N', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 와/JKB\n",
      "\tchars : ['ᄋ', 'ᅪ', '/', 'J', 'K', 'B']\n",
      "\tstripped accent+norm(NFD) Token : 관련/NNG\n",
      "\tchars : ['ᄀ', 'ᅪ', 'ᆫ', 'ᄅ', 'ᅧ', 'ᆫ', '/', 'N', 'N', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 하/XSV\n",
      "\tchars : ['ᄒ', 'ᅡ', '/', 'X', 'S', 'V']\n",
      "\tstripped accent+norm(NFD) Token : 어/EC\n",
      "\tchars : ['ᄋ', 'ᅥ', '/', 'E', 'C']\n",
      "\tstripped accent+norm(NFD) Token : 궁금하/VA\n",
      "\tchars : ['ᄀ', 'ᅮ', 'ᆼ', 'ᄀ', 'ᅳ', 'ᆷ', 'ᄒ', 'ᅡ', '/', 'V', 'A']\n",
      "\tstripped accent+norm(NFD) Token : ㄴ/ETM\n",
      "\tchars : ['ㄴ', '/', 'E', 'T', 'M']\n",
      "\tstripped accent+norm(NFD) Token : 점/NNG\n",
      "\tchars : ['ᄌ', 'ᅥ', 'ᆷ', '/', 'N', 'N', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 이/JKS\n",
      "\tchars : ['ᄋ', 'ᅵ', '/', 'J', 'K', 'S']\n",
      "\tstripped accent+norm(NFD) Token : 있/VA\n",
      "\tchars : ['ᄋ', 'ᅵ', 'ᆻ', '/', 'V', 'A']\n",
      "\tstripped accent+norm(NFD) Token : 으시/EP\n",
      "\tchars : ['ᄋ', 'ᅳ', 'ᄉ', 'ᅵ', '/', 'E', 'P']\n",
      "\tstripped accent+norm(NFD) Token : 면/EC\n",
      "\tchars : ['ᄆ', 'ᅧ', 'ᆫ', '/', 'E', 'C']\n",
      "\tstripped accent+norm(NFD) Token : 이/MM\n",
      "\tchars : ['ᄋ', 'ᅵ', '/', 'M', 'M']\n",
      "\tstripped accent+norm(NFD) Token : 번호/NNG\n",
      "\tchars : ['ᄇ', 'ᅥ', 'ᆫ', 'ᄒ', 'ᅩ', '/', 'N', 'N', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 로/JKB\n",
      "\tchars : ['ᄅ', 'ᅩ', '/', 'J', 'K', 'B']\n",
      "\tstripped accent+norm(NFD) Token : 연락/NNG\n",
      "\tchars : ['ᄋ', 'ᅧ', 'ᆫ', 'ᄅ', 'ᅡ', 'ᆨ', '/', 'N', 'N', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 주/VV\n",
      "\tchars : ['ᄌ', 'ᅮ', '/', 'V', 'V']\n",
      "\tstripped accent+norm(NFD) Token : 시/EP\n",
      "\tchars : ['ᄉ', 'ᅵ', '/', 'E', 'P']\n",
      "\tstripped accent+norm(NFD) Token : 기/ETN\n",
      "\tchars : ['ᄀ', 'ᅵ', '/', 'E', 'T', 'N']\n",
      "\tstripped accent+norm(NFD) Token : 바라/VV\n",
      "\tchars : ['ᄇ', 'ᅡ', 'ᄅ', 'ᅡ', '/', 'V', 'V']\n",
      "\tstripped accent+norm(NFD) Token : ㅂ니다/EF\n",
      "\tchars : ['ㅂ', 'ᄂ', 'ᅵ', 'ᄃ', 'ᅡ', '/', 'E', 'F']\n",
      "\tstripped accent+norm(NFD) Token : ./SF\n",
      "\tchars : ['.', '/', 'S', 'F']\n",
      "\tstripped accent+norm(NFD) Token : 성심껏/MAG\n",
      "\tchars : ['ᄉ', 'ᅥ', 'ᆼ', 'ᄉ', 'ᅵ', 'ᆷ', 'ᄁ', 'ᅥ', 'ᆺ', '/', 'M', 'A', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 돕/VV\n",
      "\tchars : ['ᄃ', 'ᅩ', 'ᆸ', '/', 'V', 'V']\n",
      "\tstripped accent+norm(NFD) Token : 아/EC\n",
      "\tchars : ['ᄋ', 'ᅡ', '/', 'E', 'C']\n",
      "\tstripped accent+norm(NFD) Token : 드리/VX\n",
      "\tchars : ['ᄃ', 'ᅳ', 'ᄅ', 'ᅵ', '/', 'V', 'X']\n",
      "\tstripped accent+norm(NFD) Token : 겠/EP\n",
      "\tchars : ['ᄀ', 'ᅦ', 'ᆻ', '/', 'E', 'P']\n",
      "\tstripped accent+norm(NFD) Token : 습니다/EF\n",
      "\tchars : ['ᄉ', 'ᅳ', 'ᆸ', 'ᄂ', 'ᅵ', 'ᄃ', 'ᅡ', '/', 'E', 'F']\n",
      "\tstripped accent+norm(NFD) Token : ./SF\n",
      "\tchars : ['.', '/', 'S', 'F']\n",
      "\tstripped accent+norm(NFD) Token : 또/MAG\n",
      "\tchars : ['ᄄ', 'ᅩ', '/', 'M', 'A', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 혹시/MAG\n",
      "\tchars : ['ᄒ', 'ᅩ', 'ᆨ', 'ᄉ', 'ᅵ', '/', 'M', 'A', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 고객/NNG\n",
      "\tchars : ['ᄀ', 'ᅩ', 'ᄀ', 'ᅢ', 'ᆨ', '/', 'N', 'N', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 만족/NNG\n",
      "\tchars : ['ᄆ', 'ᅡ', 'ᆫ', 'ᄌ', 'ᅩ', 'ᆨ', '/', 'N', 'N', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 도/NNG\n",
      "\tchars : ['ᄃ', 'ᅩ', '/', 'N', 'N', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 조사/NNG\n",
      "\tchars : ['ᄌ', 'ᅩ', 'ᄉ', 'ᅡ', '/', 'N', 'N', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 전화/NNG\n",
      "\tchars : ['ᄌ', 'ᅥ', 'ᆫ', 'ᄒ', 'ᅪ', '/', 'N', 'N', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 를/JKO\n",
      "\tchars : ['ᄅ', 'ᅳ', 'ᆯ', '/', 'J', 'K', 'O']\n",
      "\tstripped accent+norm(NFD) Token : 받/VV\n",
      "\tchars : ['ᄇ', 'ᅡ', 'ᆮ', '/', 'V', 'V']\n",
      "\tstripped accent+norm(NFD) Token : 으시/EP\n",
      "\tchars : ['ᄋ', 'ᅳ', 'ᄉ', 'ᅵ', '/', 'E', 'P']\n",
      "\tstripped accent+norm(NFD) Token : 면/EC\n",
      "\tchars : ['ᄆ', 'ᅧ', 'ᆫ', '/', 'E', 'C']\n",
      "\tstripped accent+norm(NFD) Token : 매우/MAG\n",
      "\tchars : ['ᄆ', 'ᅢ', 'ᄋ', 'ᅮ', '/', 'M', 'A', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 동의/NNG\n",
      "\tchars : ['ᄃ', 'ᅩ', 'ᆼ', 'ᄋ', 'ᅴ', '/', 'N', 'N', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 하/XSV\n",
      "\tchars : ['ᄒ', 'ᅡ', '/', 'X', 'S', 'V']\n",
      "\tstripped accent+norm(NFD) Token : ㄴ다/EF\n",
      "\tchars : ['ㄴ', 'ᄃ', 'ᅡ', '/', 'E', 'F']\n",
      "\tstripped accent+norm(NFD) Token : 로/JKB\n",
      "\tchars : ['ᄅ', 'ᅩ', '/', 'J', 'K', 'B']\n",
      "\tstripped accent+norm(NFD) Token : 칭찬/NNG\n",
      "\tchars : ['ᄎ', 'ᅵ', 'ᆼ', 'ᄎ', 'ᅡ', 'ᆫ', '/', 'N', 'N', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 하/XSV\n",
      "\tchars : ['ᄒ', 'ᅡ', '/', 'X', 'S', 'V']\n",
      "\tstripped accent+norm(NFD) Token : 어/EC\n",
      "\tchars : ['ᄋ', 'ᅥ', '/', 'E', 'C']\n",
      "\tstripped accent+norm(NFD) Token : 주/VX\n",
      "\tchars : ['ᄌ', 'ᅮ', '/', 'V', 'X']\n",
      "\tstripped accent+norm(NFD) Token : 시/EP\n",
      "\tchars : ['ᄉ', 'ᅵ', '/', 'E', 'P']\n",
      "\tstripped accent+norm(NFD) Token : 어요/EF\n",
      "\tchars : ['ᄋ', 'ᅥ', 'ᄋ', 'ᅭ', '/', 'E', 'F']\n",
      "\tstripped accent+norm(NFD) Token : 조금/NNG\n",
      "\tchars : ['ᄌ', 'ᅩ', 'ᄀ', 'ᅳ', 'ᆷ', '/', 'N', 'N', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 은/JX\n",
      "\tchars : ['ᄋ', 'ᅳ', 'ᆫ', '/', 'J', 'X']\n",
      "\tstripped accent+norm(NFD) Token : 쌀쌀하/VA\n",
      "\tchars : ['ᄊ', 'ᅡ', 'ᆯ', 'ᄊ', 'ᅡ', 'ᆯ', 'ᄒ', 'ᅡ', '/', 'V', 'A']\n",
      "\tstripped accent+norm(NFD) Token : ㄴ/ETM\n",
      "\tchars : ['ㄴ', '/', 'E', 'T', 'M']\n",
      "\tstripped accent+norm(NFD) Token : 10/SN\n",
      "\tchars : ['1', '0', '/', 'S', 'N']\n",
      "\tstripped accent+norm(NFD) Token : 월/NNB\n",
      "\tchars : ['ᄋ', 'ᅯ', 'ᆯ', '/', 'N', 'N', 'B']\n",
      "\tstripped accent+norm(NFD) Token : 의/JKG\n",
      "\tchars : ['ᄋ', 'ᅴ', '/', 'J', 'K', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 첫주/NNG\n",
      "\tchars : ['ᄎ', 'ᅥ', 'ᆺ', 'ᄌ', 'ᅮ', '/', 'N', 'N', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 이/VCP\n",
      "\tchars : ['ᄋ', 'ᅵ', '/', 'V', 'C', 'P']\n",
      "\tstripped accent+norm(NFD) Token : ㅂ니다/EF\n",
      "\tchars : ['ㅂ', 'ᄂ', 'ᅵ', 'ᄃ', 'ᅡ', '/', 'E', 'F']\n",
      "\tstripped accent+norm(NFD) Token : ./SF\n",
      "\tchars : ['.', '/', 'S', 'F']\n",
      "\tstripped accent+norm(NFD) Token : 환절/NNG\n",
      "\tchars : ['ᄒ', 'ᅪ', 'ᆫ', 'ᄌ', 'ᅥ', 'ᆯ', '/', 'N', 'N', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 기/XSN\n",
      "\tchars : ['ᄀ', 'ᅵ', '/', 'X', 'S', 'N']\n",
      "\tstripped accent+norm(NFD) Token : 감기/NNG\n",
      "\tchars : ['ᄀ', 'ᅡ', 'ᆷ', 'ᄀ', 'ᅵ', '/', 'N', 'N', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 조심/NNG\n",
      "\tchars : ['ᄌ', 'ᅩ', 'ᄉ', 'ᅵ', 'ᆷ', '/', 'N', 'N', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 하/XSV\n",
      "\tchars : ['ᄒ', 'ᅡ', '/', 'X', 'S', 'V']\n",
      "\tstripped accent+norm(NFD) Token : 시/EP\n",
      "\tchars : ['ᄉ', 'ᅵ', '/', 'E', 'P']\n",
      "\tstripped accent+norm(NFD) Token : 고/EC\n",
      "\tchars : ['ᄀ', 'ᅩ', '/', 'E', 'C']\n",
      "\tstripped accent+norm(NFD) Token : 따듯하/VA\n",
      "\tchars : ['ᄄ', 'ᅡ', 'ᄃ', 'ᅳ', 'ᆺ', 'ᄒ', 'ᅡ', '/', 'V', 'A']\n",
      "\tstripped accent+norm(NFD) Token : ㄴ/ETM\n",
      "\tchars : ['ㄴ', '/', 'E', 'T', 'M']\n",
      "\tstripped accent+norm(NFD) Token : 차/NNG\n",
      "\tchars : ['ᄎ', 'ᅡ', '/', 'N', 'N', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 와/JC\n",
      "\tchars : ['ᄋ', 'ᅪ', '/', 'J', 'C']\n",
      "\tstripped accent+norm(NFD) Token : 함께/MAG\n",
      "\tchars : ['ᄒ', 'ᅡ', 'ᆷ', 'ᄁ', 'ᅦ', '/', 'M', 'A', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 건강/NNG\n",
      "\tchars : ['ᄀ', 'ᅥ', 'ᆫ', 'ᄀ', 'ᅡ', 'ᆼ', '/', 'N', 'N', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 하/XSA\n",
      "\tchars : ['ᄒ', 'ᅡ', '/', 'X', 'S', 'A']\n",
      "\tstripped accent+norm(NFD) Token : ㄴ/ETM\n",
      "\tchars : ['ㄴ', '/', 'E', 'T', 'M']\n",
      "\tstripped accent+norm(NFD) Token : 한/MM\n",
      "\tchars : ['ᄒ', 'ᅡ', 'ᆫ', '/', 'M', 'M']\n",
      "\tstripped accent+norm(NFD) Token : 주/NNB\n",
      "\tchars : ['ᄌ', 'ᅮ', '/', 'N', 'N', 'B']\n",
      "\tstripped accent+norm(NFD) Token : 보내/VV\n",
      "\tchars : ['ᄇ', 'ᅩ', 'ᄂ', 'ᅢ', '/', 'V', 'V']\n",
      "\tstripped accent+norm(NFD) Token : 시/EP\n",
      "\tchars : ['ᄉ', 'ᅵ', '/', 'E', 'P']\n",
      "\tstripped accent+norm(NFD) Token : 기/ETN\n",
      "\tchars : ['ᄀ', 'ᅵ', '/', 'E', 'T', 'N']\n",
      "\tstripped accent+norm(NFD) Token : 바라/VV\n",
      "\tchars : ['ᄇ', 'ᅡ', 'ᄅ', 'ᅡ', '/', 'V', 'V']\n",
      "\tstripped accent+norm(NFD) Token : ㅂ니다/EF\n",
      "\tchars : ['ㅂ', 'ᄂ', 'ᅵ', 'ᄃ', 'ᅡ', '/', 'E', 'F']\n",
      "\tstripped accent+norm(NFD) Token : ./SF\n",
      "\tchars : ['.', '/', 'S', 'F']\n",
      "\tstripped accent+norm(NFD) Token : XXX/SL\n",
      "\tchars : ['X', 'X', 'X', '/', 'S', 'L']\n",
      "\tstripped accent+norm(NFD) Token : 은행/NNG\n",
      "\tchars : ['ᄋ', 'ᅳ', 'ᆫ', 'ᄒ', 'ᅢ', 'ᆼ', '/', 'N', 'N', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 모란/NNG\n",
      "\tchars : ['ᄆ', 'ᅩ', 'ᄅ', 'ᅡ', 'ᆫ', '/', 'N', 'N', 'G']\n",
      "\tstripped accent+norm(NFD) Token : 역/NNG\n",
      "\tchars : ['ᄋ', 'ᅧ', 'ᆨ', '/', 'N', 'N', 'G']\n",
      "\tstripped accent+norm(NFD) Token : XXX/SL\n",
      "\tchars : ['X', 'X', 'X', '/', 'S', 'L']\n",
      "\tstripped accent+norm(NFD) Token : 올림/NNG\n",
      "\tchars : ['ᄋ', 'ᅩ', 'ᆯ', 'ᄅ', 'ᅵ', 'ᆷ', '/', 'N', 'N', 'G']\n",
      "\n",
      "Basic_split_tokens :  ['XXX/SL', '고객/NNG', '님/XSN', '항상/MAG', 'XXX/SL', '은행/NNG', '모란/NNG', '역/NNG', '지점/NNG', '을/JKO', '이용/NNG', '하/XSV', '어/EC', '주/VX', '시/EP', '는/ETM', '고객/NNG', '님/XSN', '께/JKB', '감사/NNG', '의/JKG', '마음/NNG', '을/JKO', '전하/VV', 'ㅂ니다/EF', './SF', '혹시/MAG', '업무/NNG', '와/JKB', '관련/NNG', '하/XSV', '어/EC', '궁금하/VA', 'ㄴ/ETM', '점/NNG', '이/JKS', '있/VA', '으시/EP', '면/EC', '이/MM', '번호/NNG', '로/JKB', '연락/NNG', '주/VV', '시/EP', '기/ETN', '바라/VV', 'ㅂ니다/EF', './SF', '성심껏/MAG', '돕/VV', '아/EC', '드리/VX', '겠/EP', '습니다/EF', './SF', '또/MAG', '혹시/MAG', '고객/NNG', '만족/NNG', '도/NNG', '조사/NNG', '전화/NNG', '를/JKO', '받/VV', '으시/EP', '면/EC', '매우/MAG', '동의/NNG', '하/XSV', 'ㄴ다/EF', '로/JKB', '칭찬/NNG', '하/XSV', '어/EC', '주/VX', '시/EP', '어요/EF', '조금/NNG', '은/JX', '쌀쌀하/VA', 'ㄴ/ETM', '10/SN', '월/NNB', '의/JKG', '첫주/NNG', '이/VCP', 'ㅂ니다/EF', './SF', '환절/NNG', '기/XSN', '감기/NNG', '조심/NNG', '하/XSV', '시/EP', '고/EC', '따듯하/VA', 'ㄴ/ETM', '차/NNG', '와/JC', '함께/MAG', '건강/NNG', '하/XSA', 'ㄴ/ETM', '한/MM', '주/NNB', '보내/VV', '시/EP', '기/ETN', '바라/VV', 'ㅂ니다/EF', './SF', 'XXX/SL', '은행/NNG', '모란/NNG', '역/NNG', 'XXX/SL', '올림/NNG']\n",
      "\n",
      "Basic_output Tokens:  ['XXX/SL', '고객/NNG', '님/XSN', '항상/MAG', 'XXX/SL', '은행/NNG', '모란/NNG', '역/NNG', '지점/NNG', '을/JKO', '이용/NNG', '하/XSV', '어/EC', '주/VX', '시/EP', '는/ETM', '고객/NNG', '님/XSN', '께/JKB', '감사/NNG', '의/JKG', '마음/NNG', '을/JKO', '전하/VV', 'ㅂ니다/EF', './SF', '혹시/MAG', '업무/NNG', '와/JKB', '관련/NNG', '하/XSV', '어/EC', '궁금하/VA', 'ㄴ/ETM', '점/NNG', '이/JKS', '있/VA', '으시/EP', '면/EC', '이/MM', '번호/NNG', '로/JKB', '연락/NNG', '주/VV', '시/EP', '기/ETN', '바라/VV', 'ㅂ니다/EF', './SF', '성심껏/MAG', '돕/VV', '아/EC', '드리/VX', '겠/EP', '습니다/EF', './SF', '또/MAG', '혹시/MAG', '고객/NNG', '만족/NNG', '도/NNG', '조사/NNG', '전화/NNG', '를/JKO', '받/VV', '으시/EP', '면/EC', '매우/MAG', '동의/NNG', '하/XSV', 'ㄴ다/EF', '로/JKB', '칭찬/NNG', '하/XSV', '어/EC', '주/VX', '시/EP', '어요/EF', '조금/NNG', '은/JX', '쌀쌀하/VA', 'ㄴ/ETM', '10/SN', '월/NNB', '의/JKG', '첫주/NNG', '이/VCP', 'ㅂ니다/EF', './SF', '환절/NNG', '기/XSN', '감기/NNG', '조심/NNG', '하/XSV', '시/EP', '고/EC', '따듯하/VA', 'ㄴ/ETM', '차/NNG', '와/JC', '함께/MAG', '건강/NNG', '하/XSA', 'ㄴ/ETM', '한/MM', '주/NNB', '보내/VV', '시/EP', '기/ETN', '바라/VV', 'ㅂ니다/EF', './SF', 'XXX/SL', '은행/NNG', '모란/NNG', '역/NNG', 'XXX/SL', '올림/NNG']\n",
      "\n",
      "************** START GREEDY LONGEST MATCH FIRST ALGORITHM **************\n",
      "\t ['XXX/SL_']\n",
      "\t\t\tXXX/SL_\n",
      "\t\t\tXXX/SL\n",
      "\t\t\tXXX/S\n",
      "\t\t\tXXX/\n",
      "\t\t\tXXX\n",
      "\t\t\tXX\n",
      "\t\t\tX\r",
      "\t\t\tX              --|> Exists in vocab_file.\n",
      "\t\t\tXX/SL_\n",
      "\t\t\tXX/SL\n",
      "\t\t\tXX/S\n",
      "\t\t\tXX/\n",
      "\t\t\tXX\n",
      "\t\t\tX\r",
      "\t\t\tX              --|> Exists in vocab_file.\n",
      "\t\t\tX/SL_\r",
      "\t\t\tX/SL_          --|> Exists in vocab_file.\n",
      "\t ['고객/NNG_']\n",
      "\t\t\t고객/NNG_\r",
      "\t\t\t고객/NNG_        --|> Exists in vocab_file.\n",
      "\t ['님/XSN_']\n",
      "\t\t\t님/XSN_\r",
      "\t\t\t님/XSN_         --|> Exists in vocab_file.\n",
      "\t ['항상/MAG_']\n",
      "\t\t\t항상/MAG_\r",
      "\t\t\t항상/MAG_        --|> Exists in vocab_file.\n",
      "\t ['XXX/SL_']\n",
      "\t\t\tXXX/SL_\n",
      "\t\t\tXXX/SL\n",
      "\t\t\tXXX/S\n",
      "\t\t\tXXX/\n",
      "\t\t\tXXX\n",
      "\t\t\tXX\n",
      "\t\t\tX\r",
      "\t\t\tX              --|> Exists in vocab_file.\n",
      "\t\t\tXX/SL_\n",
      "\t\t\tXX/SL\n",
      "\t\t\tXX/S\n",
      "\t\t\tXX/\n",
      "\t\t\tXX\n",
      "\t\t\tX\r",
      "\t\t\tX              --|> Exists in vocab_file.\n",
      "\t\t\tX/SL_\r",
      "\t\t\tX/SL_          --|> Exists in vocab_file.\n",
      "\t ['은행/NNG_']\n",
      "\t\t\t은행/NNG_\r",
      "\t\t\t은행/NNG_        --|> Exists in vocab_file.\n",
      "\t ['모란/NNG_']\n",
      "\t\t\t모란/NNG_\n",
      "\t\t\t모란/NNG\n",
      "\t\t\t모란/NN\n",
      "\t\t\t모란/N\n",
      "\t\t\t모란/\n",
      "\t\t\t모란\n",
      "\t\t\t모라\n",
      "\t\t\t모ᄅ\n",
      "\t\t\t모\r",
      "\t\t\t모              --|> Exists in vocab_file.\n",
      "\t\t\t란/NNG_\r",
      "\t\t\t란/NNG_         --|> Exists in vocab_file.\n",
      "\t ['역/NNG_']\n",
      "\t\t\t역/NNG_\r",
      "\t\t\t역/NNG_         --|> Exists in vocab_file.\n",
      "\t ['지점/NNG_']\n",
      "\t\t\t지점/NNG_\r",
      "\t\t\t지점/NNG_        --|> Exists in vocab_file.\n",
      "\t ['을/JKO_']\n",
      "\t\t\t을/JKO_\r",
      "\t\t\t을/JKO_         --|> Exists in vocab_file.\n",
      "\t ['이용/NNG_']\n",
      "\t\t\t이용/NNG_\r",
      "\t\t\t이용/NNG_        --|> Exists in vocab_file.\n",
      "\t ['하/XSV_']\n",
      "\t\t\t하/XSV_\r",
      "\t\t\t하/XSV_         --|> Exists in vocab_file.\n",
      "\t ['어/EC_']\n",
      "\t\t\t어/EC_\r",
      "\t\t\t어/EC_          --|> Exists in vocab_file.\n",
      "\t ['주/VX_']\n",
      "\t\t\t주/VX_\r",
      "\t\t\t주/VX_          --|> Exists in vocab_file.\n",
      "\t ['시/EP_']\n",
      "\t\t\t시/EP_\r",
      "\t\t\t시/EP_          --|> Exists in vocab_file.\n",
      "\t ['는/ETM_']\n",
      "\t\t\t는/ETM_\r",
      "\t\t\t는/ETM_         --|> Exists in vocab_file.\n",
      "\t ['고객/NNG_']\n",
      "\t\t\t고객/NNG_\r",
      "\t\t\t고객/NNG_        --|> Exists in vocab_file.\n",
      "\t ['님/XSN_']\n",
      "\t\t\t님/XSN_\r",
      "\t\t\t님/XSN_         --|> Exists in vocab_file.\n",
      "\t ['께/JKB_']\n",
      "\t\t\t께/JKB_\r",
      "\t\t\t께/JKB_         --|> Exists in vocab_file.\n",
      "\t ['감사/NNG_']\n",
      "\t\t\t감사/NNG_\r",
      "\t\t\t감사/NNG_        --|> Exists in vocab_file.\n",
      "\t ['의/JKG_']\n",
      "\t\t\t의/JKG_\r",
      "\t\t\t의/JKG_         --|> Exists in vocab_file.\n",
      "\t ['마음/NNG_']\n",
      "\t\t\t마음/NNG_\r",
      "\t\t\t마음/NNG_        --|> Exists in vocab_file.\n",
      "\t ['을/JKO_']\n",
      "\t\t\t을/JKO_\r",
      "\t\t\t을/JKO_         --|> Exists in vocab_file.\n",
      "\t ['전하/VV_']\n",
      "\t\t\t전하/VV_\r",
      "\t\t\t전하/VV_         --|> Exists in vocab_file.\n",
      "\t ['ㅂ니다/EF_']\n",
      "\t\t\tㅂ니다/EF_\r",
      "\t\t\tㅂ니다/EF_        --|> Exists in vocab_file.\n",
      "\t ['./SF_']\n",
      "\t\t\t./SF_\r",
      "\t\t\t./SF_          --|> Exists in vocab_file.\n",
      "\t ['혹시/MAG_']\n",
      "\t\t\t혹시/MAG_\r",
      "\t\t\t혹시/MAG_        --|> Exists in vocab_file.\n",
      "\t ['업무/NNG_']\n",
      "\t\t\t업무/NNG_\r",
      "\t\t\t업무/NNG_        --|> Exists in vocab_file.\n",
      "\t ['와/JKB_']\n",
      "\t\t\t와/JKB_\r",
      "\t\t\t와/JKB_         --|> Exists in vocab_file.\n",
      "\t ['관련/NNG_']\n",
      "\t\t\t관련/NNG_\r",
      "\t\t\t관련/NNG_        --|> Exists in vocab_file.\n",
      "\t ['하/XSV_']\n",
      "\t\t\t하/XSV_\r",
      "\t\t\t하/XSV_         --|> Exists in vocab_file.\n",
      "\t ['어/EC_']\n",
      "\t\t\t어/EC_\r",
      "\t\t\t어/EC_          --|> Exists in vocab_file.\n",
      "\t ['궁금하/VA_']\n",
      "\t\t\t궁금하/VA_\r",
      "\t\t\t궁금하/VA_        --|> Exists in vocab_file.\n",
      "\t ['ㄴ/ETM_']\n",
      "\t\t\tㄴ/ETM_\r",
      "\t\t\tㄴ/ETM_         --|> Exists in vocab_file.\n",
      "\t ['점/NNG_']\n",
      "\t\t\t점/NNG_\r",
      "\t\t\t점/NNG_         --|> Exists in vocab_file.\n",
      "\t ['이/JKS_']\n",
      "\t\t\t이/JKS_\r",
      "\t\t\t이/JKS_         --|> Exists in vocab_file.\n",
      "\t ['있/VA_']\n",
      "\t\t\t있/VA_\r",
      "\t\t\t있/VA_          --|> Exists in vocab_file.\n",
      "\t ['으시/EP_']\n",
      "\t\t\t으시/EP_\r",
      "\t\t\t으시/EP_         --|> Exists in vocab_file.\n",
      "\t ['면/EC_']\n",
      "\t\t\t면/EC_\r",
      "\t\t\t면/EC_          --|> Exists in vocab_file.\n",
      "\t ['이/MM_']\n",
      "\t\t\t이/MM_\r",
      "\t\t\t이/MM_          --|> Exists in vocab_file.\n",
      "\t ['번호/NNG_']\n",
      "\t\t\t번호/NNG_\r",
      "\t\t\t번호/NNG_        --|> Exists in vocab_file.\n",
      "\t ['로/JKB_']\n",
      "\t\t\t로/JKB_\r",
      "\t\t\t로/JKB_         --|> Exists in vocab_file.\n",
      "\t ['연락/NNG_']\n",
      "\t\t\t연락/NNG_\r",
      "\t\t\t연락/NNG_        --|> Exists in vocab_file.\n",
      "\t ['주/VV_']\n",
      "\t\t\t주/VV_\r",
      "\t\t\t주/VV_          --|> Exists in vocab_file.\n",
      "\t ['시/EP_']\n",
      "\t\t\t시/EP_\r",
      "\t\t\t시/EP_          --|> Exists in vocab_file.\n",
      "\t ['기/ETN_']\n",
      "\t\t\t기/ETN_\r",
      "\t\t\t기/ETN_         --|> Exists in vocab_file.\n",
      "\t ['바라/VV_']\n",
      "\t\t\t바라/VV_\r",
      "\t\t\t바라/VV_         --|> Exists in vocab_file.\n",
      "\t ['ㅂ니다/EF_']\n",
      "\t\t\tㅂ니다/EF_\r",
      "\t\t\tㅂ니다/EF_        --|> Exists in vocab_file.\n",
      "\t ['./SF_']\n",
      "\t\t\t./SF_\r",
      "\t\t\t./SF_          --|> Exists in vocab_file.\n",
      "\t ['성심껏/MAG_']\n",
      "\t\t\t성심껏/MAG_\n",
      "\t\t\t성심껏/MAG\n",
      "\t\t\t성심껏/MA\n",
      "\t\t\t성심껏/M\n",
      "\t\t\t성심껏/\n",
      "\t\t\t성심껏\n",
      "\t\t\t성심꺼\n",
      "\t\t\t성심ᄁ\n",
      "\t\t\t성심\n",
      "\t\t\t성시\n",
      "\t\t\t성ᄉ\n",
      "\t\t\t성\r",
      "\t\t\t성              --|> Exists in vocab_file.\n",
      "\t\t\t심껏/MAG_\n",
      "\t\t\t심껏/MAG\n",
      "\t\t\t심껏/MA\n",
      "\t\t\t심껏/M\n",
      "\t\t\t심껏/\n",
      "\t\t\t심껏\n",
      "\t\t\t심꺼\n",
      "\t\t\t심ᄁ\n",
      "\t\t\t심\r",
      "\t\t\t심              --|> Exists in vocab_file.\n",
      "\t\t\t껏/MAG_\r",
      "\t\t\t껏/MAG_         --|> Exists in vocab_file.\n",
      "\t ['돕/VV_']\n",
      "\t\t\t돕/VV_\r",
      "\t\t\t돕/VV_          --|> Exists in vocab_file.\n",
      "\t ['아/EC_']\n",
      "\t\t\t아/EC_\r",
      "\t\t\t아/EC_          --|> Exists in vocab_file.\n",
      "\t ['드리/VX_']\n",
      "\t\t\t드리/VX_\r",
      "\t\t\t드리/VX_         --|> Exists in vocab_file.\n",
      "\t ['겠/EP_']\n",
      "\t\t\t겠/EP_\r",
      "\t\t\t겠/EP_          --|> Exists in vocab_file.\n",
      "\t ['습니다/EF_']\n",
      "\t\t\t습니다/EF_\r",
      "\t\t\t습니다/EF_        --|> Exists in vocab_file.\n",
      "\t ['./SF_']\n",
      "\t\t\t./SF_\r",
      "\t\t\t./SF_          --|> Exists in vocab_file.\n",
      "\t ['또/MAG_']\n",
      "\t\t\t또/MAG_\r",
      "\t\t\t또/MAG_         --|> Exists in vocab_file.\n",
      "\t ['혹시/MAG_']\n",
      "\t\t\t혹시/MAG_\r",
      "\t\t\t혹시/MAG_        --|> Exists in vocab_file.\n",
      "\t ['고객/NNG_']\n",
      "\t\t\t고객/NNG_\r",
      "\t\t\t고객/NNG_        --|> Exists in vocab_file.\n",
      "\t ['만족/NNG_']\n",
      "\t\t\t만족/NNG_\r",
      "\t\t\t만족/NNG_        --|> Exists in vocab_file.\n",
      "\t ['도/NNG_']\n",
      "\t\t\t도/NNG_\r",
      "\t\t\t도/NNG_         --|> Exists in vocab_file.\n",
      "\t ['조사/NNG_']\n",
      "\t\t\t조사/NNG_\r",
      "\t\t\t조사/NNG_        --|> Exists in vocab_file.\n",
      "\t ['전화/NNG_']\n",
      "\t\t\t전화/NNG_\r",
      "\t\t\t전화/NNG_        --|> Exists in vocab_file.\n",
      "\t ['를/JKO_']\n",
      "\t\t\t를/JKO_\r",
      "\t\t\t를/JKO_         --|> Exists in vocab_file.\n",
      "\t ['받/VV_']\n",
      "\t\t\t받/VV_\r",
      "\t\t\t받/VV_          --|> Exists in vocab_file.\n",
      "\t ['으시/EP_']\n",
      "\t\t\t으시/EP_\r",
      "\t\t\t으시/EP_         --|> Exists in vocab_file.\n",
      "\t ['면/EC_']\n",
      "\t\t\t면/EC_\r",
      "\t\t\t면/EC_          --|> Exists in vocab_file.\n",
      "\t ['매우/MAG_']\n",
      "\t\t\t매우/MAG_\r",
      "\t\t\t매우/MAG_        --|> Exists in vocab_file.\n",
      "\t ['동의/NNG_']\n",
      "\t\t\t동의/NNG_\r",
      "\t\t\t동의/NNG_        --|> Exists in vocab_file.\n",
      "\t ['하/XSV_']\n",
      "\t\t\t하/XSV_\r",
      "\t\t\t하/XSV_         --|> Exists in vocab_file.\n",
      "\t ['ㄴ다/EF_']\n",
      "\t\t\tㄴ다/EF_\r",
      "\t\t\tㄴ다/EF_         --|> Exists in vocab_file.\n",
      "\t ['로/JKB_']\n",
      "\t\t\t로/JKB_\r",
      "\t\t\t로/JKB_         --|> Exists in vocab_file.\n",
      "\t ['칭찬/NNG_']\n",
      "\t\t\t칭찬/NNG_\r",
      "\t\t\t칭찬/NNG_        --|> Exists in vocab_file.\n",
      "\t ['하/XSV_']\n",
      "\t\t\t하/XSV_\r",
      "\t\t\t하/XSV_         --|> Exists in vocab_file.\n",
      "\t ['어/EC_']\n",
      "\t\t\t어/EC_\r",
      "\t\t\t어/EC_          --|> Exists in vocab_file.\n",
      "\t ['주/VX_']\n",
      "\t\t\t주/VX_\r",
      "\t\t\t주/VX_          --|> Exists in vocab_file.\n",
      "\t ['시/EP_']\n",
      "\t\t\t시/EP_\r",
      "\t\t\t시/EP_          --|> Exists in vocab_file.\n",
      "\t ['어요/EF_']\n",
      "\t\t\t어요/EF_\r",
      "\t\t\t어요/EF_         --|> Exists in vocab_file.\n",
      "\t ['조금/NNG_']\n",
      "\t\t\t조금/NNG_\r",
      "\t\t\t조금/NNG_        --|> Exists in vocab_file.\n",
      "\t ['은/JX_']\n",
      "\t\t\t은/JX_\r",
      "\t\t\t은/JX_          --|> Exists in vocab_file.\n",
      "\t ['쌀쌀하/VA_']\n",
      "\t\t\t쌀쌀하/VA_\n",
      "\t\t\t쌀쌀하/VA\n",
      "\t\t\t쌀쌀하/V\n",
      "\t\t\t쌀쌀하/\n",
      "\t\t\t쌀쌀하\n",
      "\t\t\t쌀쌀ᄒ\n",
      "\t\t\t쌀쌀\n",
      "\t\t\t쌀싸\n",
      "\t\t\t쌀ᄊ\n",
      "\t\t\t쌀\r",
      "\t\t\t쌀              --|> Exists in vocab_file.\n",
      "\t\t\t쌀하/VA_\n",
      "\t\t\t쌀하/VA\n",
      "\t\t\t쌀하/V\n",
      "\t\t\t쌀하/\n",
      "\t\t\t쌀하\n",
      "\t\t\t쌀ᄒ\n",
      "\t\t\t쌀\r",
      "\t\t\t쌀              --|> Exists in vocab_file.\n",
      "\t\t\t하/VA_\r",
      "\t\t\t하/VA_          --|> Exists in vocab_file.\n",
      "\t ['ㄴ/ETM_']\n",
      "\t\t\tㄴ/ETM_\r",
      "\t\t\tㄴ/ETM_         --|> Exists in vocab_file.\n",
      "\t ['10/SN_']\n",
      "\t\t\t10/SN_\r",
      "\t\t\t10/SN_         --|> Exists in vocab_file.\n",
      "\t ['월/NNB_']\n",
      "\t\t\t월/NNB_\r",
      "\t\t\t월/NNB_         --|> Exists in vocab_file.\n",
      "\t ['의/JKG_']\n",
      "\t\t\t의/JKG_\r",
      "\t\t\t의/JKG_         --|> Exists in vocab_file.\n",
      "\t ['첫주/NNG_']\n",
      "\t\t\t첫주/NNG_\n",
      "\t\t\t첫주/NNG\n",
      "\t\t\t첫주/NN\n",
      "\t\t\t첫주/N\n",
      "\t\t\t첫주/\n",
      "\t\t\t첫주\n",
      "\t\t\t첫ᄌ\n",
      "\t\t\t첫\r",
      "\t\t\t첫              --|> Exists in vocab_file.\n",
      "\t\t\t주/NNG_\r",
      "\t\t\t주/NNG_         --|> Exists in vocab_file.\n",
      "\t ['이/VCP_']\n",
      "\t\t\t이/VCP_\r",
      "\t\t\t이/VCP_         --|> Exists in vocab_file.\n",
      "\t ['ㅂ니다/EF_']\n",
      "\t\t\tㅂ니다/EF_\r",
      "\t\t\tㅂ니다/EF_        --|> Exists in vocab_file.\n",
      "\t ['./SF_']\n",
      "\t\t\t./SF_\r",
      "\t\t\t./SF_          --|> Exists in vocab_file.\n",
      "\t ['환절/NNG_']\n",
      "\t\t\t환절/NNG_\n",
      "\t\t\t환절/NNG\n",
      "\t\t\t환절/NN\n",
      "\t\t\t환절/N\n",
      "\t\t\t환절/\n",
      "\t\t\t환절\n",
      "\t\t\t환저\n",
      "\t\t\t환ᄌ\n",
      "\t\t\t환\r",
      "\t\t\t환              --|> Exists in vocab_file.\n",
      "\t\t\t절/NNG_\r",
      "\t\t\t절/NNG_         --|> Exists in vocab_file.\n",
      "\t ['기/XSN_']\n",
      "\t\t\t기/XSN_\r",
      "\t\t\t기/XSN_         --|> Exists in vocab_file.\n",
      "\t ['감기/NNG_']\n",
      "\t\t\t감기/NNG_\r",
      "\t\t\t감기/NNG_        --|> Exists in vocab_file.\n",
      "\t ['조심/NNG_']\n",
      "\t\t\t조심/NNG_\r",
      "\t\t\t조심/NNG_        --|> Exists in vocab_file.\n",
      "\t ['하/XSV_']\n",
      "\t\t\t하/XSV_\r",
      "\t\t\t하/XSV_         --|> Exists in vocab_file.\n",
      "\t ['시/EP_']\n",
      "\t\t\t시/EP_\r",
      "\t\t\t시/EP_          --|> Exists in vocab_file.\n",
      "\t ['고/EC_']\n",
      "\t\t\t고/EC_\r",
      "\t\t\t고/EC_          --|> Exists in vocab_file.\n",
      "\t ['따듯하/VA_']\n",
      "\t\t\t따듯하/VA_\n",
      "\t\t\t따듯하/VA\n",
      "\t\t\t따듯하/V\n",
      "\t\t\t따듯하/\n",
      "\t\t\t따듯하\n",
      "\t\t\t따듯ᄒ\n",
      "\t\t\t따듯\n",
      "\t\t\t따드\n",
      "\t\t\t따ᄃ\n",
      "\t\t\t따\r",
      "\t\t\t따              --|> Exists in vocab_file.\n",
      "\t\t\t듯하/VA_\r",
      "\t\t\t듯하/VA_         --|> Exists in vocab_file.\n",
      "\t ['ㄴ/ETM_']\n",
      "\t\t\tㄴ/ETM_\r",
      "\t\t\tㄴ/ETM_         --|> Exists in vocab_file.\n",
      "\t ['차/NNG_']\n",
      "\t\t\t차/NNG_\r",
      "\t\t\t차/NNG_         --|> Exists in vocab_file.\n",
      "\t ['와/JC_']\n",
      "\t\t\t와/JC_\r",
      "\t\t\t와/JC_          --|> Exists in vocab_file.\n",
      "\t ['함께/MAG_']\n",
      "\t\t\t함께/MAG_\r",
      "\t\t\t함께/MAG_        --|> Exists in vocab_file.\n",
      "\t ['건강/NNG_']\n",
      "\t\t\t건강/NNG_\r",
      "\t\t\t건강/NNG_        --|> Exists in vocab_file.\n",
      "\t ['하/XSA_']\n",
      "\t\t\t하/XSA_\r",
      "\t\t\t하/XSA_         --|> Exists in vocab_file.\n",
      "\t ['ㄴ/ETM_']\n",
      "\t\t\tㄴ/ETM_\r",
      "\t\t\tㄴ/ETM_         --|> Exists in vocab_file.\n",
      "\t ['한/MM_']\n",
      "\t\t\t한/MM_\r",
      "\t\t\t한/MM_          --|> Exists in vocab_file.\n",
      "\t ['주/NNB_']\n",
      "\t\t\t주/NNB_\r",
      "\t\t\t주/NNB_         --|> Exists in vocab_file.\n",
      "\t ['보내/VV_']\n",
      "\t\t\t보내/VV_\r",
      "\t\t\t보내/VV_         --|> Exists in vocab_file.\n",
      "\t ['시/EP_']\n",
      "\t\t\t시/EP_\r",
      "\t\t\t시/EP_          --|> Exists in vocab_file.\n",
      "\t ['기/ETN_']\n",
      "\t\t\t기/ETN_\r",
      "\t\t\t기/ETN_         --|> Exists in vocab_file.\n",
      "\t ['바라/VV_']\n",
      "\t\t\t바라/VV_\r",
      "\t\t\t바라/VV_         --|> Exists in vocab_file.\n",
      "\t ['ㅂ니다/EF_']\n",
      "\t\t\tㅂ니다/EF_\r",
      "\t\t\tㅂ니다/EF_        --|> Exists in vocab_file.\n",
      "\t ['./SF_']\n",
      "\t\t\t./SF_\r",
      "\t\t\t./SF_          --|> Exists in vocab_file.\n",
      "\t ['XXX/SL_']\n",
      "\t\t\tXXX/SL_\n",
      "\t\t\tXXX/SL\n",
      "\t\t\tXXX/S\n",
      "\t\t\tXXX/\n",
      "\t\t\tXXX\n",
      "\t\t\tXX\n",
      "\t\t\tX\r",
      "\t\t\tX              --|> Exists in vocab_file.\n",
      "\t\t\tXX/SL_\n",
      "\t\t\tXX/SL\n",
      "\t\t\tXX/S\n",
      "\t\t\tXX/\n",
      "\t\t\tXX\n",
      "\t\t\tX\r",
      "\t\t\tX              --|> Exists in vocab_file.\n",
      "\t\t\tX/SL_\r",
      "\t\t\tX/SL_          --|> Exists in vocab_file.\n",
      "\t ['은행/NNG_']\n",
      "\t\t\t은행/NNG_\r",
      "\t\t\t은행/NNG_        --|> Exists in vocab_file.\n",
      "\t ['모란/NNG_']\n",
      "\t\t\t모란/NNG_\n",
      "\t\t\t모란/NNG\n",
      "\t\t\t모란/NN\n",
      "\t\t\t모란/N\n",
      "\t\t\t모란/\n",
      "\t\t\t모란\n",
      "\t\t\t모라\n",
      "\t\t\t모ᄅ\n",
      "\t\t\t모\r",
      "\t\t\t모              --|> Exists in vocab_file.\n",
      "\t\t\t란/NNG_\r",
      "\t\t\t란/NNG_         --|> Exists in vocab_file.\n",
      "\t ['역/NNG_']\n",
      "\t\t\t역/NNG_\r",
      "\t\t\t역/NNG_         --|> Exists in vocab_file.\n",
      "\t ['XXX/SL_']\n",
      "\t\t\tXXX/SL_\n",
      "\t\t\tXXX/SL\n",
      "\t\t\tXXX/S\n",
      "\t\t\tXXX/\n",
      "\t\t\tXXX\n",
      "\t\t\tXX\n",
      "\t\t\tX\r",
      "\t\t\tX              --|> Exists in vocab_file.\n",
      "\t\t\tXX/SL_\n",
      "\t\t\tXX/SL\n",
      "\t\t\tXX/S\n",
      "\t\t\tXX/\n",
      "\t\t\tXX\n",
      "\t\t\tX\r",
      "\t\t\tX              --|> Exists in vocab_file.\n",
      "\t\t\tX/SL_\r",
      "\t\t\tX/SL_          --|> Exists in vocab_file.\n",
      "\t ['올림/NNG_']\n",
      "\t\t\t올림/NNG_\r",
      "\t\t\t올림/NNG_        --|> Exists in vocab_file.\n",
      "\n",
      "Total_split_tokens :  ['X', 'X', 'X/SL_', '고객/NNG_', '님/XSN_', '항상/MAG_', 'X', 'X', 'X/SL_', '은행/NNG_', '모', '란/NNG_', '역/NNG_', '지점/NNG_', '을/JKO_', '이용/NNG_', '하/XSV_', '어/EC_', '주/VX_', '시/EP_', '는/ETM_', '고객/NNG_', '님/XSN_', '께/JKB_', '감사/NNG_', '의/JKG_', '마음/NNG_', '을/JKO_', '전하/VV_', 'ㅂ니다/EF_', './SF_', '혹시/MAG_', '업무/NNG_', '와/JKB_', '관련/NNG_', '하/XSV_', '어/EC_', '궁금하/VA_', 'ㄴ/ETM_', '점/NNG_', '이/JKS_', '있/VA_', '으시/EP_', '면/EC_', '이/MM_', '번호/NNG_', '로/JKB_', '연락/NNG_', '주/VV_', '시/EP_', '기/ETN_', '바라/VV_', 'ㅂ니다/EF_', './SF_', '성', '심', '껏/MAG_', '돕/VV_', '아/EC_', '드리/VX_', '겠/EP_', '습니다/EF_', './SF_', '또/MAG_', '혹시/MAG_', '고객/NNG_', '만족/NNG_', '도/NNG_', '조사/NNG_', '전화/NNG_', '를/JKO_', '받/VV_', '으시/EP_', '면/EC_', '매우/MAG_', '동의/NNG_', '하/XSV_', 'ㄴ다/EF_', '로/JKB_', '칭찬/NNG_', '하/XSV_', '어/EC_', '주/VX_', '시/EP_', '어요/EF_', '조금/NNG_', '은/JX_', '쌀', '쌀', '하/VA_', 'ㄴ/ETM_', '10/SN_', '월/NNB_', '의/JKG_', '첫', '주/NNG_', '이/VCP_', 'ㅂ니다/EF_', './SF_', '환', '절/NNG_', '기/XSN_', '감기/NNG_', '조심/NNG_', '하/XSV_', '시/EP_', '고/EC_', '따', '듯하/VA_', 'ㄴ/ETM_', '차/NNG_', '와/JC_', '함께/MAG_', '건강/NNG_', '하/XSA_', 'ㄴ/ETM_', '한/MM_', '주/NNB_', '보내/VV_', '시/EP_', '기/ETN_', '바라/VV_', 'ㅂ니다/EF_', './SF_', 'X', 'X', 'X/SL_', '은행/NNG_', '모', '란/NNG_', '역/NNG_', 'X', 'X', 'X/SL_', '올림/NNG_']\n"
     ]
    }
   ],
   "source": [
    "# FullTokenizer.tokenize(); End2End Tokenizer\n",
    "text = copy.copy(return_result) # text 초기화\n",
    "print('************** START TOKENING MORPHLOGY **************')\n",
    "\n",
    "# BasicTokenizer.tokenize()\n",
    "print('\\nOrigin Text:  ', text)\n",
    "# text = convert_to_unicode(text)\n",
    "text = _clean_text(text)\n",
    "print('\\nCleaned Text: ', text)\n",
    "# fn: whitespace_tokenize()\n",
    "orig_tokens = whitespace_tokenize(text)\n",
    "print('\\nOrig. Tokens: ', orig_tokens, end='\\n\\n')\n",
    "Basic_split_tokens = []\n",
    "for token in orig_tokens:\n",
    "    if (do_lower_case) and (token not in never_split):\n",
    "#         token = token.lower()\n",
    "        # fn: _run_strip_accents\n",
    "        t = unicodedata.normalize(\"NFD\", token)\n",
    "        # https://gist.github.com/Pusnow/aa865fa21f9557fa58d691a8b79f8a6d\n",
    "        # 모든 음절을 정준 분해(Canonical Decomposition)시킴\n",
    "        # '각'을 'ㄱ+ㅏ+ㄱ'으로 저장(출력되는 값은 동일)\n",
    "        output = []\n",
    "        for char in t:\n",
    "            cat = unicodedata.category(char)\n",
    "            if cat == \"Mn\":\n",
    "                # unicode category가 \"Mark, Nonspacing\"일 경우 pass\n",
    "                continue\n",
    "            output.append(char)\n",
    "        token = \"\".join(output)\n",
    "        print('\\tstripped accent+norm(NFD) Token : '+t)\n",
    "    # fn: _run_split_on_punc()\n",
    "    if text in never_split:\n",
    "        token = [text]\n",
    "    else:\n",
    "        chars = list(token)\n",
    "        i, start_new_word = 0, True\n",
    "        output = []\n",
    "        print('\\tchars : '+str(chars))\n",
    "        while i < len(chars):\n",
    "            char = chars[i]\n",
    "            if _is_punctuation(char):\n",
    "                # 공백이면 [\" \"]를 추가하고 새로운 단어로 시작\n",
    "                output.append([char])\n",
    "                start_new_word = True\n",
    "            else:\n",
    "                # 공백이 아닐 경우,\n",
    "                if start_new_word:\n",
    "                    # 새로운 문자로 시작할 경우 빈 리스트 추가.\n",
    "                    output.append([])\n",
    "                # 해당 단어부터 시작하도록 start_new_word는 False로 setting.\n",
    "                start_new_word = False\n",
    "                # 위에 추가한 빈 리스트에 각각 character를 채워넣음\n",
    "                output[-1].append(char)\n",
    "            i += 1\n",
    "        token = [\"\".join(x) for x in output]\n",
    "    Basic_split_tokens.extend(token)\n",
    "print('\\nBasic_split_tokens : ', Basic_split_tokens)\n",
    "Basic_output_tokens = whitespace_tokenize((' '.join(Basic_split_tokens)).strip())\n",
    "print('\\nBasic_output Tokens: ', Basic_output_tokens, end='\\n\\n')\n",
    "\n",
    "Total_split_tokens = [] # 최종 tokenize 결과 저장\n",
    "print('************** START GREEDY LONGEST MATCH FIRST ALGORITHM **************')\n",
    "for tokens in Basic_output_tokens:\n",
    "    tokens += '_' # adding '_'\n",
    "    # WordpieceTokenizer.tokenize()\n",
    "    unk_token = \"[UNK]\"\n",
    "    max_input_chars_per_word = 100\n",
    "    # greedy longest-match-first algorithm to perform tokenization\n",
    "    # using the given vocabulary\n",
    "    tokens = convert_to_unicode(tokens)\n",
    "    WP_output_tokens = []\n",
    "    # fn: whitespace_tokenize\n",
    "    tokens = whitespace_tokenize(tokens)\n",
    "    # start lmf algorithm!\n",
    "    print('\\t', tokens)\n",
    "    for token in tokens:\n",
    "        chars = list(token)\n",
    "        if len(chars) > max_input_chars_per_word: # 100\n",
    "            # max word로 설정한 글자 수를 넘길 경우, UNK 처리\n",
    "            WP_output_tokens.append(unk_token)\n",
    "            continue\n",
    "        is_bad = False\n",
    "        start = 0\n",
    "        sub_tokens = []\n",
    "        while start < len(chars):\n",
    "            end = len(chars)\n",
    "            cur_substr = None\n",
    "            # 첫번째 글자부터 천천히 vocab에 있는 단어인지 체크\n",
    "            while start < end:\n",
    "                substr = \"\".join(chars[start:end])\n",
    "                # do_lower_case == True일 경우에\n",
    "                # 위에서 Canonical Decomposition 과정을 거쳤기 때문에\n",
    "                # 이를 다시 Composition해줘야 vocab의 단어와 비교 가능하다.\n",
    "                substr = unicodedata.normalize('NFC', substr)\n",
    "                print('\\t\\t\\t'+substr, end='')\n",
    "                if substr in vocab:\n",
    "                    # 만약 해당 단어가 vocab에 있다면 해당 단어로 break\n",
    "                    cur_substr = substr\n",
    "                    print('\\r\\t\\t\\t{:<15}{}'.format(\n",
    "                        cur_substr, '--|> Exists in vocab_file.'))\n",
    "                    break\n",
    "                end -= 1\n",
    "                print()\n",
    "            # 만일 못찾았을 경우, (1)로 가서 [UNK] 처리.\n",
    "            if cur_substr is None:\n",
    "                is_bad = True\n",
    "                break\n",
    "            sub_tokens.append(cur_substr)\n",
    "            # 어미를 추가하기 위해 start에 end값을 할당\n",
    "            start = end\n",
    "        if is_bad: # --- (1)\n",
    "            WP_output_tokens.append(unk_token)\n",
    "        else:\n",
    "            # 정상적으로 끝났다면 sub_token을 결과값에 할당\n",
    "            WP_output_tokens.extend(sub_tokens)\n",
    "    for sub_token in WP_output_tokens:\n",
    "        Total_split_tokens.append(sub_token)\n",
    "print('\\nTotal_split_tokens : ', Total_split_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어 사전에 있는대로 짤라버린다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_a = Total_split_tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X', 'X', 'X/SL_', '고객/NNG_', '님/XSN_', '항상/MAG_', 'X', 'X', 'X/SL_', '은행/NNG_', '모', '란/NNG_', '역/NNG_', '지점/NNG_', '을/JKO_', '이용/NNG_', '하/XSV_', '어/EC_', '주/VX_', '시/EP_', '는/ETM_', '고객/NNG_', '님/XSN_', '께/JKB_', '감사/NNG_', '의/JKG_', '마음/NNG_', '을/JKO_', '전하/VV_', 'ㅂ니다/EF_', './SF_', '혹시/MAG_', '업무/NNG_', '와/JKB_', '관련/NNG_', '하/XSV_', '어/EC_', '궁금하/VA_', 'ㄴ/ETM_', '점/NNG_', '이/JKS_', '있/VA_', '으시/EP_', '면/EC_', '이/MM_', '번호/NNG_', '로/JKB_', '연락/NNG_', '주/VV_', '시/EP_', '기/ETN_', '바라/VV_', 'ㅂ니다/EF_', './SF_', '성', '심', '껏/MAG_', '돕/VV_', '아/EC_', '드리/VX_', '겠/EP_', '습니다/EF_', './SF_', '또/MAG_', '혹시/MAG_', '고객/NNG_', '만족/NNG_', '도/NNG_', '조사/NNG_', '전화/NNG_', '를/JKO_', '받/VV_', '으시/EP_', '면/EC_', '매우/MAG_', '동의/NNG_', '하/XSV_', 'ㄴ다/EF_', '로/JKB_', '칭찬/NNG_', '하/XSV_', '어/EC_', '주/VX_', '시/EP_', '어요/EF_', '조금/NNG_', '은/JX_', '쌀', '쌀', '하/VA_', 'ㄴ/ETM_', '10/SN_', '월/NNB_', '의/JKG_', '첫', '주/NNG_', '이/VCP_', 'ㅂ니다/EF_', './SF_', '환', '절/NNG_', '기/XSN_', '감기/NNG_', '조심/NNG_', '하/XSV_', '시/EP_', '고/EC_', '따', '듯하/VA_', 'ㄴ/ETM_', '차/NNG_', '와/JC_', '함께/MAG_', '건강/NNG_', '하/XSA_', 'ㄴ/ETM_', '한/MM_', '주/NNB_', '보내/VV_', '시/EP_', '기/ETN_', '바라/VV_', 'ㅂ니다/EF_', './SF_', 'X', 'X', 'X/SL_', '은행/NNG_', '모', '란/NNG_', '역/NNG_', 'X', 'X', 'X/SL_', '올림/NNG_']\n",
      "cutting\n",
      "['X', 'X', 'X/SL_', '고객/NNG_', '님/XSN_', '항상/MAG_', 'X', 'X', 'X/SL_', '은행/NNG_', '모', '란/NNG_', '역/NNG_', '지점/NNG_', '을/JKO_', '이용/NNG_', '하/XSV_', '어/EC_', '주/VX_', '시/EP_', '는/ETM_', '고객/NNG_', '님/XSN_', '께/JKB_', '감사/NNG_', '의/JKG_', '마음/NNG_', '을/JKO_', '전하/VV_', 'ㅂ니다/EF_', './SF_', '혹시/MAG_', '업무/NNG_', '와/JKB_', '관련/NNG_', '하/XSV_', '어/EC_', '궁금하/VA_', 'ㄴ/ETM_', '점/NNG_', '이/JKS_', '있/VA_', '으시/EP_', '면/EC_', '이/MM_', '번호/NNG_', '로/JKB_', '연락/NNG_', '주/VV_', '시/EP_', '기/ETN_', '바라/VV_', 'ㅂ니다/EF_', './SF_', '성', '심', '껏/MAG_', '돕/VV_', '아/EC_', '드리/VX_', '겠/EP_', '습니다/EF_', './SF_', '또/MAG_', '혹시/MAG_', '고객/NNG_', '만족/NNG_', '도/NNG_', '조사/NNG_', '전화/NNG_', '를/JKO_', '받/VV_', '으시/EP_', '면/EC_', '매우/MAG_', '동의/NNG_', '하/XSV_', 'ㄴ다/EF_', '로/JKB_', '칭찬/NNG_', '하/XSV_', '어/EC_', '주/VX_', '시/EP_', '어요/EF_', '조금/NNG_', '은/JX_', '쌀', '쌀', '하/VA_', 'ㄴ/ETM_', '10/SN_', '월/NNB_', '의/JKG_', '첫', '주/NNG_', '이/VCP_', 'ㅂ니다/EF_', './SF_', '환', '절/NNG_', '기/XSN_', '감기/NNG_', '조심/NNG_', '하/XSV_', '시/EP_', '고/EC_', '따', '듯하/VA_', 'ㄴ/ETM_', '차/NNG_', '와/JC_', '함께/MAG_', '건강/NNG_', '하/XSA_', 'ㄴ/ETM_', '한/MM_', '주/NNB_', '보내/VV_', '시/EP_', '기/ETN_', '바라/VV_', 'ㅂ니다/EF_', './SF_', 'X', 'X']\n"
     ]
    }
   ],
   "source": [
    "print(tokens_a)\n",
    "if len(tokens_a) > max_seq_length - 2:\n",
    "    print('cutting')\n",
    "    tokens_a = tokens_a[:max_seq_length-2]\n",
    "print(tokens_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "segment_ids = []\n",
    "tokens.append(\"[CLS]\")\n",
    "segment_ids.append(0)\n",
    "for token in tokens_a:\n",
    "    tokens.append(token)\n",
    "    segment_ids.append(0)\n",
    "tokens.append(\"[SEP]\")\n",
    "segment_ids.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(segment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'X', 'X', 'X/SL_', '고객/NNG_', '님/XSN_', '항상/MAG_', 'X', 'X', 'X/SL_', '은행/NNG_', '모', '란/NNG_', '역/NNG_', '지점/NNG_', '을/JKO_', '이용/NNG_', '하/XSV_', '어/EC_', '주/VX_', '시/EP_', '는/ETM_', '고객/NNG_', '님/XSN_', '께/JKB_', '감사/NNG_', '의/JKG_', '마음/NNG_', '을/JKO_', '전하/VV_', 'ㅂ니다/EF_', './SF_', '혹시/MAG_', '업무/NNG_', '와/JKB_', '관련/NNG_', '하/XSV_', '어/EC_', '궁금하/VA_', 'ㄴ/ETM_', '점/NNG_', '이/JKS_', '있/VA_', '으시/EP_', '면/EC_', '이/MM_', '번호/NNG_', '로/JKB_', '연락/NNG_', '주/VV_', '시/EP_', '기/ETN_', '바라/VV_', 'ㅂ니다/EF_', './SF_', '성', '심', '껏/MAG_', '돕/VV_', '아/EC_', '드리/VX_', '겠/EP_', '습니다/EF_', './SF_', '또/MAG_', '혹시/MAG_', '고객/NNG_', '만족/NNG_', '도/NNG_', '조사/NNG_', '전화/NNG_', '를/JKO_', '받/VV_', '으시/EP_', '면/EC_', '매우/MAG_', '동의/NNG_', '하/XSV_', 'ㄴ다/EF_', '로/JKB_', '칭찬/NNG_', '하/XSV_', '어/EC_', '주/VX_', '시/EP_', '어요/EF_', '조금/NNG_', '은/JX_', '쌀', '쌀', '하/VA_', 'ㄴ/ETM_', '10/SN_', '월/NNB_', '의/JKG_', '첫', '주/NNG_', '이/VCP_', 'ㅂ니다/EF_', './SF_', '환', '절/NNG_', '기/XSN_', '감기/NNG_', '조심/NNG_', '하/XSV_', '시/EP_', '고/EC_', '따', '듯하/VA_', 'ㄴ/ETM_', '차/NNG_', '와/JC_', '함께/MAG_', '건강/NNG_', '하/XSA_', 'ㄴ/ETM_', '한/MM_', '주/NNB_', '보내/VV_', '시/EP_', '기/ETN_', '바라/VV_', 'ㅂ니다/EF_', './SF_', 'X', 'X', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3047, 3047, 1496, 1291, 1123, 2547, 3047, 3047, 1496, 994, 315, 1692, 375, 3277, 11, 456, 9, 20, 129, 388, 22, 1291, 1123, 3353, 1308, 13, 588, 11, 276, 158, 7, 5865, 1579, 101, 266, 9, 20, 4511, 10, 187, 16, 38, 4506, 71, 80, 1883, 31, 2597, 359, 388, 49, 2019, 158, 7, 270, 855, 5181, 2544, 62, 4971, 124, 116, 7, 179, 5865, 1291, 2379, 356, 268, 823, 19, 78, 4506, 71, 1210, 2680, 9, 41, 31, 3998, 9, 20, 129, 388, 526, 4380, 21, 9212, 9212, 248, 10, 113, 60, 13, 4508, 211, 15, 158, 7, 1807, 1509, 284, 9869, 4315, 9, 388, 23, 2577, 9115, 10, 208, 56, 162, 1394, 42, 10, 92, 2227, 561, 388, 49, 2019, 158, 7, 3047, 3047, 3]\n"
     ]
    }
   ],
   "source": [
    "# convert_tokens_to_ids\n",
    "def convert_by_vocab(vocab, items):\n",
    "    output = []\n",
    "    for item in items:\n",
    "        output.append(vocab[item])\n",
    "    return output\n",
    "\n",
    "input_ids = convert_by_vocab(vocab, tokens)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "# tokens are attended to.\n",
    "input_mask = [1] * len(input_ids)\n",
    "print(input_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-pad up to the sequence length.\n",
    "while len(input_ids) < max_seq_length:\n",
    "    input_ids.append(0)\n",
    "    input_mask.append(0)\n",
    "    segment_ids.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(input_ids) == max_seq_length\n",
    "assert len(input_mask) == max_seq_length\n",
    "assert len(segment_ids) == max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_id = label_map[example.label]\n",
    "label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:guid: train-6\n",
      "INFO:tensorflow:tokens\" [CLS] X X X/SL_ 고객/NNG_ 님/XSN_ 항상/MAG_ X X X/SL_ 은행/NNG_ 모 란/NNG_ 역/NNG_ 지점/NNG_ 을/JKO_ 이용/NNG_ 하/XSV_ 어/EC_ 주/VX_ 시/EP_ 는/ETM_ 고객/NNG_ 님/XSN_ 께/JKB_ 감사/NNG_ 의/JKG_ 마음/NNG_ 을/JKO_ 전하/VV_ ㅂ니다/EF_ ./SF_ 혹시/MAG_ 업무/NNG_ 와/JKB_ 관련/NNG_ 하/XSV_ 어/EC_ 궁금하/VA_ ㄴ/ETM_ 점/NNG_ 이/JKS_ 있/VA_ 으시/EP_ 면/EC_ 이/MM_ 번호/NNG_ 로/JKB_ 연락/NNG_ 주/VV_ 시/EP_ 기/ETN_ 바라/VV_ ㅂ니다/EF_ ./SF_ 성 심 껏/MAG_ 돕/VV_ 아/EC_ 드리/VX_ 겠/EP_ 습니다/EF_ ./SF_ 또/MAG_ 혹시/MAG_ 고객/NNG_ 만족/NNG_ 도/NNG_ 조사/NNG_ 전화/NNG_ 를/JKO_ 받/VV_ 으시/EP_ 면/EC_ 매우/MAG_ 동의/NNG_ 하/XSV_ ㄴ다/EF_ 로/JKB_ 칭찬/NNG_ 하/XSV_ 어/EC_ 주/VX_ 시/EP_ 어요/EF_ 조금/NNG_ 은/JX_ 쌀 쌀 하/VA_ ㄴ/ETM_ 10/SN_ 월/NNB_ 의/JKG_ 첫 주/NNG_ 이/VCP_ ㅂ니다/EF_ ./SF_ 환 절/NNG_ 기/XSN_ 감기/NNG_ 조심/NNG_ 하/XSV_ 시/EP_ 고/EC_ 따 듯하/VA_ ㄴ/ETM_ 차/NNG_ 와/JC_ 함께/MAG_ 건강/NNG_ 하/XSA_ ㄴ/ETM_ 한/MM_ 주/NNB_ 보내/VV_ 시/EP_ 기/ETN_ 바라/VV_ ㅂ니다/EF_ ./SF_ X X [SEP]\n",
      "INFO:tensorflow:input_ids: 2 3047 3047 1496 1291 1123 2547 3047 3047 1496 994 315 1692 375 3277 11 456 9 20 129 388 22 1291 1123 3353 1308 13 588 11 276 158 7 5865 1579 101 266 9 20 4511 10 187 16 38 4506 71 80 1883 31 2597 359 388 49 2019 158 7 270 855 5181 2544 62 4971 124 116 7 179 5865 1291 2379 356 268 823 19 78 4506 71 1210 2680 9 41 31 3998 9 20 129 388 526 4380 21 9212 9212 248 10 113 60 13 4508 211 15 158 7 1807 1509 284 9869 4315 9 388 23 2577 9115 10 208 56 162 1394 42 10 92 2227 561 388 49 2019 158 7 3047 3047 3\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    }
   ],
   "source": [
    "tf.logging.info('*** Example ***')\n",
    "tf.logging.info('guid: %s' % (example.guid))\n",
    "tf.logging.info('tokens\" %s' % \" \".join(\n",
    "    [printable_text(x) for x in tokens]))\n",
    "tf.logging.info('input_ids: %s' % \" \".join([str(x) for x in input_ids]))\n",
    "tf.logging.info('input_mask: %s' % \" \".join([str(x) for x in input_mask]))\n",
    "tf.logging.info('segment_ids: %s' % \" \".join([str(x) for x in segment_ids]))\n",
    "tf.logging.info('label: %s (id = %d)' % (example.label, label_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = InputFeatures(\n",
    "    input_ids=input_ids,\n",
    "    input_mask=input_mask,\n",
    "    segment_ids=segment_ids,\n",
    "    label_id=label_id,\n",
    "    is_real_example=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_int_feature(values):\n",
    "    f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('input_ids', int64_list {\n",
      "  value: 2\n",
      "  value: 3047\n",
      "  value: 3047\n",
      "  value: 1496\n",
      "  value: 1291\n",
      "  value: 1123\n",
      "  value: 2547\n",
      "  value: 3047\n",
      "  value: 3047\n",
      "  value: 1496\n",
      "  value: 994\n",
      "  value: 315\n",
      "  value: 1692\n",
      "  value: 375\n",
      "  value: 3277\n",
      "  value: 11\n",
      "  value: 456\n",
      "  value: 9\n",
      "  value: 20\n",
      "  value: 129\n",
      "  value: 388\n",
      "  value: 22\n",
      "  value: 1291\n",
      "  value: 1123\n",
      "  value: 3353\n",
      "  value: 1308\n",
      "  value: 13\n",
      "  value: 588\n",
      "  value: 11\n",
      "  value: 276\n",
      "  value: 158\n",
      "  value: 7\n",
      "  value: 5865\n",
      "  value: 1579\n",
      "  value: 101\n",
      "  value: 266\n",
      "  value: 9\n",
      "  value: 20\n",
      "  value: 4511\n",
      "  value: 10\n",
      "  value: 187\n",
      "  value: 16\n",
      "  value: 38\n",
      "  value: 4506\n",
      "  value: 71\n",
      "  value: 80\n",
      "  value: 1883\n",
      "  value: 31\n",
      "  value: 2597\n",
      "  value: 359\n",
      "  value: 388\n",
      "  value: 49\n",
      "  value: 2019\n",
      "  value: 158\n",
      "  value: 7\n",
      "  value: 270\n",
      "  value: 855\n",
      "  value: 5181\n",
      "  value: 2544\n",
      "  value: 62\n",
      "  value: 4971\n",
      "  value: 124\n",
      "  value: 116\n",
      "  value: 7\n",
      "  value: 179\n",
      "  value: 5865\n",
      "  value: 1291\n",
      "  value: 2379\n",
      "  value: 356\n",
      "  value: 268\n",
      "  value: 823\n",
      "  value: 19\n",
      "  value: 78\n",
      "  value: 4506\n",
      "  value: 71\n",
      "  value: 1210\n",
      "  value: 2680\n",
      "  value: 9\n",
      "  value: 41\n",
      "  value: 31\n",
      "  value: 3998\n",
      "  value: 9\n",
      "  value: 20\n",
      "  value: 129\n",
      "  value: 388\n",
      "  value: 526\n",
      "  value: 4380\n",
      "  value: 21\n",
      "  value: 9212\n",
      "  value: 9212\n",
      "  value: 248\n",
      "  value: 10\n",
      "  value: 113\n",
      "  value: 60\n",
      "  value: 13\n",
      "  value: 4508\n",
      "  value: 211\n",
      "  value: 15\n",
      "  value: 158\n",
      "  value: 7\n",
      "  value: 1807\n",
      "  value: 1509\n",
      "  value: 284\n",
      "  value: 9869\n",
      "  value: 4315\n",
      "  value: 9\n",
      "  value: 388\n",
      "  value: 23\n",
      "  value: 2577\n",
      "  value: 9115\n",
      "  value: 10\n",
      "  value: 208\n",
      "  value: 56\n",
      "  value: 162\n",
      "  value: 1394\n",
      "  value: 42\n",
      "  value: 10\n",
      "  value: 92\n",
      "  value: 2227\n",
      "  value: 561\n",
      "  value: 388\n",
      "  value: 49\n",
      "  value: 2019\n",
      "  value: 158\n",
      "  value: 7\n",
      "  value: 3047\n",
      "  value: 3047\n",
      "  value: 3\n",
      "}\n",
      "), ('input_mask', int64_list {\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "  value: 1\n",
      "}\n",
      "), ('segment_ids', int64_list {\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "  value: 0\n",
      "}\n",
      "), ('label_ids', int64_list {\n",
      "  value: 0\n",
      "}\n",
      "), ('is_real_example', int64_list {\n",
      "  value: 1\n",
      "}\n",
      ")])\n"
     ]
    }
   ],
   "source": [
    "features = collections.OrderedDict()\n",
    "\n",
    "features['input_ids'] = create_int_feature(feature.input_ids)\n",
    "features['input_mask'] = create_int_feature(feature.input_mask)\n",
    "features['input_ids'] = create_int_feature(feature.input_ids)\n",
    "features['segment_ids'] = create_int_feature(feature.segment_ids)\n",
    "features['label_ids'] = create_int_feature([feature.label_id])\n",
    "features['is_real_example'] = create_int_feature([int(feature.is_real_example)])\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_example = tf.train.Example(features=tf.train.Features(feature=features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "features {\n",
       "  feature {\n",
       "    key: \"input_ids\"\n",
       "    value {\n",
       "      int64_list {\n",
       "        value: 2\n",
       "        value: 3047\n",
       "        value: 3047\n",
       "        value: 1496\n",
       "        value: 1291\n",
       "        value: 1123\n",
       "        value: 2547\n",
       "        value: 3047\n",
       "        value: 3047\n",
       "        value: 1496\n",
       "        value: 994\n",
       "        value: 315\n",
       "        value: 1692\n",
       "        value: 375\n",
       "        value: 3277\n",
       "        value: 11\n",
       "        value: 456\n",
       "        value: 9\n",
       "        value: 20\n",
       "        value: 129\n",
       "        value: 388\n",
       "        value: 22\n",
       "        value: 1291\n",
       "        value: 1123\n",
       "        value: 3353\n",
       "        value: 1308\n",
       "        value: 13\n",
       "        value: 588\n",
       "        value: 11\n",
       "        value: 276\n",
       "        value: 158\n",
       "        value: 7\n",
       "        value: 5865\n",
       "        value: 1579\n",
       "        value: 101\n",
       "        value: 266\n",
       "        value: 9\n",
       "        value: 20\n",
       "        value: 4511\n",
       "        value: 10\n",
       "        value: 187\n",
       "        value: 16\n",
       "        value: 38\n",
       "        value: 4506\n",
       "        value: 71\n",
       "        value: 80\n",
       "        value: 1883\n",
       "        value: 31\n",
       "        value: 2597\n",
       "        value: 359\n",
       "        value: 388\n",
       "        value: 49\n",
       "        value: 2019\n",
       "        value: 158\n",
       "        value: 7\n",
       "        value: 270\n",
       "        value: 855\n",
       "        value: 5181\n",
       "        value: 2544\n",
       "        value: 62\n",
       "        value: 4971\n",
       "        value: 124\n",
       "        value: 116\n",
       "        value: 7\n",
       "        value: 179\n",
       "        value: 5865\n",
       "        value: 1291\n",
       "        value: 2379\n",
       "        value: 356\n",
       "        value: 268\n",
       "        value: 823\n",
       "        value: 19\n",
       "        value: 78\n",
       "        value: 4506\n",
       "        value: 71\n",
       "        value: 1210\n",
       "        value: 2680\n",
       "        value: 9\n",
       "        value: 41\n",
       "        value: 31\n",
       "        value: 3998\n",
       "        value: 9\n",
       "        value: 20\n",
       "        value: 129\n",
       "        value: 388\n",
       "        value: 526\n",
       "        value: 4380\n",
       "        value: 21\n",
       "        value: 9212\n",
       "        value: 9212\n",
       "        value: 248\n",
       "        value: 10\n",
       "        value: 113\n",
       "        value: 60\n",
       "        value: 13\n",
       "        value: 4508\n",
       "        value: 211\n",
       "        value: 15\n",
       "        value: 158\n",
       "        value: 7\n",
       "        value: 1807\n",
       "        value: 1509\n",
       "        value: 284\n",
       "        value: 9869\n",
       "        value: 4315\n",
       "        value: 9\n",
       "        value: 388\n",
       "        value: 23\n",
       "        value: 2577\n",
       "        value: 9115\n",
       "        value: 10\n",
       "        value: 208\n",
       "        value: 56\n",
       "        value: 162\n",
       "        value: 1394\n",
       "        value: 42\n",
       "        value: 10\n",
       "        value: 92\n",
       "        value: 2227\n",
       "        value: 561\n",
       "        value: 388\n",
       "        value: 49\n",
       "        value: 2019\n",
       "        value: 158\n",
       "        value: 7\n",
       "        value: 3047\n",
       "        value: 3047\n",
       "        value: 3\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature {\n",
       "    key: \"input_mask\"\n",
       "    value {\n",
       "      int64_list {\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "        value: 1\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature {\n",
       "    key: \"is_real_example\"\n",
       "    value {\n",
       "      int64_list {\n",
       "        value: 1\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature {\n",
       "    key: \"label_ids\"\n",
       "    value {\n",
       "      int64_list {\n",
       "        value: 0\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature {\n",
       "    key: \"segment_ids\"\n",
       "    value {\n",
       "      int64_list {\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "        value: 0\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.write(tf_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./output_dir/smishing/train.tf_record'"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:***** Running training *****\n",
      "INFO:tensorflow:  Num examples = 100\n",
      "INFO:tensorflow:  Batch size = 32\n",
      "INFO:tensorflow:  Num steps = 9\n"
     ]
    }
   ],
   "source": [
    "tf.logging.info(\"***** Running training *****\")\n",
    "tf.logging.info(\"  Num examples = %d\", len(train_examples))\n",
    "tf.logging.info(\"  Batch size = %d\", FLAGS.train_batch_size)\n",
    "tf.logging.info(\"  Num steps = %d\", num_train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacing안한 전체 데이터로 돌려보기!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = SmishingProcessor()\n",
    "label_list = processor.get_labels()\n",
    "\n",
    "# get train samples\n",
    "train_examples = processor.get_train_examples(dacon_path, 'train.tsv')\n",
    "num_train_steps = int(\n",
    "    len(train_examples) / FLAGS.train_batch_size * FLAGS.num_train_epochs)\n",
    "num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x00000219A36F36A8>) includes params argument, but params are not passed to Estimator.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\jinma\\AppData\\Local\\Temp\\tmpafvhq326\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\jinma\\\\AppData\\\\Local\\\\Temp\\\\tmpafvhq326', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002199E5846D8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': None}\n",
      "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
      "WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\n"
     ]
    }
   ],
   "source": [
    "# record ETRI model weights\n",
    "FLAGS.init_checkpoint = path + 'model.ckpt'\n",
    "\n",
    "model_fn = model_fn_builder(\n",
    "    bert_config=bert_config,\n",
    "    num_labels=len(label_list),             # 2\n",
    "    init_checkpoint=FLAGS.init_checkpoint,  # None\n",
    "    learning_rate=FLAGS.learning_rate,      # 5e-05\n",
    "    num_train_steps=num_train_steps,        # 22195\n",
    "    num_warmup_steps=num_warmup_steps,      # 2219\n",
    "    use_tpu=FLAGS.use_tpu,                  # False\n",
    "    use_one_hot_embeddings=FLAGS.use_tpu)   # False\n",
    "\n",
    "# If TPU is not available, this will fall back to normal Estimator on CPU\n",
    "# or GPU\n",
    "estimator = tf.contrib.tpu.TPUEstimator(\n",
    "    use_tpu=FLAGS.use_tpu,                        # False\n",
    "    model_fn=model_fn,\n",
    "    config=run_config,\n",
    "    train_batch_size=FLAGS.train_batch_size,      # 32\n",
    "    eval_batch_size=FLAGS.eval_batch_size,        # 8\n",
    "    predict_batch_size=FLAGS.predict_batch_size   # 8\n",
    ")\n",
    "\n",
    "FLAGS.output_dir = './output_dir/smishing/'\n",
    "\n",
    "tf.gfile.MakeDirs(FLAGS.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_based_convert_examples_to_features(\n",
    "    examples, label_lsit, max_seq_length, tokenizer, output_file):\n",
    "    \n",
    "    writer = tf.python_io.TFRecordWriter(output_file)\n",
    "\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        if ex_index % 10000 == 0:\n",
    "            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "\n",
    "        feature = convert_single_example(ex_index, example, label_list,\n",
    "                                         max_seq_length, tokenizer)\n",
    "\n",
    "        def create_int_feature(values):\n",
    "            f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
    "            return f\n",
    "\n",
    "        features = collections.OrderedDict()\n",
    "        features[\"input_ids\"] = create_int_feature(feature.input_ids)\n",
    "        features[\"input_mask\"] = create_int_feature(feature.input_mask)\n",
    "        features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n",
    "        features[\"label_ids\"] = create_int_feature([feature.label_id])\n",
    "        features[\"is_real_example\"] = create_int_feature(\n",
    "            [int(feature.is_real_example)])\n",
    "\n",
    "        tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = os.path.join(FLAGS.output_dir, 'train_non_spacing.tf_record')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_based_convert_examples_to_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
