{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BARTModel 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import warnings\n",
    "from typing import Any, Dict, Tuple, Union, Optional, List\n",
    "\n",
    "import numpy as np\n",
    "from overrides import overrides\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import transformers\n",
    "from transformers import BartConfig, BartTokenizer, BartModel\n",
    "from transformers.models.bart.modeling_bart import BartEncoder, BartDecoder\n",
    "from transformers.utils import logging\n",
    "from transformers.modeling_utils import PreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.__version__ == 1.7.1+cu110\n",
      "4.2.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(f\"torch.__version__ == {torch.__version__}\")\n",
    "print(transformers.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BartConfig()\n",
    "bart = BartModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BartModel.\\_\\_init\\_\\_\n",
    "- BartPretrainedModel을 상속받음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50265)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart.config.pad_token_id, bart.config.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50265, 1024, padding_idx=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart.shared # torch.nn.Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `BartPretrainedModel`의 init_weight 메서드 실시, 뜯어보자\n",
    "- `nn.Linear`, `nn.Embedding`의 경우 config의 std로 초기값 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BartPretrainedModel(PreTrainedModel):\n",
    "    config_class = BartConfig\n",
    "    base_model_prefix = \"model\"\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        std = self.config.init_std\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, BartSinusoidalPositionalEmbedding):\n",
    "            pass\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "\n",
    "    @property\n",
    "    def dummy_inputs(self):\n",
    "        pad_token = self.config.pad_token_id\n",
    "        input_ids = torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]], device=self.device)\n",
    "        dummy_inputs = {\n",
    "            \"attention_mask\": input_ids.ne(pad_token),\n",
    "            \"input_ids\": input_ids,\n",
    "        }\n",
    "        return dummy_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### BartEncoder\n",
    ">- `BartPretrainedModel` 객체를 동일하게 상속받음\n",
    ">### BartEncoder.\\_\\_init\\_\\_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout: 0.1,\n",
      "layerdrop: 0.0\n",
      "embed_dim: 1024,\n",
      "embed_scale: 1.0,\n",
      "padding_idx: 1,\n",
      "max_source_positions: 1024\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "dropout: {bart.encoder.dropout},\n",
    "layerdrop: {bart.config.encoder_layerdrop}\n",
    "embed_dim: {bart.config.d_model},\n",
    "embed_scale: {math.sqrt(bart.config.d_model) if config.scale_embedding else 1.0},\n",
    "padding_idx: {bart.config.pad_token_id},\n",
    "max_source_positions: {bart.config.max_position_embeddings}\n",
    "\"\"\".strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50265, 1024, padding_idx=1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart.encoder.embed_tokens # __init__에서 받아올 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BartLearnedPositionalEmbedding(1026, 1024, padding_idx=1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(bart.config.static_position_embeddings) # 21.01.06 commit으로 삭제!\n",
    "bart.encoder.embed_positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- config.static_position_embeddings에 따라 어떤 객체를 사용할지 갈림\n",
    "    - if True, `BartSinusoidalPositionalEmbedding`\n",
    "    - else: `BartLearnedPositionalEmbedding`\n",
    "- config.encoder_layers의 수만큼 EncoderLayer를 쌓음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.encoder_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartEncoderLayer(\n",
       "  (self_attn): BartAttention(\n",
       "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart.encoder.layers[0] # 이 layer를 12개 쌓음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- layernorm을 어떻게 적용할지 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n"
     ]
    }
   ],
   "source": [
    "if config.normalize_embedding:\n",
    "    print(BartLayerNorm(embed_dim))\n",
    "else:\n",
    "    print(nn.Identity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "if config.add_final_layer_norm:\n",
    "    print(BartLayerNorm(config.d_model))\n",
    "else:\n",
    "    print(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BartLayerNorm(\n",
    "    normalized_shape: torch.Size, eps: float = 1e-5, elementwise_affine: bool = True\n",
    "):\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            from apex.normalization import FusedLayerNorm\n",
    "\n",
    "            return FusedLayerNorm(normalized_shape, eps, elementwise_affine)\n",
    "        except ImportError:\n",
    "            pass\n",
    "    return torch.nn.LayerNorm(normalized_shape, eps, elementwise_affine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states= torch.randn(2, 5, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7331, -0.4191,  0.7951,  ...,  0.4873, -0.5494, -0.2944],\n",
       "         [-0.7643,  1.8104, -0.0323,  ..., -0.4546,  0.5776, -0.7373],\n",
       "         [-1.1619,  1.9948,  0.4805,  ..., -1.0691, -0.7803,  0.6411],\n",
       "         [ 0.0236,  0.1118, -0.2880,  ..., -1.5818,  0.1992, -0.9446],\n",
       "         [ 0.3735, -1.4478,  0.8767,  ...,  1.2091, -0.4567,  0.4698]],\n",
       "\n",
       "        [[ 0.4276, -1.4758,  0.0165,  ...,  1.9631, -0.1555, -1.0019],\n",
       "         [ 0.6768, -0.3537,  0.9676,  ..., -1.3469, -0.1781,  1.4861],\n",
       "         [ 0.5480, -1.0024,  0.4656,  ...,  0.5370, -0.4840,  0.0959],\n",
       "         [-1.5319,  0.8093, -0.3881,  ..., -0.5653, -0.3972,  0.5072],\n",
       "         [-1.0625,  0.7415, -0.8247,  ...,  0.1221, -0.1593, -0.0284]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.LayerNorm(1024)(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7331, -0.4191,  0.7951,  ...,  0.4873, -0.5494, -0.2944],\n",
       "         [-0.7643,  1.8104, -0.0323,  ..., -0.4546,  0.5776, -0.7373],\n",
       "         [-1.1619,  1.9948,  0.4805,  ..., -1.0691, -0.7803,  0.6411],\n",
       "         [ 0.0236,  0.1118, -0.2880,  ..., -1.5819,  0.1992, -0.9446],\n",
       "         [ 0.3735, -1.4478,  0.8767,  ...,  1.2091, -0.4567,  0.4698]],\n",
       "\n",
       "        [[ 0.4276, -1.4758,  0.0165,  ...,  1.9631, -0.1555, -1.0019],\n",
       "         [ 0.6768, -0.3537,  0.9676,  ..., -1.3469, -0.1781,  1.4862],\n",
       "         [ 0.5480, -1.0024,  0.4656,  ...,  0.5370, -0.4840,  0.0959],\n",
       "         [-1.5319,  0.8093, -0.3881,  ..., -0.5653, -0.3972,  0.5072],\n",
       "         [-1.0625,  0.7415, -0.8247,  ...,  0.1221, -0.1593, -0.0284]]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = hidden_states.mean(dim=-1)\n",
    "mean = mean[:, :, None].expand((*mean.size(), 1024))\n",
    "std = hidden_states.std(dim=-1, unbiased=False)\n",
    "std = std[:, :, None].expand((*std.size(), 1024))\n",
    "\n",
    "(hidden_states - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### BartEncoder.forward\n",
    ">#### Ch0. forward의 input\n",
    ">```python\n",
    "input_ids            = None,\n",
    "attention_mask       = None,\n",
    "inputs_embeds        = None,\n",
    "output_attentions    = None,\n",
    "output_hidden_states = None,\n",
    "return_dict          = None,\n",
    ">```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### Ch1. config으로 input setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "(False,)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(config.output_attentions)\n",
    "print((config.output_hidden_states,))\n",
    "print(config.use_return_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- input을 아래의 코드로 처리\n",
    ">```python\n",
    "\\# retrieve input_ids and inputs_embeds\n",
    "if input_ids is not None and inputs_embeds is not None:\n",
    "    raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "elif input_ids is not None:\n",
    "    input_shape = input_ids.size()\n",
    "    input_ids = input_ids.view(-1, input_shape[-1])\n",
    "elif inputs_embeds is not None:\n",
    "    input_shape = inputs_embeds.size()[:-1]\n",
    "else:\n",
    "    raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "if inputs_embeds is None:\n",
    "    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n",
    ">```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartLearnedPositionalEmbedding(1026, 1024, padding_idx=1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart.encoder.embed_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1024])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart.encoder.embed_positions(bart.dummy_inputs['input_ids'].size()).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### 아래의 과정으로 처리\n",
    ">- input을 embedding (주어진 경우는 그냥 넘어감)\n",
    ">- input_shape으로 position vector 얻음\n",
    ">- hidden_states를 input_embeds + embeds_pos로 계산\n",
    ">- hidden_states를 LayerNorm해주고 Dropout 실시\n",
    ">- attention_mask가 None이 아니면 아래 코드로 expand (**디코더랑 처리가 조금 다름!**)\n",
    ">- output_hidden_states, output_attentions이 None이 아니면,\n",
    "    - () tuple을 주고 None이면 그냥 None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int]=None):\n",
    "    bsz, src_len = mask.size()\n",
    "    tgt_len = tgt_len if tgt_len is not None else src_len\n",
    "    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n",
    "    inverted_mask = 1.0 - expanded_mask\n",
    "    return inverted_mask.masked_fill(inverted_mask.bool(), torch.finfo(dtype).min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- 그리고 나선, BartEncoderLayer별로 아래의 연산을 수행\n",
    ">```python\n",
    "for encoder_layer in self.layers:\n",
    "    if output_hidden_states:\n",
    "        encoder_states = encoder_states + (hidden_states,)\n",
    "    # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "    dropout_probability = random.uniform(0, 1)\n",
    "    if self.training and (dropout_probability < self.layerdrop):  # skip the layer\n",
    "        attn = None\n",
    "    else:\n",
    "        hidden_states, attn = encoder_layer(\n",
    "            hidden_states, \n",
    "            attention_mask, \n",
    "            output_attentions=output_attentions\n",
    "        )\n",
    "    if output_attentions:\n",
    "        all_attentions = all_attentions + (attn,)\n",
    ">```\n",
    ">- 그 다음, layer_norm이 None이 아니면 hidden_states를 layer normalization\n",
    ">- output_hidden_states가 None이 아니면, encoder_states에 (hidden_states,)를 더해줌\n",
    ">- return_dict가 True인지 False인지에 따라 출력 결과물이 달라짐\n",
    "    - `False`: \n",
    "    ```python \n",
    "    tuple(\n",
    "        v for v in [\n",
    "            hidden_states, encoder_states, all_attentions\n",
    "        ] if v is not None\n",
    "    )\n",
    "    ```\n",
    "    - `True`:\n",
    "    ```python\n",
    "    BaseModelOutput(\n",
    "        last_hidden_state=hidden_states, \n",
    "        hidden_states=encoder_states, \n",
    "        attentions=all_attentions\n",
    "    )\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### BartDecoder\n",
    ">- `BartPretrainedModel` 객체를 동일하게 상속받음\n",
    ">### BartDecoder.\\_\\_init\\_\\_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout: 0.1\n",
      "layerdrop: 0.0\n",
      "embed_dim: 1024,\n",
      "embed_scale: 1.0,\n",
      "padding_idx: 1,\n",
      "max_source_positions: 1024\n"
     ]
    }
   ],
   "source": [
    "dropout = config.dropout\n",
    "layerdrop = config.decoder_layerdrop\n",
    "\n",
    "embed_dim = config.d_model\n",
    "embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n",
    "padding_idx = config.pad_token_id\n",
    "max_source_positions = config.max_position_embeddings\n",
    "\n",
    "print(f\"\"\"\n",
    "dropout: {dropout}\n",
    "layerdrop: {layerdrop}\n",
    "embed_dim: {embed_dim},\n",
    "embed_scale: {embed_scale},\n",
    "padding_idx: {padding_idx},\n",
    "max_source_positions: {max_source_positions}\n",
    "\"\"\".strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decoder 차별점\n",
    "do_blenderbot_90_layernorm = config.do_blenderbot_90_layernorm  # layernorm variant\n",
    "do_blenderbot_90_layernorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_tokens: Optional[nn.Embedding] = None\n",
    "\n",
    "# None이면\n",
    "embed_tokens = nn.Embedding(config.vocab_size, embed_dim, padding_idx)\n",
    "\n",
    "# None이 아니면\n",
    "embed_tokens = embed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- config.static_position_embeddings에 따라 어떤 객체를 사용할지 갈림\n",
    "    - if True, `BartSinusoidalPositionalEmbedding`\n",
    "    - else: `BartLearnedPositionalEmbedding`\n",
    "- config.encoder_layers의 수만큼 EncoderLayer를 쌓음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.decoder_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartDecoderLayer(\n",
       "  (self_attn): BartAttention(\n",
       "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (encoder_attn): BartAttention(\n",
       "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart.decoder.layers[0] # 이 layer를 12개 쌓음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n"
     ]
    }
   ],
   "source": [
    "if config.normalize_embedding:\n",
    "    print(BartLayerNorm(embed_dim)) # config.d_model과 동일\n",
    "else:\n",
    "    print(nn.Identity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "if config.add_final_layer_norm:\n",
    "    print(BartLayerNorm(config.d_model))\n",
    "else:\n",
    "    print(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### BartDecoder.forward\n",
    ">#### Ch0. forward의 input\n",
    ">```python\n",
    "input_ids=None,\n",
    "attention_mask=None,\n",
    "encoder_hidden_states=None,\n",
    "encoder_attention_mask=None,\n",
    "past_key_values=None,\n",
    "inputs_embeds=None,\n",
    "use_cache=None,\n",
    "output_attentions=None,\n",
    "output_hidden_states=None,\n",
    "return_dict=None,\n",
    ">```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "(False,)\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(config.output_attentions)\n",
    "print((config.output_hidden_states,))\n",
    "print(config.use_cache) # 차이점\n",
    "print(config.use_return_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- input을 아래의 코드로 처리\n",
    ">```python\n",
    "\\# retrieve input_ids and inputs_embeds\n",
    "if input_ids is not None and inputs_embeds is not None:\n",
    "    raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n",
    "elif input_ids is not None:\n",
    "    input_shape = input_ids.size()\n",
    "    input_ids = input_ids.view(-1, input_shape[-1])\n",
    "elif inputs_embeds is not None:\n",
    "    input_shape = inputs_embeds.size()[:-1]\n",
    "else:\n",
    "    raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n",
    "if inputs_embeds is None:\n",
    "    inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n",
    ">```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decoder 차이점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# past_key_values_length\n",
    "past_key_values = None\n",
    "past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
    "past_key_values_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decoder 차이점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = bart.dummy_inputs['attention_mask']\n",
    "input_shape = bart.dummy_inputs['input_ids'].size()\n",
    "input_ids = bart.dummy_inputs['input_ids'].view(-1, input_shape[-1])\n",
    "inputs_embeds = embed_tokens(input_ids) * embed_scale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_causal_mask(input_ids_shape: torch.Size, dtype: torch.dtype, past_key_values_length: int = 0):\n",
    "    \"\"\"\n",
    "    Make causal mask used for bi-directional self-attention.\n",
    "    \"\"\"\n",
    "    bsz, tgt_len = input_ids_shape\n",
    "    mask = torch.full((tgt_len, tgt_len), float(\"-inf\"))\n",
    "    mask_cond = torch.arange(mask.size(-1))\n",
    "    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
    "    mask = mask.to(dtype)\n",
    "\n",
    "    if past_key_values_length > 0:\n",
    "        mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype), mask], dim=-1)\n",
    "    return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attentoin Mask 처리\n",
    "\n",
    "# Create causal mask\n",
    "# [bsz, seq_len] -> [bsz, 1, tgt_seq_len, srxc_seq_len]\n",
    "combined_attention_mask = None\n",
    "if input_shape[-1] > 1: # 걍 무조건 하는거나 다름없음\n",
    "    combined_attention_mask = _make_causal_mask(\n",
    "        input_shape, inputs_embeds.dtype, past_key_values_length=past_key_values_length\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 5, 5])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_attention_mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create decoder_padding_mask if not provided and needed\n",
    "# 4.12.20 (PVP): Not a fan of this \"magical\" function that\n",
    "# automatically creates attention_mask for padded tokens\n",
    "# => this is inconsistent with other models\n",
    "# => Pegasus uses the pad_token as decoder_start_token_id, so that this could\n",
    "# pose some problems.\n",
    "if (\n",
    "    attention_mask is None\n",
    "    and input_ids is not None\n",
    "    and input_shape[-1] > 1\n",
    "    and config.pad_token_id in input_ids\n",
    "):\n",
    "    # should be kept for backwards compatibility\n",
    "    attention_mask = input_ids.ne(config.pad_token_id).to(torch.long)\n",
    "    # never mask leading token, even if it is pad\n",
    "    attention_mask[:, 0] = attention_mask[:, 1]\n",
    "    if past_key_values_length > 0:\n",
    "        attention_mask = torch.cat(\n",
    "            [\n",
    "                torch.ones(\n",
    "                    (input_shape[0], past_key_values_length), dtype=torch.long, device=input_ids.device\n",
    "                ),\n",
    "                attention_mask,\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = bart.encoder(**bart.dummy_inputs, return_dict=True, output_attentions=True, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 5, 5])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.attentions[-1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand encoder attention mask\n",
    "encoder_hidden_states = None\n",
    "encoder_attention_mask = None\n",
    "\n",
    "# BartModel에서 encoder의 결과값을 Decoder에 넣어줌!\n",
    "if encoder_hidden_states is not None and encoder_attention_mask is not None:\n",
    "    # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "    encoder_attention_mask = _expand_mask(\n",
    "        encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])\n",
    "else:\n",
    "    print('지금은 None!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "if attention_mask is not None and combined_attention_mask is not None:\n",
    "    # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "    combined_attention_mask = combined_attention_mask + _expand_mask(\n",
    "        attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### 아래의 과정으로 처리! (Encoder와 유사)\n",
    ">- input_shape으로 position vector 얻음, past_key_values_length도 넣어줌\n",
    ">\n",
    ">#### Decoder에서 다른 점!\n",
    ">- do_blenderbot_90_layernorm이 True가 아니라면 Encoder와 동일하게 계산\n",
    "    - hidden_states를 input_embeds + embeds_pos로 계산\n",
    "    - hidden_states를 LayerNorm해줌\n",
    ">- do_blenderbot_90_layernorm가 True면\n",
    "    - inputs_embeds를 LayerNorm해주고 (이 결과값이 hidden_states)\n",
    "    - hidden_states에 embeds_pos를 더해줌\n",
    ">- 이 후, Dropout\n",
    ">- 그리고 나서 아래 값들에 대해 Tuple을 할당\n",
    ">```python\n",
    "all_hidden_states = () if output_hidden_states else None\n",
    "all_self_attns = () if output_attentions else None\n",
    "all_cross_attentions = () if output_attentions else None\n",
    "next_decoder_cache = () if use_cache else None\n",
    ">```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- 그리고 나선, BartDecoderLayer별로 아래의 연산을 수행\n",
    ">```python\n",
    "for idx, decoder_layer in enumerate(self.layers): # Add idx\n",
    "    # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "    if output_hidden_states:\n",
    "        # encoder_states = encoder_states + (hidden_states,) # Encoder\n",
    "        all_hidden_states += (hidden_states,)\n",
    "    dropout_probability = random.uniform(0, 1)\n",
    "    if self.training and (dropout_probability < self.layerdrop):  # skip the layer\n",
    "        # attn = None\n",
    "        # Encoder에선 if output_attentions: 구문을 도는데\n",
    "        # Decoder에선 걍 continue\n",
    "        continue\n",
    "    hidden_states, layer_self_attn, present_key_value, layer_cross_attn = decoder_layer(\n",
    "        hidden_states, \n",
    "        attention_mask=combined_attention_mask, \n",
    "        encoder_hidden_states=encoder_hidden_states,\n",
    "        encoder_attention_mask=encoder_attention_mask,\n",
    "        past_key_value=past_key_value,\n",
    "        output_attentions=output_attentions,\n",
    "    )\n",
    "    # Decoder에서 추가된 부분\n",
    "    if use_cache:\n",
    "        next_decoder_cache += (present_key_value,)\n",
    "    if output_attentions:\n",
    "        # all_attentions = all_attentions + (attn,)\n",
    "        all_self_attns += (layer_self_attn,)\n",
    "        all_cross_attentions += (layer_cross_attn,)\n",
    "if output_hidden_states: # add hidden states from the last decoder layer\n",
    "    all_hidden_states += (hidden_states,)\n",
    ">```\n",
    ">- output_hidden_states가 None이 아니면, all_hidden_states에 (hidden_states,)를 더해줌\n",
    "    - encoder_states였었음\n",
    ">- 그 다음, layer_norm이 None이 아니면 hidden_states를 layer normalization\n",
    ">- use_cache가 True면 next_decoder_cache를, 아니면 None을 next_cache에 할당\n",
    ">- return_dict가 True인지 False인지에 따라 출력 결과물이 달라짐\n",
    "    - `False`: \n",
    "    ```python \n",
    "    tuple(\n",
    "        v for v in [\n",
    "            hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions\n",
    "        ] if v is not None\n",
    "    )\n",
    "    ```\n",
    "    - `True`:\n",
    "    ```python\n",
    "    BaseModelOutputWithPastAndCrossAttentions(\n",
    "        last_hidden_state=hidden_states,\n",
    "        past_key_values=next_cache,\n",
    "        hidden_states=all_hidden_states,\n",
    "        attentions=all_self_attns,\n",
    "        cross_attentions=all_cross_attentions,\n",
    "    )\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BartModel.forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ch0. forward의 input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "input_ids              = None\n",
    "attention_mask         = None\n",
    "decoder_input_ids      = None\n",
    "decoder_attention_mask = None\n",
    "encoder_outputs        = None\n",
    "past_key_values        = None\n",
    "inputs_embeds          = None\n",
    "decoder_inputs_embeds  = None\n",
    "use_cache              = None\n",
    "output_attentions      = None\n",
    "output_hidden_states   = None\n",
    "return_dict            = None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. config으로 input setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "p(config.pad_token_id)\n",
    "p(config.output_attentions)\n",
    "p(config.output_hidden_states)\n",
    "p(config.use_cache)\n",
    "p(config.use_return_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = bart.dummy_inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 1024])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart.encoder(input_ids).last_hidden_state.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output = bart.decoder(\n",
    "    input_ids, encoder_hidden_states=bart.encoder(input_ids).last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'past_key_values'])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 1024])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output.last_hidden_state.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction = torch.randn(32, 10, 1024)\n",
    "reconstruction = torch.softmax(reconstruction, dim=-1)\n",
    "\n",
    "clean = torch.LongTensor(32, 10).random_(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected target size (32, 1024), got torch.Size([32, 10])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-145-7e25b81670f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreconstruction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    960\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    961\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m--> 962\u001b[1;33m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[0;32m    963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2466\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2467\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2468\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\basic\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2272\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2273\u001b[0m             raise ValueError('Expected target size {}, got {}'.format(\n\u001b[1;32m-> 2274\u001b[1;33m                 out_size, target.size()))\n\u001b[0m\u001b[0;32m   2275\u001b[0m         \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2276\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected target size (32, 1024), got torch.Size([32, 10])"
     ]
    }
   ],
   "source": [
    "nn.CrossEntropyLoss()(reconstruction, clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{torch.Size([2, 16, 5, 64])}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([v.size() for pkv in decoder_output.past_key_values for v in pkv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int):\n",
    "    \"\"\"\n",
    "    Shift input ids one token to the right, and wrap the last non pad token (usually <eos>).\n",
    "    \"\"\"\n",
    "    prev_output_tokens = input_ids.clone()\n",
    "\n",
    "    assert pad_token_id is not None, \"self.model.config.pad_token_id has to be defined.\"\n",
    "    # replace possible -100 values in labels by `pad_token_id`\n",
    "    prev_output_tokens.masked_fill_(prev_output_tokens == -100, pad_token_id)\n",
    "\n",
    "    index_of_eos = (prev_output_tokens.ne(pad_token_id).sum(dim=1) - 1).unsqueeze(-1)\n",
    "    decoder_start_tokens = prev_output_tokens.gather(1, index_of_eos).squeeze()\n",
    "    prev_output_tokens[:, 1:] = prev_output_tokens[:, :-1].clone()\n",
    "    prev_output_tokens[:, 0] = decoder_start_tokens\n",
    "\n",
    "    return prev_output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  6, 10,  4,  2],\n",
       "        [ 0,  8, 12,  2,  1]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  0,  6, 10,  4],\n",
       "        [ 2,  0,  8, 12,  2]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift_tokens_right(input_ids, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ch2. Encoder\n",
    "#### Ch2.번외 Enc output -> BaseModelOutput setting\n",
    "\n",
    "- return_dict가 True이고\n",
    "- encoder_outputs이 BaseModelOutput 객체가 아니면\n",
    "- 아래 코드로 형변환시켜줌\n",
    "\n",
    "```python\n",
    "encoder_outputs = BaseModelOutput(\n",
    "    last_hidden_state=encoder_outputs[0],\n",
    "    hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
    "    attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ch3. Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ch4. 최종 output\n",
    "- return_dict가 False일 경우엔\n",
    "    - decoder_outputs + encoder_outputs를 출력\n",
    "- 그 외의 경우엔\n",
    "    - Seq2SeqModelOutput에 결과값을 입력 후 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = bart(**bart.dummy_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'past_key_values', 'encoder_last_hidden_state'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 1024])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.last_hidden_state.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{torch.Size([2, 16, 5, 64])}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([v.size() for pkv in output.past_key_values for v in pkv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 1024, 16, 16)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16 * 64, config.d_model, config.encoder_attention_heads, config.decoder_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 1024])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.encoder_last_hidden_state.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BartModel(BartPretrainedModel):\n",
    "    \n",
    "    def __init__(self, config: BartConfig):\n",
    "        pass\n",
    "    \n",
    "    @overrides\n",
    "    def get_input_embeddings(self):\n",
    "        return self.shared\n",
    "        \n",
    "    @overrides\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.shared = value\n",
    "        self.encoder.embed_tokens = self.shared\n",
    "        self.decoder.embed_tokens = self.shared\n",
    "        \n",
    "    def get_encoder(self):\n",
    "        return self.encoder\n",
    "    \n",
    "    def get_decoder(self):\n",
    "        return self.decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 번외"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PreTrainedModel 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.configuration_utils import PretrainedConfig\n",
    "\n",
    "# file_utils.py\n",
    "DUMMY_INPUTS = [[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]]\n",
    "DUMMY_MASK = [[1, 1, 1, 1, 1], [1, 1, 1, 0, 0], [0, 0, 0, 1, 1]]\n",
    "\n",
    "pt_config = PretrainedConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_utils import ModuleUtilsMixin\n",
    "from transformers.generation_utils import GenerationMixin # Beam Search 파보쟈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_config.is_encoder_decoder, pt_config.tie_word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.is_encoder_decoder, config.tie_word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({}, {})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.pruned_heads, pt_config.pruned_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin):\n",
    "    config_class = None\n",
    "    base_model_prefix = \"\"\n",
    "    _keys_to_ignore_on_load_missing = None\n",
    "    _keys_to_ignore_on_load_unexpected = None\n",
    "    _keys_to_ignore_on_save = None\n",
    "    \n",
    "    @property\n",
    "    def dummy_inputs(self) -> Dict[str, torch.Tensor]:\n",
    "        return {'input_ids': torch.tensor(DUMMY_INPUTS)}\n",
    "    \n",
    "    def __init__(self, config: PretrainedConfig, *inputs, **kwargs):\n",
    "        super().__init__()\n",
    "        if not isinstance(config, PretrainedConfig):\n",
    "            raise ValueError(\n",
    "                \"Parameter config in `{}(config)` should be an instance of class `PretrainedConfig`. \"\n",
    "                \"To create a model from a pretrained model use \"\n",
    "                \"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(\n",
    "                    self.__class__.__name__, self.__class__.__name__\n",
    "                )\n",
    "            )\n",
    "        # Save config and origin of the pretrained weights if given in model\n",
    "        self.config = config\n",
    "        self.name_or_path = config.name_or_path\n",
    "        \n",
    "    @property\n",
    "    def base_model(self) -> nn.Module:\n",
    "        return getattr(self, self.base_model_prefix, self)\n",
    "    \n",
    "    def get_input_embeddings(self) -> nn.Module:\n",
    "        base_model = getattr(self, self.base_model_prefix, self)\n",
    "        if base_model is not self:\n",
    "            return base_model.get_input_embeddings()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "    def set_input_embeddings(self, value: nn.Module):\n",
    "        base_model = getattr(self, self.base_model_prefix, self)\n",
    "        if base_model is not self:\n",
    "            base_model.set_input_embeddings(value)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "    def get_output_embeddings(self) -> nn.Module:\n",
    "        return None # Overwrite for models with output embeddings\n",
    "    \n",
    "    def tie_weights(self):\n",
    "        output_embeddings = self.get_output_embeddings()\n",
    "        if output_embeddings is not None and self.config.tie_word_embeddings:\n",
    "            self._tie_or_clone_weights(output_embeddings, self.get_input_embeddings())\n",
    "\n",
    "        if self.config.is_encoder_decoder and self.config.tie_encoder_decoder:\n",
    "            if hasattr(self, self.base_model_prefix):\n",
    "                self = getattr(self, self.base_model_prefix)\n",
    "            self._tie_encoder_decoder_weights(\n",
    "                self.encoder, self.decoder, self.base_model_prefix)\n",
    "            \n",
    "    @staticmethod\n",
    "    def _tie_encoder_decoder_weights(encoder: nn.Module, decoder: nn.Module, base_model_prefix: str):\n",
    "        uninitialized_encoder_weights: List[str] = []\n",
    "        \"\"\"\n",
    "        1. encoder, decoder class가 같은지 체크!\n",
    "            >> In this case make sure that all encoder weights are correctly initialized.\n",
    "        2. weights를 recursively하게 tie\n",
    "            >> tie_encoder_to_decoder_recursively 함수는 내부에서 구현되어있음\n",
    "        \"\"\"\n",
    "        tie_encoder_to_decoder_recursively(\n",
    "            decoder, encoder, base_model_prefix, uninitialized_encoder_weights)\n",
    "        \n",
    "    def _tie_or_clone_weights(self, output_embeddings, input_embeddings):\n",
    "        \"\"\"\n",
    "        Tie or clone module weights depending of whether we are using\n",
    "        TorchScript or not\n",
    "        \"\"\"\n",
    "        if self.config.torchscript:\n",
    "            output_embeddings.weight = nn.Parameter(input_embeddings.weight.clone())\n",
    "        else:\n",
    "            output_embeddings.weight = input_embeddings.weight\n",
    "\n",
    "        if getattr(output_embeddings, \"bias\", None) is not None:\n",
    "            output_embeddings.bias.data = torch.nn.functional.pad(\n",
    "                output_embeddings.bias.data,\n",
    "                (\n",
    "                    0,\n",
    "                    output_embeddings.weight.shape[0] - output_embeddings.bias.shape[0],\n",
    "                ),\n",
    "                \"constant\",\n",
    "                0,\n",
    "            )\n",
    "        if hasattr(output_embeddings, \"out_features\") and hasattr(input_embeddings, \"num_embeddings\"):\n",
    "            output_embeddings.out_features = input_embeddings.num_embeddings\n",
    "            \n",
    "    def resize_token_embeddings(self, new_num_tokens: Optional[int] = None) -> torch.nn.Embedding:\n",
    "        pass\n",
    "    \n",
    "    def _resize_token_embeddings(self, new_num_tokens):\n",
    "        pass\n",
    "    \n",
    "    def _get_resized_embeddings(\n",
    "        self, old_embeddings: torch.nn.Embedding, new_num_tokens: Optional[int] = None\n",
    "    ) -> torch.nn.Embedding:\n",
    "        pass\n",
    "    \n",
    "    def _get_resized_lm_head(\n",
    "        self, old_lm_head: torch.nn.Linear, new_num_tokens: Optional[int] = None, transposed: Optional[bool] = False\n",
    "    ) -> torch.nn.Linear:\n",
    "        pass\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initializes and prunes weights if needed.\n",
    "        \"\"\"\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        # Prune heads if needed\n",
    "        if self.config.pruned_heads:\n",
    "            self.prune_heads(self.config.pruned_heads)\n",
    "\n",
    "        # Tie weights if needed\n",
    "        self.tie_weights()\n",
    "        \n",
    "    def prune_heads(self, heads_to_prune: Dict[int, List[int]]):\n",
    "        \"\"\"\n",
    "        Prunes heads of the base model.\n",
    "        Arguments:\n",
    "            heads_to_prune (:obj:`Dict[int, List[int]]`):\n",
    "                Dictionary with keys being selected layer indices (:obj:`int`) and associated values being the list of\n",
    "                heads to prune in said layer (list of :obj:`int`). For instance {1: [0, 2], 2: [2, 3]} will prune heads\n",
    "                0 and 2 on layer 1 and heads 2 and 3 on layer 2.\n",
    "        \"\"\"\n",
    "        # save new sets of pruned heads as union of previously stored pruned heads and newly pruned heads\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            union_heads = set(self.config.pruned_heads.get(layer, [])) | set(heads)\n",
    "            self.config.pruned_heads[layer] = list(union_heads)  # Unfortunately we have to store it as list for JSON\n",
    "\n",
    "        self.base_model._prune_heads(heads_to_prune)\n",
    "        \n",
    "    def save_pretrained(self, save_directory: Union[str, os.PathLike]):\n",
    "        pass\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], *model_args, **kwargs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "logger.info(\n",
    "    f\"{decoder.__class__} and {encoder.__class__} are not equal. In this case make sure that all encoder weights are correctly initialized.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart.encoder.__class__ != bart.decoder.__class__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Module의 apply 메서드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeVar, Callable\n",
    "\n",
    "T = TypeVar('T', bound='Module')\n",
    "\n",
    "def apply(self: T, fn: Callable[['Module'], None]) -> T:\n",
    "    for module in self.children():\n",
    "        module.apply(fn)\n",
    "    fn(self)\n",
    "    return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder, Decoder의 embed_positions\n",
    "- forward의 인자가 tensor가 아니라 torch.Size!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BartSinusoidalPositionalEmbedding(nn.Embedding):\n",
    "    \"\"\"This module produces sinusoidal positional embeddings of any length.\"\"\"\n",
    "\n",
    "    def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None):\n",
    "        super().__init__(num_positions, embedding_dim)\n",
    "        self.weight = self._init_weight(self.weight)\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_weight(out: nn.Parameter):\n",
    "        \"\"\"\n",
    "        Identical to the XLM create_sinusoidal_embeddings except features are not interleaved. \n",
    "        The cos features are in the 2nd half of the vector. [dim // 2:]\n",
    "        \"\"\"\n",
    "        n_pos, dim = out.shape\n",
    "        position_enc = np.array(\n",
    "            [\n",
    "                [pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)]\n",
    "                for pos in range(n_pos)\n",
    "            ]\n",
    "        )\n",
    "        out.requires_grad = False  # set early to avoid an error in pytorch-1.8+\n",
    "        sentinel = dim // 2 if dim % 2 == 0 else (dim // 2) + 1\n",
    "        out[:, 0:sentinel] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))\n",
    "        out[:, sentinel:]  = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n",
    "        out.detach_()\n",
    "        return out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0):\n",
    "        \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n",
    "        bsz, seq_len = input_ids_shape[:2]\n",
    "        positions = torch.arange(\n",
    "            past_key_values_length,\n",
    "            past_key_values_length + seq_len, \n",
    "            dtype=torch.long, \n",
    "            device=self.weight.device\n",
    "        )\n",
    "        return super().forward(positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\cfrac{pos}{10000^{\\cfrac{2}{d_{model}}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BartLearnedPositionalEmbedding(nn.Embedding):\n",
    "    \"\"\"\n",
    "    This module learns positional embeddings up to a fixed maximum size. Padding ids are ignored by either offsetting\n",
    "    based on padding_idx or by setting padding_idx to None and ensuring that the appropriate position ids are passed to\n",
    "    the forward function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, num_embeddings: int, embedding_dim: int, padding_idx: int, offset: int\n",
    "    ):\n",
    "        # Bart is set up so that if padding_idx is specified then offset the embedding ids by 2\n",
    "        # and adjust num_embeddings appropriately. Other models dont have this hack\n",
    "        self.offset = offset\n",
    "        assert padding_idx is not None, \"`padding_idx` should not be None, but of type int\"\n",
    "        num_embeddings += offset\n",
    "        super().__init__(num_embeddings, embedding_dim, padding_idx=padding_idx)\n",
    "\n",
    "    def forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0):\n",
    "        \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n",
    "        bsz, seq_len = input_ids_shape[:2]\n",
    "        positions = torch.arange(\n",
    "            past_key_values_length, \n",
    "            past_key_values_length + seq_len, \n",
    "            dtype=torch.long, \n",
    "            device=self.weight.device\n",
    "        )\n",
    "        return super().forward(positions + self.offset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BartEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BartEncoderLayer(nn.Module):\n",
    "    def __init__(self, config: BartConfig):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.d_model\n",
    "        self.self_attn = BartAttention(\n",
    "            embed_dim=self.embed_dim,\n",
    "            num_heads=config.encoder_attention_heads,\n",
    "            dropout=config.attention_dropout,\n",
    "        )\n",
    "        self.normalize_before = config.normalize_before\n",
    "        self.self_attn_layer_norm = BartLayerNorm(self.embed_dim)\n",
    "        self.dropout = config.dropout\n",
    "        self.activation_fn = ACT2FN[config.activation_function]\n",
    "        self.activation_dropout = config.activation_dropout\n",
    "        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)\n",
    "        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)\n",
    "        self.final_layer_norm = BartLayerNorm(self.embed_dim)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, attention_mask: torch.Tensor, output_attentions: bool = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (:obj:`torch.FloatTensor`): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
    "            attention_mask (:obj:`torch.FloatTensor`): attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "            output_attentions (:obj:`bool`): Whether the base model outputs attentions. This requires the attentions tensor to be reshaped in this function.\n",
    "        \"\"\"\n",
    "        residual = hidden_states\n",
    "        if self.normalize_before:\n",
    "            hidden_states = self.self_attn_layer_norm(hidden_states)\n",
    "        hidden_states, attn_weights, _ = self.self_attn(\n",
    "            hidden_states=hidden_states, attention_mask=attention_mask, output_attentions=output_attentions\n",
    "        )\n",
    "        hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "        hidden_states = residual + hidden_states\n",
    "        if not self.normalize_before:\n",
    "            hidden_states = self.self_attn_layer_norm(hidden_states)\n",
    "\n",
    "        residual = hidden_states\n",
    "        if self.normalize_before:\n",
    "            hidden_states = self.final_layer_norm(hidden_states)\n",
    "        hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
    "        hidden_states = F.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "        hidden_states = residual + hidden_states\n",
    "        if not self.normalize_before:\n",
    "            hidden_states = self.final_layer_norm(hidden_states)\n",
    "        if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():\n",
    "            clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n",
    "            hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n",
    "        return hidden_states, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BartDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BartDecoderLayer(nn.Module):\n",
    "    def __init__(self, config: BartConfig):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.d_model\n",
    "\n",
    "        self.self_attn = BartAttention(\n",
    "            embed_dim=self.embed_dim,\n",
    "            num_heads=config.decoder_attention_heads,\n",
    "            dropout=config.attention_dropout,\n",
    "            is_decoder=True,\n",
    "        )\n",
    "        self.dropout = config.dropout\n",
    "        self.activation_fn = ACT2FN[config.activation_function]\n",
    "        self.activation_dropout = config.activation_dropout\n",
    "        self.normalize_before = config.normalize_before\n",
    "\n",
    "        self.self_attn_layer_norm = BartLayerNorm(self.embed_dim)\n",
    "        self.encoder_attn = BartAttention(\n",
    "            self.embed_dim,\n",
    "            config.decoder_attention_heads,\n",
    "            dropout=config.attention_dropout,\n",
    "            is_decoder=True,\n",
    "        )\n",
    "        self.encoder_attn_layer_norm = BartLayerNorm(self.embed_dim)\n",
    "        self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n",
    "        self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n",
    "        self.final_layer_norm = BartLayerNorm(self.embed_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        output_attentions: Optional[torch.Tensor] = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (:obj:`torch.FloatTensor`): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
    "            attention_mask (:obj:`torch.FloatTensor`): attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "            encoder_hidden_states (:obj:`torch.FloatTensor`): cross attention input to the layer of shape `(seq_len, batch, embed_dim)`\n",
    "            encoder_attention_mask (:obj:`torch.FloatTensor`): encoder attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "            past_key_value (:obj:`Tuple(torch.FloatTensor)`): cached past key and value projection states\n",
    "            output_attentions (:obj:`bool`): Whether the base model outputs attentions. This requires the attentions tensor to be reshaped in this function.\n",
    "        \"\"\"\n",
    "        residual = hidden_states\n",
    "        if self.normalize_before:\n",
    "            hidden_states = self.self_attn_layer_norm(hidden_states)\n",
    "\n",
    "        # Self Attention\n",
    "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
    "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
    "        # add present self-attn cache to positions 1,2 of present_key_value tuple\n",
    "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            past_key_value=self_attn_past_key_value,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "        hidden_states = residual + hidden_states\n",
    "        if not self.normalize_before:\n",
    "            hidden_states = self.self_attn_layer_norm(hidden_states)\n",
    "\n",
    "        # Cross-Attention Block\n",
    "        cross_attn_present_key_value = None\n",
    "        cross_attn_weights = None\n",
    "        if encoder_hidden_states is not None:\n",
    "            residual = hidden_states\n",
    "            if self.normalize_before:\n",
    "                hidden_states = self.encoder_attn_layer_norm(hidden_states)\n",
    "\n",
    "            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n",
    "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
    "            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n",
    "                hidden_states=hidden_states,\n",
    "                key_value_states=encoder_hidden_states,\n",
    "                attention_mask=encoder_attention_mask,\n",
    "                past_key_value=cross_attn_past_key_value,\n",
    "                output_attentions=output_attentions,\n",
    "            )\n",
    "            hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "            hidden_states = residual + hidden_states\n",
    "            if not self.normalize_before:\n",
    "                hidden_states = self.encoder_attn_layer_norm(hidden_states)\n",
    "\n",
    "            # add cross-attn to positions 3,4 of present_key_value tuple\n",
    "            present_key_value = present_key_value + cross_attn_present_key_value\n",
    "\n",
    "        # Fully Connected\n",
    "        residual = hidden_states\n",
    "        if self.normalize_before:\n",
    "            hidden_states = self.final_layer_norm(hidden_states)\n",
    "        hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
    "        hidden_states = F.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "        hidden_states = residual + hidden_states\n",
    "        if not self.normalize_before:\n",
    "            hidden_states = self.final_layer_norm(hidden_states)\n",
    "\n",
    "        return (\n",
    "            hidden_states,\n",
    "            self_attn_weights,\n",
    "            present_key_value,\n",
    "            cross_attn_weights,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
