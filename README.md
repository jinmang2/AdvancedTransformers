# Korean Transformers
- Transformersì— ê´€ë ¨ëœ ê°œë…ë“¤ êµ¬í˜„ ë ˆí¬

## ğŸ¤— transformersì—ì„œ ì‚¬ìš©ë˜ëŠ” íŠ¹ë³„í•œ ê¸°ìˆ ë“¤
- gradient checkpoint
- reversible residual connection
- dynamic padding
- chunk feed forward network
- 3d, 4d multi-head scaled dot product attention
- various positional embedding
- various heads
- porting script
- generation mixin
- parrallelism mixin
- pushtohub mixin
- how to make tokenization script?
- trainer
- various utils


## Reference

### BPE
- [A New Algorithm for Data Compression](https://www.derczynski.com/papers/archive/BPE_Gage.pdf)

### Wordpiece
- [Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/abs/1609.08144)

### Transformers
- [Attention Is All You Need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
